<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.20.1 by Michael Rose
  Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<!-- <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> -->

<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Is my data normal? - Rik Voorhaar</title>
<meta name="description" content="Normally distributed data is great, but how do you know whether your data is normally distributed?">


  <meta name="author" content="Dr. Rik Voorhaar">


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Rik Voorhaar">
<meta property="og:title" content="Is my data normal?">
<meta property="og:url" content="https://blog.rikvoorhaar.com/normal-data/">


  <meta property="og:description" content="Normally distributed data is great, but how do you know whether your data is normally distributed?">



  <meta property="og:image" content="https://blog.rikvoorhaar.com/imgs/teasers/normal-data.png">





  <meta property="article:published_time" content="2020-08-10T00:00:00-05:00">





  

  


<link rel="canonical" href="https://blog.rikvoorhaar.com/normal-data/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Rik Voorhaar",
      "url": "https://blog.rikvoorhaar.com/"
    
  }
</script>






<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Rik Voorhaar Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<!-- <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"> -->
<script src="https://kit.fontawesome.com/ca9a31e360.js" crossorigin="anonymous"></script>


<!--[if IE]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->
<link rel="shortcut icon" type="image/x-icon" href="/favicon.ico">

<!-- end custom head snippets -->
<link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link href="https://fonts.googleapis.com/css2?family=Bitter:wght@500;600;700;800&family=Open+Sans:ital,wght@0,300;0,400;0,500;0,600;0,700;0,800;1,300;1,400;1,500;1,600;1,700;1,800&display=swap" rel="stylesheet">
  </head>

  <body class="layout--posts">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/"><img src="/assets/images/logo_name.svg" alt=""></a>
        
        <a class="site-title" href="/">
           
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/cv/">CV</a>
            </li><li class="masthead__menu-item">
              <a href="/">Blog</a>
            </li><li class="masthead__menu-item">
              <a href="/about/">Hobbies</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      <div class="page__hero-background"> </div>





<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  
    <div class="author__avatar">
      
        <img src="/imgs/avatar.jpg" alt="Dr. Rik Voorhaar" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Dr. Rik Voorhaar</h3>
    
    
      <div class="author__bio" itemprop="description">
        <p>ML Software Developer and curious mind</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Contact info and socials</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">Copenhagen, Denmark</span>
        </li>
      

      
        
          
            <li><a href="mailto:rik.voorhaar@gmail.com" rel="nofollow noopener noreferrer"><i class="far fa-fw fa-envelope" aria-hidden="true"></i><span class="label">rik.voorhaar@gmail.com</span></a></li>
          
        
          
            <li><a href="https://github.com/RikVoorhaar" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
            <li><a href="https://www.facebook.com/WH.Voorhaar" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span class="label">Facebook</span></a></li>
          
        
          
            <li><a href="https://discord.com/users/354966598643220480" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-discord" aria-hidden="true"></i><span class="label">Discord</span></a></li>
          
        
          
            <li><a href="https://www.linkedin.com/in/rik-voorhaar" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span class="label">LinkedIn</span></a></li>
          
        
          
            <li><a href="https://www.last.fm/user/Tilpo" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-lastfm-square" aria-hidden="true"></i><span class="label">Last.fm</span></a></li>
          
        
          
            <li><a href="https://www.goodreads.com/user/show/62542056-tilpo" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-goodreads" aria-hidden="true"></i><span class="label">Goodreads</span></a></li>
          
        
          
            <li><a href="https://steamcommunity.com/profiles/76561197996562422" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-steam" aria-hidden="true"></i><span class="label">Steam</span></a></li>
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <div class="archive">
    <br>
    
      <h1 id="page-title" class="page__title">Is my data normal?</h1>
    
    <p>Normally distributed data is great. It is easy to interpet, and many statistical and machine learning method work much better on normally distributed data. But how do we know if our data is actually normally distributed?</p>

<p>Let’s start with the well-known MNIST digit dataset. This is a very famous dataset. It consists of many 28x28 grayscale images of hand-drawn digits 0-9. Classifying digits is a great testing problem for many machine learning algorithms, and it’s often used for this purpose in education. <a href="https://www.kaggle.com/c/digit-recognizer">You can find this data on kaggle</a></p>

<p>As random variable we will take the average of the rows, which is just the mean brightness for each image.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mnist</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'train.csv'</span><span class="p">)</span>
<span class="n">mnist</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="s">"label"</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">mean_brightness</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">mnist</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>We can visualize the distribution of this data by using kernel density estimation. We can compare it to a normal distribution by also plotting the density of a normal distribution with same mean and standard deviation.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">scipy.stats</span>
<span class="k">def</span> <span class="nf">plot_normal</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="c1"># Estimate the distribution of the data
</span>    <span class="n">kde</span> <span class="o">=</span> <span class="n">scipy</span><span class="p">.</span><span class="n">stats</span><span class="p">.</span><span class="n">gaussian_kde</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="c1"># Make a normally distributed probability distribution
</span>    <span class="n">normal_distribution</span> <span class="o">=</span> <span class="n">scipy</span><span class="p">.</span><span class="n">stats</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="n">scale</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">std</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>

    <span class="c1"># Plot the two distributions
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">data</span><span class="p">),</span><span class="nb">max</span><span class="p">(</span><span class="n">data</span><span class="p">),</span><span class="n">num</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">kde</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="n">label</span><span class="o">=</span><span class="s">'Data'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">normal_distribution</span><span class="p">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="n">label</span><span class="o">=</span><span class="s">'Normal distribution'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plot_normal</span><span class="p">(</span><span class="n">mean_brightness</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/imgs/normal_data_5_0.svg" alt="svg" /></p>

<p>Going by these plots, our data is indeed wonderfully normally distributed! This is not surprising; the mean of a large number of random variables tends to be normally distributed, even if they are not identically distributed.</p>

<p>But what if we focus on a single pixel?</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">single_pixel</span> <span class="o">=</span> <span class="n">mnist</span><span class="p">[</span><span class="s">"pixel290"</span><span class="p">]</span>

<span class="n">plot_normal</span><span class="p">(</span><span class="n">single_pixel</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/imgs/normal_data_7_0.svg" alt="svg" /></p>

<p>We see two problems, this data’s distribution is bimodal (i.e. it has two different peaks), and it’s clipped off near the extreme values of 0 and 255. This makes sense, since we’re dealing with grayscale images.</p>

<h2 id="how-do-we-measure-if-data-is-normally-distributed">How do we measure if data is normally distributed?</h2>

<p>Looking at plots like these is a great part of exploratory data analysis. But it’s also very useful to output a concrete number and purely based on this number decide whether or not something is normally distributed.</p>

<p>One general approach is to measure the distance between the distribution of our data, and a best fit normal distribution. Let \(\hat X\) denote the estimated distribution of our data \(X\), and \(\mathcal N=\mathcal N(\mu,\sigma)\) denote a normal distribution with mean and variation estimated from \(X\). Then a common way to measure the distance between \(\hat X\) and \(\mathcal N\) is the Kullback-Leibler divergence. It has the following form:</p>

\[D_{KL}(\hat X|\mathcal N) = \int_{-\infty}^\infty\! \hat X(x)\log\left(\frac{\hat X(x)}{\mathcal N(x)}\right)\,\mathrm dx,\]

<p>where \(\hat X(x)\) and \(\mathcal N(x)\) denote the probability density functions. Note that we run into trouble if \(\mathcal N(x)=0\) but \(\hat X(x)\neq 0\). Fortunately this never happens in our case, since the normal distribution has positive density everywhere. Let’s compute compute this KL divergence for our two examples.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">scipy.integrate</span>
<span class="k">def</span> <span class="nf">KL_divergence_normal</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="c1"># Estimate the distribution of the data
</span>    <span class="n">kde</span> <span class="o">=</span> <span class="n">scipy</span><span class="p">.</span><span class="n">stats</span><span class="p">.</span><span class="n">gaussian_kde</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="c1"># Make a normally distributed probability distribution
</span>    <span class="n">normal_distribution</span> <span class="o">=</span> <span class="n">scipy</span><span class="p">.</span><span class="n">stats</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="n">scale</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">std</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
    <span class="n">norm_pdf</span> <span class="o">=</span> <span class="n">normal_distribution</span><span class="p">.</span><span class="n">pdf</span>

    <span class="c1"># Constrict the range of values to the interval (0,255)
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="mi">1000</span><span class="p">)</span>
    
    <span class="c1"># Estimate KL divergence by sampling on 1000 points
</span>    <span class="k">return</span> <span class="n">scipy</span><span class="p">.</span><span class="n">stats</span><span class="p">.</span><span class="n">entropy</span><span class="p">(</span><span class="n">kde</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="n">norm_pdf</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"KL divergence of mean brightness </span><span class="si">{</span><span class="n">KL_divergence_normal</span><span class="p">(</span><span class="n">mean_brightness</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"KL divergence of single pixel </span><span class="si">{</span><span class="n">KL_divergence_normal</span><span class="p">(</span><span class="n">single_pixel</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>KL divergence of mean brightness 0.02243918311004641
KL divergence of single pixel 0.6474992150590142
</code></pre></div></div>

<p>We cleary see that the mean brightness example has a much smaller KL divergence with respect to the best-fit normal distribution than the data for a single pixel, as expected.</p>

<p>However, there is a fundamental issue with this method. We used the kernel density estimation to get an estimated distribution of our data, but this tends to signficantly smoothen the data. Especially if we don’t have too many data points, this can lead to our data looking normally distributed when it is in fact not the case.</p>

<h2 id="the-estimated-cumulative-distribution">The estimated cumulative distribution</h2>

<p>The solution is simple. While estimating the <em>probability density</em> function requires smoothing the data, it’s much easier to estimate the <em>cumulative distribution</em> function. It only involves sorting our data. We can estimate the distribution function \(\hat F\) by:</p>

\[\hat F(t) = \frac{\#\{\text{samples}\leq t\}}{\#\{\text{samples}\}}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_normal_cdf</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>

    <span class="c1"># plot estimated CDF
</span>    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="n">data</span><span class="p">),</span><span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)),</span><span class="n">label</span><span class="o">=</span><span class="s">"Data"</span><span class="p">)</span>

    <span class="c1"># Make a normally distributed probability distribution, and plot CDF
</span>    <span class="n">normal_distribution</span> <span class="o">=</span> <span class="n">scipy</span><span class="p">.</span><span class="n">stats</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="n">scale</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">std</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">data</span><span class="p">),</span><span class="nb">max</span><span class="p">(</span><span class="n">data</span><span class="p">),</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">normal_distribution</span><span class="p">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="n">label</span><span class="o">=</span><span class="s">"Normal distribution"</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s">"CDF of </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s"> compared to normal CDF"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plot_normal_cdf</span><span class="p">(</span><span class="n">mean_brightness</span><span class="p">,</span><span class="n">name</span><span class="o">=</span><span class="s">"Mean brightness"</span><span class="p">)</span>
<span class="n">plot_normal_cdf</span><span class="p">(</span><span class="n">single_pixel</span><span class="p">,</span><span class="n">name</span><span class="o">=</span><span class="s">"Single pixel"</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/imgs/normal_data_12_0.svg" alt="svg" /></p>

<p><img src="/imgs/normal_data_12_1.svg" alt="svg" /></p>

<p>These CDF plots again visually confirm that one distribution is quite close to a normal distribution, whereass the other is not. In the second plot you can clearly see the singularity at 0 and 255 as well.</p>

<p>Using this CDF we propose a very simple statistic giving the distance between two distributions. If \(\hat F\) is the estimated CDF, and we’re comparing it to the CDF \(G\), then we define the <em>Kolmogorov-Smirnov</em> statistic by</p>

\[D = \sup_x|\hat F(x)-G(x)|\]

<p>We can compute this statistic directly with a <code class="language-plaintext highlighter-rouge">scipy</code> function:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">ks_stat</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="c1"># normalize the data
</span>    <span class="n">data</span> <span class="o">=</span> <span class="n">data</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="n">std</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="c1"># compute KS statistic
</span>    <span class="k">return</span> <span class="n">scipy</span><span class="p">.</span><span class="n">stats</span><span class="p">.</span><span class="n">kstest</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="s">"norm"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Kolmogorov-Smirnov statistic for Mean brightnes: %.4f, with p-value %.3E"</span><span class="o">%</span> <span class="n">ks_stat</span><span class="p">(</span><span class="n">mean_brightness</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Kolmogorov-Smirnov statistic for single pixel: %.4f, with p-value %.3E"</span><span class="o">%</span> <span class="n">ks_stat</span><span class="p">(</span><span class="n">single_pixel</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Kolmogorov-Smirnov statistic for Mean brightnes: 0.0315, with p-value 1.361E-36
Kolmogorov-Smirnov statistic for single pixel: 0.2608, with p-value 0.000E+00
</code></pre></div></div>

<p>Here we see that the mean brigthness data has much lower KS statistic than the single pixel. This function also produces a p-value, assigning a probality to the null-hypothesis that the data is exactly normally distributed. In both cases this p-value is very tiny, which means that we have enough statistical evidence to conclude that even the mean brightness data is not normally distributed. This is mainly because we’re basing this computation on 42000 data points, so even though the distance is to the normal distribution is small in this case, it’s still statistically significant. Let’s try to do the same, but subsample to only 100 points.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">"Kolmogorov-Smirnov statistic for Mean brightnes (n=100): %.4f, with p-value %.3f"</span><span class="o">%</span> <span class="n">ks_stat</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="n">mean_brightness</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Kolmogorov-Smirnov statistic for single pixel (n=100): %.4f, with p-value %.3E"</span><span class="o">%</span> <span class="n">ks_stat</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="n">single_pixel</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">)))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Kolmogorov-Smirnov statistic for Mean brightnes (n=100): 0.0925, with p-value 0.339
Kolmogorov-Smirnov statistic for single pixel (n=100): 0.2624, with p-value 1.449E-06
</code></pre></div></div>

<p>Now we see for both cases a much higher p-value. For the mean brightness case we do not see a statistically significant deviation from a normal distribution.</p>

<p>So far all the methods we mentioned are completely general, and work on any (univariate) distribution. But for normal distributions there are also some other tests which are slightly more powerful (i.e. they have p-values that converge faster with increasing number of samples). In any case all these tests all use the estimated CDF.</p>

<h2 id="mixing">Mixing</h2>

<p>Now in our case of a single pixel, it actually looks like the distribution is a mixture of two truncated normal distributions. Is there a way to obtain the parameters of these distributions and see whether this fits better? One very general method to obtain a best-fit distribution is through maximum likelihood estimation. The idea here is that rather than asking “what are the parameters that best describe our data?”, we should ask “given some parameters, what is the likelihood we got our data?”. This leads to the likelihood function, which assigns to a set of parameters a probability of observing our data. The “best” parameters are then those that assign the highest likelihood to our data. This is a relative crude method, because it tells us nothing about how good our estimate of the parameters is, but more on that later.</p>

<p>In our case we have a mixture of two normal distribitions (truncated to \([0, 255]\)). Such a distribution is parametrized by \(\theta=(\mu_1,\sigma_1,\mu_2,\sigma_2,t)\), where \(\mu_i\) denote the means and \(\sigma_i\) denote the standard deviation of both normal distribitions, and \(t\) is a number between 0 and 1 giving a weight to either distribution. Ignoring the truncation, this distribution has density given by</p>

\[f(x|\theta) = \frac{1}{\sigma_1 \sqrt{2 \pi}}\exp\left(-\frac12\left(\frac{x-\mu_1}{\sigma_1}\right)^2\right)+(1-t) \frac{1}{\sigma_2 \sqrt{2 \pi}}\exp\left(-\frac12\left(\frac{x-\mu_2}{\sigma_2}\right)^2\right)\]

<p>The likelihood of observing our data \((y_1,\dots,y_n)\) is then given by</p>

\[\mathcal L(y|\theta) = \prod_{i=1}^n f(y_i|\theta)\]

<p>Because taking a product of many small numbers is computationally unstable, we typically take a logarithm to get the log-likelihood function:</p>

\[\ell (y|\theta) = \sum_{i=1}^n \log f(y_i|\theta)\]

<p>We can maximize this using any standard optimizer, so long as we know the derivatives. Here the fact that we’re dealing with truncated normal distribitions actually doesn’t matter, since up to a multiplicative constant the distribitions are the same in the range \([0, 255]\) where all the data lives. The partial derivatives of the log-likelihood function \(\ell\) are given by</p>

\[\frac{\partial}{\partial \theta_i} \ell(y|\theta) = \sum_{i=1}^n \frac{\frac{\partial}{\partial \theta_i}f(y_i|\theta)}{f(y_i|\theta)}\]

<p>Fortunately there are algorithms for this particular problem that are much better than directly maximizing this function. These use the powerful EM algorithm. Due to their ubiquity in machine learning, there is a good implementation of Gaussian mixture models in <code class="language-plaintext highlighter-rouge">scikit-learn</code>, so let’s use that! To make a visual comparison, we will plot both the CDF and the estimate kernel densities.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.mixture</span> <span class="kn">import</span> <span class="n">GaussianMixture</span>

<span class="k">def</span> <span class="nf">fit_mixture</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">n_components</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">n_components</span><span class="p">)</span>
    <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">model</span>

<span class="k">def</span> <span class="nf">plot_model_cdf</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">model</span><span class="p">,</span><span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>

    <span class="c1"># plot estimated CDF
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="n">data</span><span class="p">),</span><span class="n">x</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s">"Data"</span><span class="p">)</span>

    <span class="c1"># estimate model CDF from 1000 datapoints, and truncate
</span>    <span class="n">n_samples</span> <span class="o">=</span> <span class="mi">10000</span>
    <span class="n">samples</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span>  <span class="n">model</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">samples</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">samples</span> <span class="k">if</span> <span class="p">(</span><span class="mi">0</span><span class="o">&lt;=</span><span class="n">s</span><span class="o">&lt;=</span><span class="mi">255</span><span class="p">)]</span>

    <span class="c1"># plot the CDF of gaussian mixture
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">samples</span><span class="p">))</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"Gaussian mixture"</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s">"CDF of </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s"> compared to Gaussian mixture CDF"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>

    <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>

    <span class="c1"># Estimate the distribution of the data and mixture
</span>    <span class="n">kde_data</span> <span class="o">=</span> <span class="n">scipy</span><span class="p">.</span><span class="n">stats</span><span class="p">.</span><span class="n">gaussian_kde</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="mf">0.1</span><span class="p">)</span>
    <span class="n">kde_mixture</span> <span class="o">=</span> <span class="n">scipy</span><span class="p">.</span><span class="n">stats</span><span class="p">.</span><span class="n">gaussian_kde</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>

    <span class="c1"># Plot the two distributions
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">data</span><span class="p">),</span><span class="nb">max</span><span class="p">(</span><span class="n">data</span><span class="p">),</span><span class="n">num</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">kde_data</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="n">label</span><span class="o">=</span><span class="s">'Data'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">kde_mixture</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="n">label</span><span class="o">=</span><span class="s">'Gaussian mixture'</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s">"PDF of </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s"> compared to Gaussian mixture PDF"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">fit_mixture</span><span class="p">(</span><span class="n">mean_brightness</span><span class="p">,</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plot_model_cdf</span><span class="p">(</span><span class="n">mean_brightness</span><span class="p">,</span><span class="n">model</span><span class="p">,</span><span class="n">name</span><span class="o">=</span><span class="s">"mean brightness"</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">fit_mixture</span><span class="p">(</span><span class="n">single_pixel</span><span class="p">,</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plot_model_cdf</span><span class="p">(</span><span class="n">single_pixel</span><span class="p">,</span><span class="n">model</span><span class="p">,</span><span class="n">name</span><span class="o">=</span><span class="s">"single pixel"</span><span class="p">)</span>

</code></pre></div></div>

<p><img src="/imgs/normal_data_19_0.svg" alt="svg" /></p>

<p><img src="/imgs/normal_data_19_1.svg" alt="svg" /></p>

<p>For the mean brigthness data it seems that the gaussian mixture with two components isn’t much better or even worse! This is because it is close to normally distributed already.</p>

<p>For the single pixel data there is a clear improvement, but it’s still clear that the data is not well represented by a gaussian mixture. This looks surprising from the estimated density function, since it <em>looks</em> like a mixture of two gaussians, but this is deceiving. It turns out that this is mostly due to the smoothing of the kernel density estimation. The CDF of this data also doesn’t look continuous at 0, so this is difficult to model even with a gaussian mixture model.</p>

<h2 id="what-is-the-point-of-all-this">What is the point of all this?</h2>

<p>Knowing the distribution of your features is very useful. Many machine learning and statistical methods need the data to be normalized, and work best if all your features are actually normally distributed. However if a particular feature has a very skewed distribution, as in for example the single pixel data shown here, then simply subtracting the mean and dividing by the standard deviation may not be the best way to normalize the data. In this case this pixel is usually either black or white, so we could choose to replace this feature with a binary feature that just says whether the data is black or white, and this may be more interpretable.</p>


Recent posts




<div class="entries-grid">
  
    



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/gmres-teaser.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/gmres/" rel="permalink">GMRES: or how to do fast linear algebra
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2022-03-10T00:00:00-06:00">March 10, 2022</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        16 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">Linear least-squares system pop up everywhere, and there are many fast way to solve them. We’ll be looking at one such way: GMRES.
</p>
  </article>
</div>

  
    



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/discrete-function-tensor.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/discrete-function-tensor/" rel="permalink">Machine learning with discretized functions and tensors
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2022-03-10T00:00:00-06:00">March 10, 2022</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        18 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">We recently made a paper about supervised machine learning using tensors, here’s the gist of how this works.
</p>
  </article>
</div>

  
    



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/st-vitus-rank-10.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/low-rank-matrix/" rel="permalink">Low-rank matrices: using structure to recover missing data
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2021-09-26T00:00:00-05:00">September 26, 2021</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        10 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">A lot of data is naturally of ‘low rank’. I will explain what this means, and how to exploit this fact.
</p>
  </article>
</div>

  
    



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/python_docx/doc_comparison.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/python-docx/" rel="permalink">How to edit Microsoft Word documents in Python
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2021-08-29T00:00:00-05:00">August 29, 2021</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        6 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">Parsing and editing Word documents automatically can be extremely useful, but doing it in Python is not that straightforward.
</p>
  </article>
</div>

  
    



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/st-vitus-deblurred.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/deconvolution-part4/" rel="permalink">Blind deconvolution #4: Blind deconvolution
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2021-05-31T00:00:00-05:00">May 31, 2021</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        6 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">Finally, let’s look at how we can automatically sharpen images, without knowing how they were blurred in the first place.
</p>
  </article>
</div>

  
    



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/cow-weird-blur.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/deconvolution-part3/" rel="permalink">Blind Deconvolution #3: More about non-blind deconvolution
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2021-05-02T00:00:00-05:00">May 2, 2021</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        5 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">Deconvolving and sharpening images is actually pretty tricky. Let’s have a look at some more advanced methods for deconvolution.
</p>
  </article>
</div>

  
    



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/st-vitus-laplace.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/deconvolution-part2/" rel="permalink">Blind Deconvolution #2: Image Priors
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2021-04-09T00:00:00-05:00">April 9, 2021</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        10 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">In order to automatically sharpen images, we need to first understand how a computer can judge how ‘natural’ an image looks.
</p>
  </article>
</div>

  
    



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/st-vitus-blur.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/deconvolution-part1/" rel="permalink">Blind Deconvolution #1: Non-blind Deconvolution
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2021-03-13T00:00:00-06:00">March 13, 2021</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        6 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">Deconvolution is one of the cornerstones of image processing. Let’s take a look at how it works.
</p>
  </article>
</div>

  
</div>

<!-- <ul class="taxonomy__index">
  
  
    <li>
      <a href="#2022">
        <strong>2022</strong> <span class="taxonomy__count">2</span>
      </a>
    </li>
  
    <li>
      <a href="#2021">
        <strong>2021</strong> <span class="taxonomy__count">7</span>
      </a>
    </li>
  
    <li>
      <a href="#2020">
        <strong>2020</strong> <span class="taxonomy__count">7</span>
      </a>
    </li>
  
</ul> -->
<!-- 





  <section id="2022" class="taxonomy__section">
    <h2 class="archive__subtitle">2022</h2>
    <div class="entries-grid">
      
        



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/gmres-teaser.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/gmres/" rel="permalink">GMRES: or how to do fast linear algebra
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2022-03-10T00:00:00-06:00">March 10, 2022</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        16 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">Linear least-squares system pop up everywhere, and there are many fast way to solve them. We’ll be looking at one such way: GMRES.
</p>
  </article>
</div>

      
        



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/discrete-function-tensor.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/discrete-function-tensor/" rel="permalink">Machine learning with discretized functions and tensors
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2022-03-10T00:00:00-06:00">March 10, 2022</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        18 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">We recently made a paper about supervised machine learning using tensors, here’s the gist of how this works.
</p>
  </article>
</div>

      
    </div>
    <a href="#page-title" class="back-to-top">Back to top &uarr;</a>
  </section>

  <section id="2021" class="taxonomy__section">
    <h2 class="archive__subtitle">2021</h2>
    <div class="entries-grid">
      
        



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/st-vitus-rank-10.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/low-rank-matrix/" rel="permalink">Low-rank matrices: using structure to recover missing data
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2021-09-26T00:00:00-05:00">September 26, 2021</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        10 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">A lot of data is naturally of ‘low rank’. I will explain what this means, and how to exploit this fact.
</p>
  </article>
</div>

      
        



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/python_docx/doc_comparison.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/python-docx/" rel="permalink">How to edit Microsoft Word documents in Python
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2021-08-29T00:00:00-05:00">August 29, 2021</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        6 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">Parsing and editing Word documents automatically can be extremely useful, but doing it in Python is not that straightforward.
</p>
  </article>
</div>

      
        



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/st-vitus-deblurred.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/deconvolution-part4/" rel="permalink">Blind deconvolution #4: Blind deconvolution
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2021-05-31T00:00:00-05:00">May 31, 2021</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        6 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">Finally, let’s look at how we can automatically sharpen images, without knowing how they were blurred in the first place.
</p>
  </article>
</div>

      
        



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/cow-weird-blur.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/deconvolution-part3/" rel="permalink">Blind Deconvolution #3: More about non-blind deconvolution
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2021-05-02T00:00:00-05:00">May 2, 2021</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        5 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">Deconvolving and sharpening images is actually pretty tricky. Let’s have a look at some more advanced methods for deconvolution.
</p>
  </article>
</div>

      
        



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/st-vitus-laplace.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/deconvolution-part2/" rel="permalink">Blind Deconvolution #2: Image Priors
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2021-04-09T00:00:00-05:00">April 9, 2021</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        10 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">In order to automatically sharpen images, we need to first understand how a computer can judge how ‘natural’ an image looks.
</p>
  </article>
</div>

      
        



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/st-vitus-blur.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/deconvolution-part1/" rel="permalink">Blind Deconvolution #1: Non-blind Deconvolution
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2021-03-13T00:00:00-06:00">March 13, 2021</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        6 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">Deconvolution is one of the cornerstones of image processing. Let’s take a look at how it works.
</p>
  </article>
</div>

      
        



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/email-time-series.jpg" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/email-time-series/" rel="permalink">Time series analysis of my email traffic
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2021-02-13T00:00:00-06:00">February 13, 2021</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        6 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">I have 15 years worth of email traffic data, let’s take a closer look and discover some fascinating patterns.
</p>
  </article>
</div>

      
    </div>
    <a href="#page-title" class="back-to-top">Back to top &uarr;</a>
  </section>

  <section id="2020" class="taxonomy__section">
    <h2 class="archive__subtitle">2020</h2>
    <div class="entries-grid">
      
        



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/music-2020.jpg" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/music_2020/" rel="permalink">2020 in music
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2020-12-31T00:00:00-06:00">December 31, 2020</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        4 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">2020 was a great year for music, I will look back and give some thoughts on the best albums that came out in 20202.
</p>
  </article>
</div>

      
        



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/bayes-exam.jpg" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/bayes_exam/" rel="permalink">Modeling uncertainty in exam scores
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2020-11-09T00:00:00-06:00">November 9, 2020</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        3 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">We use exams to determine how much a student knows, but exams aren’t perfect. How can we estimate the uncertainty in students’ exams scores?
</p>
  </article>
</div>

      
        



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/validation-data.jpg" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/validation-size/" rel="permalink">How big should my validation set be?
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2020-08-26T00:00:00-05:00">August 26, 2020</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        13 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">Cross validation is extremely important, but how should we choose the size of our validation and test sets?
</p>
  </article>
</div>

      
        



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/lastfm.jpg" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/lastfm/" rel="permalink">How do my music preferences evolve?
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2020-08-12T00:00:00-05:00">August 12, 2020</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        5 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">I use last.fm to track my music listening. Let’s look at my data to discover how my musical preferences evolve over time.
</p>
  </article>
</div>

      
        



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/normal-data.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/normal-data/" rel="permalink">Is my data normal?
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2020-08-10T00:00:00-05:00">August 10, 2020</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        6 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">Normally distributed data is great, but how do you know whether your data is normally distributed?
</p>
  </article>
</div>

      
        



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/figure-skating.jpg" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/figure-skating/" rel="permalink">Bias in figure skating judging
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2020-06-20T00:00:00-05:00">June 20, 2020</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        3 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">Judging in figure skating is biased. Let’s use data science to figure out just how bad the issue is.
</p>
  </article>
</div>

      
        



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/first-post.jpg" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/first-post/" rel="permalink">First post
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2020-06-19T07:19:44-05:00">June 19, 2020</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        less than 1 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">My first post in this blog
</p>
  </article>
</div>

      
    </div>
    <a href="#page-title" class="back-to-top">Back to top &uarr;</a>
  </section>
 -->

  </div>
</div>
    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    <!-- 
      <li><strong>Contact info and socials</strong></li>
     -->

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2022 Rik Voorhaar. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>







  <script>
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'G-Q5W21THKW2']);
  
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>









  </body>
</html>
