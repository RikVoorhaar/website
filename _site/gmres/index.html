<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.20.1 by Michael Rose
  Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<!-- <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> -->

<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>GMRES: or how to do fast linear algebra - Rik Voorhaar</title>
<meta name="description" content="Linear least-squares system pop up everywhere, and there are many fast way to solve them. We’ll be looking at one such way: GMRES.">


  <meta name="author" content="Dr. Rik Voorhaar">


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Rik Voorhaar">
<meta property="og:title" content="GMRES: or how to do fast linear algebra">
<meta property="og:url" content="https://blog.rikvoorhaar.com/gmres/">


  <meta property="og:description" content="Linear least-squares system pop up everywhere, and there are many fast way to solve them. We’ll be looking at one such way: GMRES.">



  <meta property="og:image" content="https://blog.rikvoorhaar.com/imgs/teasers/gmres-teaser.png">





  <meta property="article:published_time" content="2022-03-10T00:00:00-06:00">





  

  


<link rel="canonical" href="https://blog.rikvoorhaar.com/gmres/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Rik Voorhaar",
      "url": "https://blog.rikvoorhaar.com/"
    
  }
</script>






<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Rik Voorhaar Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<!-- <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"> -->
<script src="https://kit.fontawesome.com/ca9a31e360.js" crossorigin="anonymous"></script>


<!--[if IE]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->
<link rel="shortcut icon" type="image/x-icon" href="/favicon.ico">

<!-- end custom head snippets -->
<link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link href="https://fonts.googleapis.com/css2?family=Bitter:wght@500;600;700;800&family=Open+Sans:ital,wght@0,300;0,400;0,500;0,600;0,700;0,800;1,300;1,400;1,500;1,600;1,700;1,800&display=swap" rel="stylesheet">
  </head>

  <body class="layout--posts">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/"><img src="/assets/images/logo_name.svg" alt=""></a>
        
        <a class="site-title" href="/">
           
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/cv/">CV</a>
            </li><li class="masthead__menu-item">
              <a href="/">Blog</a>
            </li><li class="masthead__menu-item">
              <a href="/about/">Hobbies</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      <div class="page__hero-background"> </div>





<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  
    <div class="author__avatar">
      
        <img src="/imgs/avatar.jpg" alt="Dr. Rik Voorhaar" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Dr. Rik Voorhaar</h3>
    
    
      <div class="author__bio" itemprop="description">
        <p>ML Software Developer and curious mind</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Contact info and socials</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">Copenhagen, Denmark</span>
        </li>
      

      
        
          
            <li><a href="mailto:rik.voorhaar@gmail.com" rel="nofollow noopener noreferrer"><i class="far fa-fw fa-envelope" aria-hidden="true"></i><span class="label">rik.voorhaar@gmail.com</span></a></li>
          
        
          
            <li><a href="https://github.com/RikVoorhaar" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
            <li><a href="https://www.facebook.com/WH.Voorhaar" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span class="label">Facebook</span></a></li>
          
        
          
            <li><a href="https://discord.com/users/354966598643220480" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-discord" aria-hidden="true"></i><span class="label">Discord</span></a></li>
          
        
          
            <li><a href="https://www.linkedin.com/in/rik-voorhaar" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span class="label">LinkedIn</span></a></li>
          
        
          
            <li><a href="https://www.last.fm/user/Tilpo" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-lastfm-square" aria-hidden="true"></i><span class="label">Last.fm</span></a></li>
          
        
          
            <li><a href="https://www.goodreads.com/user/show/62542056-tilpo" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-goodreads" aria-hidden="true"></i><span class="label">Goodreads</span></a></li>
          
        
          
            <li><a href="https://steamcommunity.com/profiles/76561197996562422" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-steam" aria-hidden="true"></i><span class="label">Steam</span></a></li>
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <div class="archive">
    <br>
    
      <h1 id="page-title" class="page__title">GMRES: or how to do fast linear algebra</h1>
    
    <p>Linear algebra is the foundation of modern science, and the fact that computers can do linear algebra <em>very
fast</em> is one of the primary reasons modern algorithms work so well in practice. In this blog post we will dive
into some of the principles of fast numerical linear algebra, and learn how to solve least-squares problems
using the GMRES algorithm. We apply this to the deconvolution problem, which we already discussed at length in
previous blog posts.</p>

<h2 id="linear-least-squares-problem">Linear least-squares problem</h2>

<p>The linear least-squares problem is one of the most common minimization problems we encounter. It takes the following form:</p>

\[\min_x \|Ax-b\|^2\]

<p>Here \(A\) is an \(n\times n\) matrix, and \(x,b\in\mathbb R^{n}\) are vectors. If \(A\) is invertible, then this
problem has a simple, unique solution: \(x = A^{-1}b\). However, there are two big reasons why we should <em>almost never</em>
use \(A^{-1}\) to solve the least-squares problem in practice:</p>
<ol>
  <li>It is expensive to compute \(A^{-1}\).</li>
  <li>This solution numerically unstable.</li>
</ol>

<p>Assuming \(A\) doesn’t have any useful structure, point 1. is not that bad. Solving the least-squares problem in
a smart way costs \(O(n^3)\), and doing it using matrix-inversion also costs \(O(n^3)\), just with a larger hidden
constant. The real killer is the instability. To see this in action, let’s take a matrix that is <em>almost
singular</em>, and see what happens when we solve the least-squares problem.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">179</span><span class="p">)</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">20</span>

<span class="c1"># Create almost singular matrix
</span><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">eye</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">A</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1e-20</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">A</span> <span class="o">@</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">A</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Random vector b
</span><span class="n">b</span> <span class="o">=</span> <span class="n">A</span> <span class="o">@</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,))</span> <span class="o">+</span> <span class="mf">1e-3</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>

<span class="c1"># Solve least-squares with inverse
</span><span class="n">A_inv</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">inv</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">A_inv</span> <span class="o">@</span> <span class="n">b</span>
<span class="n">error</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">A</span> <span class="o">@</span> <span class="n">x</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"error for matrix inversion method: </span><span class="si">{</span><span class="n">error</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">e</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>


<span class="c1"># Solve least-squares with dedicated routine
</span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">rcond</span><span class="o">=</span><span class="bp">None</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">error</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">A</span> <span class="o">@</span> <span class="n">x</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"error for dedicated method: </span><span class="si">{</span><span class="n">error</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">e</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>error for matrix inversion method: 3.6223e+02
error for dedicated method: 2.8275e-08
</code></pre></div></div>

<p>In this case we took a 20x20 matrix \(A\) with ones on the diagonals, except for one entry where it has value
<code class="language-plaintext highlighter-rouge">1e-20</code>, and then we shuffled everything around by multiplying by a random matrix. The entries of \(A\) are
not so big, but the entries of \(A^{-1}\) will be <em>gigantic</em>. This results in the fact that the solution
obtained as \(x=A^{-1}b\) does not satisfy \(Ax=b\) in practice. The solution found by using the <code class="language-plaintext highlighter-rouge">np.linalg.lstsq</code>
routine is much better.</p>

<p>The reason that the inverse-matrix method fails badly in this case can be summarized using the <em>condition
number</em> \(\kappa(A)\). It expresses how much the error \(\|Ax-b\|\) with \(x=A^{-1}b\) is going to change if we
change \(b\) slightly, in the worst case. The condition number gives a notion of how much numerical errors get
amplified when we solve the linear system. We can compute it as the ratio between the smallest and largest
singular values of the matrix \(A\):</p>

\[\kappa(A) = \sigma_1(A) / \sigma_n(A)\]

<p>In the case above the condition number is really big:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">cond</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1.1807555508404976e+16
</code></pre></div></div>

<p>Large condition numbers mean that <em>any</em> numerical method is going to struggle to give a good solution, but for
numerically unstable methods the problem is a lot worse.</p>

<h2 id="using-structure">Using structure</h2>

<p>While the numerical stability of algorithms is a fascinating topic, it is not what we came here for today.
Instead, let’s revisit the first reason why using matrix inversion for solving linear problems is bad. I
mentioned that matrix inversion and better alternatives take \(O(n^3)\) to solve the least squares problem
\(\min_a\|Ax-b\|^2\), <em>if there is no extra structure on</em> \(A\) <em>that we can exploit</em>.</p>

<p>What if there <em>is</em> such structure? For example, what if \(A\) is a huge sparse matrix? For example the Netflix
dataset we considered <a href="/low-rank-matrix/">in this blog post</a> is of size 480,189 x 17,769. Putting aside the
fact that it is not square, inverting matrices of that kind of size is infeasible. Moreover, the inverse
matrix isn’t necessarily sparse anymore, so we lose that valuable structure as well.</p>

<p>Another example arose in my <a href="/deconvolution-part1/">first post on deconvolution</a>. There we tried to solve the linear problem</p>

\[\min_x \|k * x -y\|^2\]

<p>where \(k * x\) denotes <em>convolution</em>. Convolution is a linear operation, but requires only \(O(n\log n)\) to
compute, whereas writing it out as a matrix would require \(n\times n\) entries, which can quickly become too
large.</p>

<p>In situations like this, we have no choice but to devise an algorithm that makes use of the structure of \(A\).
What the two situations above have in common is that storing \(A\) as a dense matrix is expensive, but computing
matrix-vector products \(Ax\) is cheap. The algorithm we are going to come up with is going to be <em>iterative</em>;
we start with some initial guess \(x_0\), and then improve it until we find a solution of the desired accuracy.</p>

<p>We don’t have much to work with; we have a vector \(x_0\) and the ability fo compute matrix-vector products.
Crucially, we assumed our matrix \(A\) is <em>square</em>. This means that \(x_0\) and \(Ax_0\) have the same shape, and
therefore we can also compute \(A^2x_0\), or in fact \(A^rx_0\) for any \(r\). The idea is then to try to express
the solution to the least-squares problem as linear combination of the vectors</p>

\[\mathcal K_r(A,x_0):=\{x_0, Ax_0,A^2x_0,\ldots,A^{r-1}x_0\}.\]

<p>This results in a class of algorithms known as <em>Krylov subspace methods</em>. Before diving further into how they work, let’s see one in action. We take a 2500 x 2500 sparse matrix with 5000 non-zero entries (which includes the entire diagonal).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">scipy.sparse</span>
<span class="kn">import</span> <span class="nn">scipy.sparse.linalg</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">perf_counter_ns</span>

<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">179</span><span class="p">)</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">2500</span>
<span class="n">N</span> <span class="o">=</span> <span class="n">n</span>
<span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>

<span class="c1"># Create random sparse (n, n) matrix with N non-zero entries
</span><span class="n">coords</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="n">n</span> <span class="o">*</span> <span class="n">n</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">coords</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">unravel_index</span><span class="p">(</span><span class="n">coords</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>
<span class="n">values</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>
<span class="n">A_sparse</span> <span class="o">=</span> <span class="n">scipy</span><span class="p">.</span><span class="n">sparse</span><span class="p">.</span><span class="n">coo_matrix</span><span class="p">((</span><span class="n">values</span><span class="p">,</span> <span class="n">coords</span><span class="p">),</span> <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">)</span>
<span class="n">A_sparse</span> <span class="o">=</span> <span class="n">A_sparse</span><span class="p">.</span><span class="n">tocsr</span><span class="p">()</span>
<span class="n">A_sparse</span> <span class="o">+=</span> <span class="n">scipy</span><span class="p">.</span><span class="n">sparse</span><span class="p">.</span><span class="n">eye</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">A_dense</span> <span class="o">=</span> <span class="n">A_sparse</span><span class="p">.</span><span class="n">toarray</span><span class="p">()</span>

<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">A_sparse</span> <span class="o">@</span> <span class="n">b</span>

<span class="c1"># Solve using np.linalg.lstsq
</span><span class="n">time_before</span> <span class="o">=</span> <span class="n">perf_counter_ns</span><span class="p">()</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">A_dense</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">rcond</span><span class="o">=</span><span class="bp">None</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">time_taken</span> <span class="o">=</span> <span class="p">(</span><span class="n">perf_counter_ns</span><span class="p">()</span> <span class="o">-</span> <span class="n">time_before</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1e-6</span>
<span class="n">error</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">A_dense</span> <span class="o">@</span> <span class="n">x</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Using dense solver: error: </span><span class="si">{</span><span class="n">error</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">e</span><span class="si">}</span><span class="s"> in time </span><span class="si">{</span><span class="n">time_taken</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s">ms"</span><span class="p">)</span>

<span class="c1"># Solve using inverse matrix
</span><span class="n">time_before</span> <span class="o">=</span> <span class="n">perf_counter_ns</span><span class="p">()</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">inv</span><span class="p">(</span><span class="n">A_dense</span><span class="p">)</span> <span class="o">@</span> <span class="n">x</span>
<span class="n">time_taken</span> <span class="o">=</span> <span class="p">(</span><span class="n">perf_counter_ns</span><span class="p">()</span> <span class="o">-</span> <span class="n">time_before</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1e-6</span>
<span class="n">error</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">A_dense</span> <span class="o">@</span> <span class="n">x</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Using matrix inversion: error: </span><span class="si">{</span><span class="n">error</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">e</span><span class="si">}</span><span class="s"> in time </span><span class="si">{</span><span class="n">time_taken</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s">ms"</span><span class="p">)</span>

<span class="c1"># Solve using GMRES
</span><span class="n">time_before</span> <span class="o">=</span> <span class="n">perf_counter_ns</span><span class="p">()</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">scipy</span><span class="p">.</span><span class="n">sparse</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">gmres</span><span class="p">(</span><span class="n">A_sparse</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">time_taken</span> <span class="o">=</span> <span class="p">(</span><span class="n">perf_counter_ns</span><span class="p">()</span> <span class="o">-</span> <span class="n">time_before</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1e-6</span>
<span class="n">error</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">A_sparse</span> <span class="o">@</span> <span class="n">x</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Using sparse solver: error: </span><span class="si">{</span><span class="n">error</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">e</span><span class="si">}</span><span class="s"> in time </span><span class="si">{</span><span class="n">time_taken</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s">ms"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Using dense solver: error: 1.4449e-25 in time 2941.5ms
Using matrix inversion: error: 2.4763e+03 in time 507.0ms
Using sparse solver: error: 2.5325e-13 in time 6.4ms
</code></pre></div></div>

<p>As we see above, the sparse matrix solver solves this problem in a fraction of the time, and the difference is just going to get bigger with larger matrices. Above we use the GMRES routine, and it is very simple. It constructs an orthonormal basis of the Krylov subspace \(\mathcal K_m(A,x_0)\), and then finds the best solution in this subspace by solving a small \((m+1)\times m\) linear system. Before figuring out the details, below is a simple implementation:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">gmres</span><span class="p">(</span><span class="n">linear_map</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">n_iter</span><span class="p">):</span>
    <span class="c1"># Initialization
</span>    <span class="n">n</span> <span class="o">=</span> <span class="n">x0</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">H</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_iter</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_iter</span><span class="p">))</span>
    <span class="n">r0</span> <span class="o">=</span> <span class="n">b</span> <span class="o">-</span> <span class="n">linear_map</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">r0</span><span class="p">)</span>
    <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_iter</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
    <span class="n">V</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">r0</span> <span class="o">/</span> <span class="n">beta</span>

    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iter</span><span class="p">):</span>
        <span class="c1"># Compute next Krylov vector
</span>        <span class="n">w</span> <span class="o">=</span> <span class="n">linear_map</span><span class="p">(</span><span class="n">V</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>

        <span class="c1"># Gram-Schmidt orthogonalization
</span>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">H</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">V</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">w</span> <span class="o">-=</span> <span class="n">H</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">H</span><span class="p">[</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>

        <span class="c1"># Add new vector to basis
</span>        <span class="n">V</span><span class="p">[</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">w</span> <span class="o">/</span> <span class="n">H</span><span class="p">[</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>

    <span class="c1"># Find best approximation in the basis V
</span>    <span class="n">e1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_iter</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">e1</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">beta</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">e1</span><span class="p">,</span> <span class="n">rcond</span><span class="o">=</span><span class="bp">None</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Convert result back to full basis and return
</span>    <span class="n">x_new</span> <span class="o">=</span> <span class="n">x0</span> <span class="o">+</span> <span class="n">V</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span>
    <span class="k">return</span> <span class="n">x_new</span>

<span class="c1"># Try out the GMRES routine
</span><span class="n">time_before</span> <span class="o">=</span> <span class="n">perf_counter_ns</span><span class="p">()</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">linear_map</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">A_sparse</span> <span class="o">@</span> <span class="n">x</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">gmres</span><span class="p">(</span><span class="n">linear_map</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="n">time_taken</span> <span class="o">=</span> <span class="p">(</span><span class="n">perf_counter_ns</span><span class="p">()</span> <span class="o">-</span> <span class="n">time_before</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1e-6</span>
<span class="n">error</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">A_sparse</span> <span class="o">@</span> <span class="n">x</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Using GMRES: error: </span><span class="si">{</span><span class="n">error</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">e</span><span class="si">}</span><span class="s"> in time </span><span class="si">{</span><span class="n">time_taken</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s">ms"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Using GMRES: error: 1.1039e-15 in time 12.9ms
</code></pre></div></div>

<p>This clearly works; it’s not as fast as the <code class="language-plaintext highlighter-rouge">scipy</code> implementation of the same algorithm, but we’ll do something about that soon.</p>

<p>Let’s take a more detailed look at what the GMRES algorithm is doing. We iteratively define an orthonormal basis \(V_m = \{v_0,v_1,\dots,v_{m-1}\}\). We start with \(v_0 = r_0 / \|r_0\|\), where \(r_0 = b-Ax_0\) is the <em>residual</em> of the initial guess \(x_0\). In each iteration we then set \(w = A v_j\), and take \(v_{j+1} = w - \sum_i (w^\top v_{i})v_i\); i.e. we ensure \(v_{j+1}\) is orthogonal to all previous \(v_0,\dots,v_j\). Therefore \(V_m\) is an orthonormal basis of the Krylov subspace \(\mathcal K_m(A,r_0)\).</p>

<p>Once we have this basis, we want to solve the minimization problem:</p>

\[\min_{x\in \mathcal K_m(A,r_0)} \|A(x_0+x)-b\|\]

<p>Since \(V_m\) is a basis, we can write \(x = V_m y\) for some \(y\in \mathbb R^m\). Also note that in this basis \(b-Ax_0 = r_0 = \beta v_0 = \beta V_m e_1\) where \(\beta = \|r_0\|\) and \(e_1= (1,0,\dots,0)\). This allows us to rewrite the minimization problem:</p>

\[\min_{y\in\mathbb R^m} \|AV_my - \beta V_me_1\|\]

<p>To solve this minimization problem we need one more trick. In the algorithm we computed a matrix \(H\), it is defined like this:</p>

\[H_{ij} = v_i^\top (Av_j-\sum_k H_{kj}v_k) = v_i^\top A v_j\]

<p>These are precisely the coefficients of the Gram-Schmidt orthogonalization, and hence \(A v_j = \sum_{i=1}^{j+1} H_{ij}v_i\), giving the matrix equality \(AV_m = HV_m\). Now we can rewrite the minimization problem even further and get</p>

\[\min_{y\in\mathbb R^m} \|V_m (Hy - \beta e_1)\| = \min_{y\in\mathbb R^m} \|Hy - \beta e_1\|\]

<p>The minimization problem is therefore reduced to an \((m+1)\times m\) problem! The cost of this is \(O(m^3)\), and as long as we don’t use too many steps \(m\), this cost can be very reasonable. After solving for \(y\), we then get the estimate \(x = x_0 + V_m y\).</p>

<h2 id="restarting">Restarting</h2>

<p>In the current implementation of GMRES we specify the number of steps in advance, which is not ideal. If we converge to the right solution in less steps, then we are doing unnecessary work. If we don’t get a satisfying solution after the specified number of steps, we might need to start over. This is however not a big problem; we can use the output \(x=x_0+V_my\) as new initialization when we restart.</p>

<p>This gives a nice recipe for <em>GMRES with restarting</em>. We run GMRES for \(m\) steps with \(x_i\) as initialization to get a new estimate \(x_{i+1}\). We then check if \(x_{i+1}\) is good enough, if not, we repeat the GMRES procedure for another \(m\) steps.</p>

<p>It is possible to get a good estimate of the residual norm after <em>each</em> step of GMRES, not just every \(m\) steps. However, this is relatively technical to implement, so we will just consider the variation of GMRES with restarting.</p>

<p>How often should we restart? This really depends on the problem we’re trying to solve, since there is a
trade-off. More steps in between each restart will typically result in convergence in fewer steps, <em>but</em> it is
more expensive and also requires more memory. The computational cost scales as \(O(m^3)\), and the memory cost
scales linearly in \(m\) (if the matrix size \(n\) is much bigger than \(m\)). Let’s see this trade-off in action on a model problem.</p>

<h2 id="deconvolution">Deconvolution</h2>

<p>Recall that the deconvolution problem is of the following form:</p>

\[\min_x \|k * x -y\|^2\]

<p>for a fixed <em>kernel</em> \(k\) and signal \(y\). The convolution operation \(k*x\) is linear in \(x\), and we can
therefore treat this as a linear least-squares problem and solve it using GMRES. The operation \(k*x\) can be
written in matrix form as \(Kx\), where \(K\) is a matrix. For large images or signals, the matrix \(K\) can be
gigantic, and we never want to explicitly store \(K\) in memory. Fortunately, GMRES only cares about
matrix-vector products \(Kx\), making this a very good candidate to solve with GMRES.</p>

<p>Let’s consider the problem of sharpening (deconvolving) a 128x128 picture blurred using Gaussian blur. To make
the problem more interesting, the kernel \(k\) used for deconvolution will be slightly different from the kernel
used for blurring. This is inspired by the blind deconvolution problem, where we not only have to find \(x\),
but also the kernel \(k\) itself.</p>

<p>We solve this problem with GMRES using different number of steps between restarts, and plot how the error
evolves over time.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">image</span>
<span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="n">random_motion_blur</span>
<span class="kn">from</span> <span class="nn">scipy.signal</span> <span class="kn">import</span> <span class="n">convolve2d</span>

<span class="c1"># Define the Gaussian blur kernel
</span><span class="k">def</span> <span class="nf">gaussian_psf</span><span class="p">(</span><span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">9</span><span class="p">):</span>
    <span class="n">gauss_psf</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="n">N</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">N</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">gauss_psf</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">gauss_psf</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">sigma</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">gauss_psf</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">"i,j-&gt;ij"</span><span class="p">,</span> <span class="n">gauss_psf</span><span class="p">,</span> <span class="n">gauss_psf</span><span class="p">)</span>
    <span class="n">gauss_psf</span> <span class="o">=</span> <span class="n">gauss_psf</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">gauss_psf</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gauss_psf</span>


<span class="c1"># Load the image and blur it
</span><span class="n">img</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="n">imread</span><span class="p">(</span><span class="s">"imgs/vitus128.png"</span><span class="p">)</span>
<span class="n">gauss_psf_true</span> <span class="o">=</span> <span class="n">gaussian_psf</span><span class="p">(</span><span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">gauss_psf_almost</span> <span class="o">=</span> <span class="n">gaussian_psf</span><span class="p">(</span><span class="n">sigma</span><span class="o">=</span><span class="mf">1.05</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">img_blur</span> <span class="o">=</span> <span class="n">convolve2d</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">gauss_psf_true</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s">"same"</span><span class="p">)</span>


<span class="c1"># Define the convolution linear map
</span><span class="n">linear_map</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">convolve2d</span><span class="p">(</span>
    <span class="n">x</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">img</span><span class="p">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">gauss_psf_almost</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s">"same"</span>
<span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>


<span class="c1"># Apply GMRES for different restart frequencies and measure time taken
</span><span class="n">total_its</span> <span class="o">=</span> <span class="mi">2000</span>
<span class="n">n_restart_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">500</span><span class="p">]</span>
<span class="n">losses_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
<span class="k">for</span> <span class="n">n_restart</span> <span class="ow">in</span> <span class="n">n_restart_list</span><span class="p">:</span>
    <span class="n">time_before</span> <span class="o">=</span> <span class="n">perf_counter_ns</span><span class="p">()</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">img_blur</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x0</span>

    <span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">total_its</span> <span class="o">//</span> <span class="n">n_restart</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">gmres</span><span class="p">(</span><span class="n">linear_map</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_restart</span><span class="p">)</span>
        <span class="n">error</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">linear_map</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
        <span class="n">losses</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>
    <span class="n">time_taken</span> <span class="o">=</span> <span class="p">(</span><span class="n">perf_counter_ns</span><span class="p">()</span> <span class="o">-</span> <span class="n">time_before</span><span class="p">)</span> <span class="o">/</span> <span class="mf">1e9</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Best loss for </span><span class="si">{</span><span class="n">n_restart</span><span class="si">}</span><span class="s"> restart frequency is </span><span class="si">{</span><span class="n">error</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">e</span><span class="si">}</span><span class="s"> in </span><span class="si">{</span><span class="n">time_taken</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">s"</span><span class="p">)</span>
    <span class="n">losses_dict</span><span class="p">[</span><span class="n">n_restart</span><span class="p">]</span> <span class="o">=</span> <span class="n">losses</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Best loss for 20 restart frequency is 9.3595e-16 in 11.32s
Best loss for 50 restart frequency is 2.4392e-22 in 11.71s
Best loss for 200 restart frequency is 6.3063e-28 in 17.34s
Best loss for 500 restart frequency is 6.9367e-28 in 30.50s
</code></pre></div></div>

<p><img src="/imgs/gmres/gmres_13_0.svg" alt="svg" /></p>

<p>We observe that with all restart frequencies we converge to a result with very low error. The larger the
number of steps between restarts, the faster we converge. Remember however that the cost of GMRES rises as
\(O(m^3)\) with the number of steps \(m\) between restarts, so a larger number of steps is not always better. For
example we see that \(m=20\) and \(m=50\) produced almost identical runtime, but for \(m=200\) the runtime for 2000
total steps is already significantly bigger, and the effect is even bigger for \(m=500\).  This means that if we
want to get converge as fast as possible <em>in terms of runtime</em>, we’re best off with somewhere between \(m=50\)
and \(m=200\) steps between each reset.</p>

<h2 id="gpu-implementation">GPU implementation</h2>

<p>If we do simple profiling, we see that almost all of the time in this function is spent on the 2D convolution.
Indeed this is why the runtime does not seem to scale os \(O(m^3)\) for the values of \(m\) we tried above.
It simply takes a while before the \(O(m^3)\) factor becomes dominant over the time spent by matrix-vector
products.</p>

<p>This also means that it should be straightforward to speed up – we just need to do the convolution on a GPU.
It is not as simple as that however; if we just do the convolution on GPU and the rest of the operations on
CPU, then the bottleneck quickly becomes moving the data between CPU and GPU (unless we are working on a
system where CPU and GPU share memory).</p>

<p>Fortunately the entire GMRES algorithm is not so complex, and we can use hardware acceleration by simply
translating the algorithm to use a fast computational library. There are several such libraries available for
Python:</p>
<ul>
  <li>TensorFlow</li>
  <li>PyTorch</li>
  <li>DASK</li>
  <li>CuPy</li>
  <li>JAX</li>
  <li>Numba</li>
</ul>

<p>In this context CuPy might be the easiest to use; its syntax is very similar to numpy. However, I would
also like to make use of JIT (Just-in-time) compilation, particularly since this can limit unnecessary data
movement. Furthermore, it really depends on the situation which low-level CUDA functions are best called in
different situations (especially for something like convolution), and JIT compilation can offer significant
optimizations here.</p>

<p>TensorFlow, DASK and PyTorch are really focussed on machine-learning and neural networks, and the way we
interact with these libraries might not be the best for this kind of algorithm. In fact, I tried to make an
efficient GMRES implementation using these libraries, and I was really struggling; I feel these libraries
simply aren’t the right tool for this job.</p>

<p>Numba is also great, I could basically feed it the code I already wrote and it would probably compile the
function and make it several times faster <em>on CPU</em>. Unfortunately, the support for GPU is still lacking quite
a bit in Numba, and we would therefore still leave quite a bit of performance on the table.</p>

<p>In the end we will implement it in JAX. Like CuPy, it has an API very similar to numpy which means it’s easy to translation. However, it also supports JIT, meaning we can potentially make much faster functions. Without further
ado, let’s implement the GMRES algorithm in JAX and see what kind of speedup we can get.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="n">jnp</span>
<span class="kn">import</span> <span class="nn">jax</span>

<span class="c1"># Define the linear operator
</span><span class="n">img_shape</span> <span class="o">=</span> <span class="n">img</span><span class="p">.</span><span class="n">shape</span>
<span class="k">def</span> <span class="nf">do_convolution</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">jax</span><span class="p">.</span><span class="n">scipy</span><span class="p">.</span><span class="n">signal</span><span class="p">.</span><span class="n">convolve2d</span><span class="p">(</span>
        <span class="n">x</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">img_shape</span><span class="p">),</span> <span class="n">gauss_psf_almost</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s">"same"</span>
    <span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">gmres_jax</span><span class="p">(</span><span class="n">linear_map</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">n_iter</span><span class="p">):</span>
    <span class="c1"># Initialization
</span>    <span class="n">n</span> <span class="o">=</span> <span class="n">x0</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">r0</span> <span class="o">=</span> <span class="n">b</span> <span class="o">-</span> <span class="n">linear_map</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">r0</span><span class="p">)</span>
    <span class="n">V</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_iter</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
    <span class="n">V</span> <span class="o">=</span> <span class="n">V</span><span class="p">.</span><span class="n">at</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nb">set</span><span class="p">(</span><span class="n">r0</span> <span class="o">/</span> <span class="n">beta</span><span class="p">)</span>
    <span class="n">H</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_iter</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_iter</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">loop_body</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">pair</span><span class="p">):</span>
        <span class="s">"""
        One basic step of GMRES; compute new Krylov vector and orthogonalize.
        """</span>
        <span class="n">H</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">pair</span>

        <span class="n">w</span> <span class="o">=</span> <span class="n">linear_map</span><span class="p">(</span><span class="n">V</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">V</span> <span class="o">@</span> <span class="n">w</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">w</span> <span class="o">-</span> <span class="p">(</span><span class="n">V</span><span class="p">.</span><span class="n">T</span><span class="p">)</span> <span class="o">@</span> <span class="n">h</span>
        <span class="n">v_norm</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>

        <span class="n">H</span> <span class="o">=</span> <span class="n">H</span><span class="p">.</span><span class="n">at</span><span class="p">[:,</span> <span class="n">j</span><span class="p">].</span><span class="nb">set</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        <span class="n">H</span> <span class="o">=</span> <span class="n">H</span><span class="p">.</span><span class="n">at</span><span class="p">[</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">j</span><span class="p">].</span><span class="nb">set</span><span class="p">(</span><span class="n">v_norm</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">V</span><span class="p">.</span><span class="n">at</span><span class="p">[</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">].</span><span class="nb">set</span><span class="p">(</span><span class="n">v</span> <span class="o">/</span> <span class="n">v_norm</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">H</span><span class="p">,</span> <span class="n">V</span>

    <span class="c1"># Do n_iter iterations of basic GMRES step
</span>    <span class="n">H</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="n">fori_loop</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_iter</span><span class="p">,</span> <span class="n">loop_body</span><span class="p">,</span> <span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">V</span><span class="p">))</span>

    <span class="c1"># Solve the linear system in the basis V
</span>    <span class="n">e1</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_iter</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">e1</span> <span class="o">=</span> <span class="n">e1</span><span class="p">.</span><span class="n">at</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nb">set</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">e1</span><span class="p">,</span> <span class="n">rcond</span><span class="o">=</span><span class="bp">None</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Convert result back to full basis and return
</span>    <span class="n">x_new</span> <span class="o">=</span> <span class="n">x0</span> <span class="o">+</span> <span class="n">V</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span>
    <span class="k">return</span> <span class="n">x_new</span>


<span class="n">b</span> <span class="o">=</span> <span class="n">img_blur</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x0</span>
<span class="n">n_restart</span> <span class="o">=</span> <span class="mi">50</span>

<span class="c1"># Declare JIT compiled version of gmres_jax
</span><span class="n">gmres_jit</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">jit</span><span class="p">(</span><span class="n">gmres_jax</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Compiling function:"</span><span class="p">)</span>
<span class="o">%</span><span class="n">time</span> <span class="n">x</span> <span class="o">=</span> <span class="n">gmres_jit</span><span class="p">(</span><span class="n">do_convolution</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">n_restart</span><span class="p">).</span><span class="n">block_until_ready</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">Profiling functions. numpy version:"</span><span class="p">)</span>
<span class="o">%</span><span class="n">timeit</span> <span class="n">x</span> <span class="o">=</span> <span class="n">gmres</span><span class="p">(</span><span class="n">linear_map</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">n_restart</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">Profiling functions. JAX version:"</span><span class="p">)</span>
<span class="o">%</span><span class="n">timeit</span> <span class="n">x</span> <span class="o">=</span> <span class="n">gmres_jit</span><span class="p">(</span><span class="n">do_convolution</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">n_restart</span><span class="p">).</span><span class="n">block_until_ready</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Compiling function:
CPU times: user 1.94 s, sys: 578 ms, total: 2.51 s
Wall time: 2.01 s

Profiling functions. numpy version:
263 ms ± 25.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

Profiling functions. JAX version:
9.16 ms ± 90.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
</code></pre></div></div>

<p>With the JAX version running on my GPU, we get a 30x times speedup! Not bad, if you ask me. If we run the same
code on CPU, we still get a 4x speedup. This means that the version compiled by JAX is already faster in its
own right.</p>

<p>The code above may look a bit strange, and there are definitely some things that might need some explanation.
First of all, note that the first time we call <code class="language-plaintext highlighter-rouge">gmres_jit</code> it takes much longer than the subsequent calls.
This is because the function is JIT – just in time compiled. On the first call, JAX runs through the entire
function and makes a big graph of all the operations that need to be done, it then optimizes (simplifies) this
graph, and compiles it to create a very fast function. This compilation step obviously takes some time, but
the great thing is that we only need to do it once.</p>

<p>Note the way we create the function <code class="language-plaintext highlighter-rouge">gmres_jit</code>:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="n">gmres_jit</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">jit</span><span class="p">(</span><span class="n">gmres_jax</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
</code></pre></div></div>
<p>Here we tell JAX that if the first or the fourth argument changes, the function needs to be recompiled. This
is because both these arguments are python literals (the first is a function, the fourth is the number of
iterations), whereas the other two arguments are arrays.</p>

<p>The shape of the arrays <code class="language-plaintext highlighter-rouge">V</code> and <code class="language-plaintext highlighter-rouge">H</code> depend on the last argument <code class="language-plaintext highlighter-rouge">n_iter</code>. However, the compiler needs to know the shape of these arrays <em>at compile time</em>. Therefore, we need to recompile the function every time that <code class="language-plaintext highlighter-rouge">n_iter</code> changes. The same is true for the <code class="language-plaintext highlighter-rouge">linear_map</code> argument; the
shape of the vector <code class="language-plaintext highlighter-rouge">w</code> depends on <code class="language-plaintext highlighter-rouge">linear_map</code> in principle.</p>

<p>Next, consider the fact that there is no more <code class="language-plaintext highlighter-rouge">for</code> loop in the code, and it is instead replaced by</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="n">H</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="n">fori_loop</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_iter</span><span class="p">,</span> <span class="n">loop_body</span><span class="p">,</span> <span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">V</span><span class="p">))</span>
</code></pre></div></div>
<p>We could in fact use a for loop here as well, and it would give an identical result but it would take much
longer to compile. The reason for this is that, as mentioned, JAX runs through the entire function and makes a
graph of all the operations that need to be done. If we leave in the for loop, then each iteration of the loop
would add more and more operations to the graph (the loop is ‘unrolled’), making a really big graph. By using
<code class="language-plaintext highlighter-rouge">jax.lax.fori_loop</code> we can skip this, and end up with a much smaller graph to be compiled.</p>

<p>One disadvantage of this approach is that the size of all arrays needs to be known at compile time. In the
original algorithm we did not compute <code class="language-plaintext highlighter-rouge">(V.T) @ h</code> for example, but rather <code class="language-plaintext highlighter-rouge">(V[:j+1].T) @ h</code>. Now we can’t do
that, because the size of <code class="language-plaintext highlighter-rouge">V[:j+1]</code> is not known at compile time. The end result ends up being the same
because at iteration <code class="language-plaintext highlighter-rouge">j</code>, we have <code class="language-plaintext highlighter-rouge">V[j+1:] = 0</code>. This actually means that over all the iterations of <code class="language-plaintext highlighter-rouge">j</code> we
end up doing about double the work for this particular operation. However, because the operation is so much
faster on a GPU this is not a big problem.</p>

<p>As we can see, writing code for GPUs requires a bit more thought than writing code for CPUs. Sometimes we even
end up with less efficient code, but this can be entirely offset by the improved speed of the GPU.</p>

<h2 id="condition-numbers-and-eigenvalues">Condition numbers and eigenvalues</h2>

<p>We see above that GMRES provides a very fast and accurate solution to the deconvolution problem. This has a lot to do with the fact that the convolution matrix is very well-conditioned. We can see this by looking at the singular of this matrix. The convolution matrix for a 128x128 image is a bit too big to work with, but we can see what happens for 32x32 images.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">N</span> <span class="o">=</span> <span class="mi">11</span>
<span class="n">psf</span> <span class="o">=</span> <span class="n">gaussian_psf</span><span class="p">(</span><span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>
<span class="n">img_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">create_conv_mat</span><span class="p">(</span><span class="n">psf</span><span class="p">,</span> <span class="n">img_shape</span><span class="p">):</span>
    <span class="n">tot_dim</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">prod</span><span class="p">(</span><span class="n">img_shape</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">apply_psf</span><span class="p">(</span><span class="n">signal</span><span class="p">):</span>
        <span class="n">signal</span> <span class="o">=</span> <span class="n">signal</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">img_shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">convolve2d</span><span class="p">(</span><span class="n">signal</span><span class="p">,</span> <span class="n">psf</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s">"same"</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">conv_mat</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">tot_dim</span><span class="p">,</span> <span class="n">tot_dim</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">tot_dim</span><span class="p">):</span>
        <span class="n">signal</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">tot_dim</span><span class="p">)</span>
        <span class="n">signal</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">conv_mat</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">apply_psf</span><span class="p">(</span><span class="n">signal</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">conv_mat</span>

<span class="n">conv_mat</span> <span class="o">=</span> <span class="n">create_conv_mat</span><span class="p">(</span><span class="n">psf</span><span class="p">,</span> <span class="n">img_shape</span><span class="p">)</span>
<span class="n">svdvals</span> <span class="o">=</span> <span class="n">scipy</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">svdvals</span><span class="p">(</span><span class="n">conv_mat</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">svdvals</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">yscale</span><span class="p">(</span><span class="s">'log'</span><span class="p">)</span>
<span class="n">cond_num</span> <span class="o">=</span> <span class="n">svdvals</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">svdvals</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s">"Singular values. Condition number: </span><span class="si">{</span><span class="n">cond_num</span><span class="si">:</span><span class="p">.</span><span class="mi">0</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/imgs/gmres/gmres_19_1.svg" alt="svg" /></p>

<p>As we can see, the condition number is only 4409, which makes the matrix very well-conditioned. Moreover, the
singular values decay somewhat gradually. What’s more, the convolution matrix is actually symmetric and
positive definite. This makes the linear system relatively easy to solve, and explains why it works so well.</p>

<p>This is because the kernel we use – the Gaussian kernel – is itself symmetric. For a non-symmetric kernel,
the situation is more complicated. Below we show what happens for a non-symmetric kernel, the same type as we
used before in the blind deconvolution series of blog posts.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="n">random_motion_blur</span>

<span class="n">N</span> <span class="o">=</span> <span class="mi">11</span>
<span class="n">psf_gaussian</span> <span class="o">=</span> <span class="n">gaussian_psf</span><span class="p">(</span><span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>
<span class="n">psf</span> <span class="o">=</span> <span class="n">random_motion_blur</span><span class="p">(</span>
    <span class="n">N</span><span class="o">=</span><span class="n">N</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.98</span><span class="p">,</span> <span class="n">vel_scale</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>
<span class="n">img_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>

<span class="c1"># plot the kernels
</span><span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">psf_gaussian</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Gaussian kernel"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">psf</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Non-symmetric kernel"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># study convolution matrix
</span><span class="n">conv_mat</span> <span class="o">=</span> <span class="n">create_conv_mat</span><span class="p">(</span><span class="n">psf</span><span class="p">,</span> <span class="n">img_shape</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">eigs</span> <span class="o">=</span> <span class="n">scipy</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">eigvals</span><span class="p">(</span><span class="n">conv_mat</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s">"Eigenvalues"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"Imaginary part"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"Real part"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">real</span><span class="p">(</span><span class="n">eigs</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">imag</span><span class="p">(</span><span class="n">eigs</span><span class="p">),</span> <span class="n">marker</span><span class="o">=</span><span class="s">"."</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/imgs/gmres/gmres_21_0.svg" alt="svg" /></p>

<p><img src="/imgs/gmres/gmres_21_2.svg" alt="svg" /></p>

<p>We see that the eigenvalues of this convolution matrix are distributed <em>around</em> zero. The convolution matrix for the gaussian kernel is symmetric and positive definite – all eigenvalues are positive real numbers. GMRES works really well when almost all eigenvalues lie in an ellipse <em>not containing zero</em>. That is clearly not the case here, and we in fact also see that GMRES doesn’t work well for this particular problem.
(Note that we now switch to 256x256 images instead of 128x128, since our new implementation of GMRES is much faster)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">img</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="n">imread</span><span class="p">(</span><span class="s">"imgs/vitus256.png"</span><span class="p">)</span>
<span class="n">psf</span> <span class="o">=</span> <span class="n">random_motion_blur</span><span class="p">(</span>
    <span class="n">N</span><span class="o">=</span><span class="n">N</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.98</span><span class="p">,</span> <span class="n">vel_scale</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>

<span class="n">img_blur</span> <span class="o">=</span> <span class="n">convolve2d</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">psf</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s">"same"</span><span class="p">)</span>
<span class="n">img_shape</span> <span class="o">=</span> <span class="n">img</span><span class="p">.</span><span class="n">shape</span>


<span class="k">def</span> <span class="nf">do_convolution</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">scipy</span><span class="p">.</span><span class="n">signal</span><span class="p">.</span><span class="n">convolve2d</span><span class="p">(</span>
        <span class="n">x</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">img_shape</span><span class="p">),</span> <span class="n">psf</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s">"same"</span>
    <span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span>


<span class="n">b</span> <span class="o">=</span> <span class="n">img_blur</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x0</span>
<span class="n">n_restart</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">n_its</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_its</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">gmres_jit</span><span class="p">(</span><span class="n">do_convolution</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_restart</span><span class="p">)</span>
    <span class="n">error</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">do_convolution</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="n">losses</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/imgs/gmres/gmres_24_1.svg" alt="svg" /></p>

<p>Not does it take much more iterations to converge, the final result is unsatisfactory at best. Clearly without
further modifications the GMRES method doesn’t work well for deconvolution for non-symmetric kernels.</p>

<h2 id="changing-the-spectrum">Changing the spectrum</h2>

<p>As mentioned, GMRES works best when the eigenvalues of the matrix \(A\) are in an ellipse not including zero, which is not the case for our convolution matrix. There is fortunately a very simple solution to this: instead of solving the linear least-squares problem</p>

\[\min_x \|Ax - b\|_2^2\]

<p>We solve the linear least squares problem</p>

\[\min_x \|A^\top A x - A^\top b\|^2\]

<p>This will have the same solution, but the eigenvalues of \(A^\top A\) are better behaved. Any matrix like this will be positive semi-definite, and all eigenvalues will be real and non-negative. They therefore all fit inside an ellipse that doesn’t include zero, and we will get much better convergence with GMRES. In general, we could multiply by any matrix \(B\) to obtain the linear least-squares problem</p>

\[\min_x \|BAX-Bb\|^2\]

<p>If we choose \(B\) such that the spectrum (eigenvalues) of \(BA\) are nicer, then we can improve convergence of GMRES. This trick is called <em>preconditioning</em>. Choosing a good <em>preconditioner</em> depends a lot on the problem at hand, and is the subject of a lot of research. In this context, \(A^\top\) turns out to function as an excellent preconditioner, as we shall see.</p>

<p>To apply this trick to the deconvolution problem, we need to be able to take the transpose of the convolution
operation. Fortunately, this is equivalent to convolution with a reflected version \(\overline k\) of the kernel
\(k\). That is, we will apply GMRES to the linear least-squares problem</p>

\[\min_x \|\overline k *(k*x) - \overline k * y\|\]

<p>let’s see this in action below.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">img</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="n">imread</span><span class="p">(</span><span class="s">"imgs/vitus256.png"</span><span class="p">)</span>
<span class="n">psf</span> <span class="o">=</span> <span class="n">random_motion_blur</span><span class="p">(</span>
    <span class="n">N</span><span class="o">=</span><span class="n">N</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.98</span><span class="p">,</span> <span class="n">vel_scale</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>

<span class="n">psf_reversed</span> <span class="o">=</span> <span class="n">psf</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">img_blur</span> <span class="o">=</span> <span class="n">convolve2d</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">psf</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s">"same"</span><span class="p">)</span>
<span class="n">img_shape</span> <span class="o">=</span> <span class="n">img</span><span class="p">.</span><span class="n">shape</span>


<span class="k">def</span> <span class="nf">do_convolution</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">scipy</span><span class="p">.</span><span class="n">signal</span><span class="p">.</span><span class="n">convolve2d</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">img_shape</span><span class="p">),</span> <span class="n">psf</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s">"same"</span><span class="p">)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">scipy</span><span class="p">.</span><span class="n">signal</span><span class="p">.</span><span class="n">convolve2d</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">psf_reversed</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s">"same"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>


<span class="n">b</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">scipy</span><span class="p">.</span><span class="n">signal</span><span class="p">.</span><span class="n">convolve2d</span><span class="p">(</span><span class="n">img_blur</span><span class="p">,</span> <span class="n">psf_reversed</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s">"same"</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x0</span>
<span class="n">n_restart</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">n_its</span> <span class="o">=</span> <span class="mi">20</span>

<span class="c1"># run once to compile
</span><span class="n">gmres_jit</span><span class="p">(</span><span class="n">do_convolution</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_restart</span><span class="p">)</span>

<span class="n">time_start</span> <span class="o">=</span> <span class="n">perf_counter_ns</span><span class="p">()</span>
<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_its</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">gmres_jit</span><span class="p">(</span><span class="n">do_convolution</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_restart</span><span class="p">)</span>
    <span class="n">error</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">do_convolution</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="n">losses</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>
<span class="n">time_taken</span> <span class="o">=</span> <span class="p">(</span><span class="n">perf_counter_ns</span><span class="p">()</span> <span class="o">-</span> <span class="n">time_start</span><span class="p">)</span> <span class="o">/</span> <span class="mf">1e9</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Deconvolution in </span><span class="si">{</span><span class="n">time_taken</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> s"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Deconvolution in 1.40 s
</code></pre></div></div>

<p><img src="/imgs/gmres/gmres_28_0.svg" alt="svg" /></p>

<p>Except for some ringing around the edges, this produces very good result. Compared to other methods of
deconvolution (as discussed in <a href="/deconvolution-part3">this blog post</a>) this in fact shows much less ringing
artifacts. It’s pretty fast as well. Even though it takes us around 2000 iterations to converge, the
differences between the image after 50 steps or 2000 steps is not that big visually speaking. Let’s see how the 
solution develops with different numbers of iterations:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x0</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x0</span>

<span class="n">results_dict</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">n_its</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">]:</span>
    <span class="n">x0</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
    <span class="c1"># run once to compile
</span>    <span class="n">gmres_jit</span><span class="p">(</span><span class="n">do_convolution</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">n_its</span><span class="p">)</span>

    <span class="n">time_start</span> <span class="o">=</span> <span class="n">perf_counter_ns</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">gmres_jit</span><span class="p">(</span><span class="n">do_convolution</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">n_its</span><span class="p">)</span>
    <span class="n">time_taken</span> <span class="o">=</span> <span class="p">(</span><span class="n">perf_counter_ns</span><span class="p">()</span> <span class="o">-</span> <span class="n">time_start</span><span class="p">)</span> <span class="o">/</span> <span class="mf">1e7</span>
    <span class="n">results_dict</span><span class="p">[</span><span class="n">n_its</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">time_taken</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/imgs/gmres/gmres_31_0.svg" alt="svg" /></p>

<p>After just 100 iterations the result is pretty good, and this takes just 64ms. This makes it a viable method
for deconvolution, roughly equally as fast as Richardson-Lucy deconvolution, but suffering less from boundary
artifacts. The regularization methods we have discussed in the deconvolution blog posts also work in this
setting, and are good to use in the case where there is noise, or where we don’t precisely know the
convolution kernel. That is however out of the scope of this blog post.</p>

<h2 id="conclusion">Conclusion</h2>

<p>GMRES is an easy to implement, fast and robust method for solving <em>structured</em> linear system, where we only
have access to matrix-vector products \(Ax\). It is often used for solving sparse systems, but as we have
demonstrated, it can also be used for solving the deconvolution problem in a way that is competitive with
existing methods. Sometimes a preconditioner is needed to get good performance out of GMRES, but choosing a
good preconditioner can be difficult. If we implement GMRES on a GPU it can reach much higher speeds than on
CPU.</p>


Recent posts




<div class="entries-grid">
  
    



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/gmres-teaser.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/gmres/" rel="permalink">GMRES: or how to do fast linear algebra
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2022-03-10T00:00:00-06:00">March 10, 2022</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        16 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">Linear least-squares system pop up everywhere, and there are many fast way to solve them. We’ll be looking at one such way: GMRES.
</p>
  </article>
</div>

  
    



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/discrete-function-tensor.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/discrete-function-tensor/" rel="permalink">Machine learning with discretized functions and tensors
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2022-03-10T00:00:00-06:00">March 10, 2022</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        17 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">We recently made a paper about supervised machine learning using tensors, here’s the gist of how this works.
</p>
  </article>
</div>

  
    



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/st-vitus-rank-10.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/low-rank-matrix/" rel="permalink">Low-rank matrices: using structure to recover missing data
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2021-09-26T00:00:00-05:00">September 26, 2021</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        10 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">A lot of data is naturally of ‘low rank’. I will explain what this means, and how to exploit this fact.
</p>
  </article>
</div>

  
    



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/python_docx/doc_comparison.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/python-docx/" rel="permalink">How to edit Microsoft Word documents in Python
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2021-08-29T00:00:00-05:00">August 29, 2021</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        6 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">Parsing and editing Word documents automatically can be extremely useful, but doing it in Python is not that straightforward.
</p>
  </article>
</div>

  
    



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/st-vitus-deblurred.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/deconvolution-part4/" rel="permalink">Blind deconvolution #4: Blind deconvolution
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2021-05-31T00:00:00-05:00">May 31, 2021</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        6 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">Finally, let’s look at how we can automatically sharpen images, without knowing how they were blurred in the first place.
</p>
  </article>
</div>

  
    



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/cow-weird-blur.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/deconvolution-part3/" rel="permalink">Blind Deconvolution #3: More about non-blind deconvolution
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2021-05-02T00:00:00-05:00">May 2, 2021</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        5 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">Deconvolving and sharpening images is actually pretty tricky. Let’s have a look at some more advanced methods for deconvolution.
</p>
  </article>
</div>

  
    



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/st-vitus-laplace.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/deconvolution-part2/" rel="permalink">Blind Deconvolution #2: Image Priors
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2021-04-09T00:00:00-05:00">April 9, 2021</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        10 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">In order to automatically sharpen images, we need to first understand how a computer can judge how ‘natural’ an image looks.
</p>
  </article>
</div>

  
    



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/st-vitus-blur.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/deconvolution-part1/" rel="permalink">Blind Deconvolution #1: Non-blind Deconvolution
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2021-03-13T00:00:00-06:00">March 13, 2021</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        6 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">Deconvolution is one of the cornerstones of image processing. Let’s take a look at how it works.
</p>
  </article>
</div>

  
</div>

<!-- <ul class="taxonomy__index">
  
  
    <li>
      <a href="#2022">
        <strong>2022</strong> <span class="taxonomy__count">2</span>
      </a>
    </li>
  
    <li>
      <a href="#2021">
        <strong>2021</strong> <span class="taxonomy__count">7</span>
      </a>
    </li>
  
    <li>
      <a href="#2020">
        <strong>2020</strong> <span class="taxonomy__count">7</span>
      </a>
    </li>
  
</ul> -->
<!-- 





  <section id="2022" class="taxonomy__section">
    <h2 class="archive__subtitle">2022</h2>
    <div class="entries-grid">
      
        



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/gmres-teaser.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/gmres/" rel="permalink">GMRES: or how to do fast linear algebra
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2022-03-10T00:00:00-06:00">March 10, 2022</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        16 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">Linear least-squares system pop up everywhere, and there are many fast way to solve them. We’ll be looking at one such way: GMRES.
</p>
  </article>
</div>

      
        



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/discrete-function-tensor.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/discrete-function-tensor/" rel="permalink">Machine learning with discretized functions and tensors
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2022-03-10T00:00:00-06:00">March 10, 2022</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        17 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">We recently made a paper about supervised machine learning using tensors, here’s the gist of how this works.
</p>
  </article>
</div>

      
    </div>
    <a href="#page-title" class="back-to-top">Back to top &uarr;</a>
  </section>

  <section id="2021" class="taxonomy__section">
    <h2 class="archive__subtitle">2021</h2>
    <div class="entries-grid">
      
        



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/st-vitus-rank-10.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/low-rank-matrix/" rel="permalink">Low-rank matrices: using structure to recover missing data
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2021-09-26T00:00:00-05:00">September 26, 2021</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        10 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">A lot of data is naturally of ‘low rank’. I will explain what this means, and how to exploit this fact.
</p>
  </article>
</div>

      
        



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/python_docx/doc_comparison.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/python-docx/" rel="permalink">How to edit Microsoft Word documents in Python
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2021-08-29T00:00:00-05:00">August 29, 2021</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        6 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">Parsing and editing Word documents automatically can be extremely useful, but doing it in Python is not that straightforward.
</p>
  </article>
</div>

      
        



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/st-vitus-deblurred.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/deconvolution-part4/" rel="permalink">Blind deconvolution #4: Blind deconvolution
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2021-05-31T00:00:00-05:00">May 31, 2021</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        6 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">Finally, let’s look at how we can automatically sharpen images, without knowing how they were blurred in the first place.
</p>
  </article>
</div>

      
        



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/cow-weird-blur.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/deconvolution-part3/" rel="permalink">Blind Deconvolution #3: More about non-blind deconvolution
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2021-05-02T00:00:00-05:00">May 2, 2021</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        5 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">Deconvolving and sharpening images is actually pretty tricky. Let’s have a look at some more advanced methods for deconvolution.
</p>
  </article>
</div>

      
        



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/st-vitus-laplace.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/deconvolution-part2/" rel="permalink">Blind Deconvolution #2: Image Priors
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2021-04-09T00:00:00-05:00">April 9, 2021</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        10 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">In order to automatically sharpen images, we need to first understand how a computer can judge how ‘natural’ an image looks.
</p>
  </article>
</div>

      
        



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/st-vitus-blur.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/deconvolution-part1/" rel="permalink">Blind Deconvolution #1: Non-blind Deconvolution
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2021-03-13T00:00:00-06:00">March 13, 2021</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        6 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">Deconvolution is one of the cornerstones of image processing. Let’s take a look at how it works.
</p>
  </article>
</div>

      
        



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/email-time-series.jpg" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/email-time-series/" rel="permalink">Time series analysis of my email traffic
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2021-02-13T00:00:00-06:00">February 13, 2021</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        6 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">I have 15 years worth of email traffic data, let’s take a closer look and discover some fascinating patterns.
</p>
  </article>
</div>

      
    </div>
    <a href="#page-title" class="back-to-top">Back to top &uarr;</a>
  </section>

  <section id="2020" class="taxonomy__section">
    <h2 class="archive__subtitle">2020</h2>
    <div class="entries-grid">
      
        



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/music-2020.jpg" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/music_2020/" rel="permalink">2020 in music
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2020-12-31T00:00:00-06:00">December 31, 2020</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        4 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">2020 was a great year for music, I will look back and give some thoughts on the best albums that came out in 20202.
</p>
  </article>
</div>

      
        



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/bayes-exam.jpg" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/bayes_exam/" rel="permalink">Modeling uncertainty in exam scores
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2020-11-09T00:00:00-06:00">November 9, 2020</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        3 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">We use exams to determine how much a student knows, but exams aren’t perfect. How can we estimate the uncertainty in students’ exams scores?
</p>
  </article>
</div>

      
        



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/validation-data.jpg" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/validation-size/" rel="permalink">How big should my validation set be?
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2020-08-26T00:00:00-05:00">August 26, 2020</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        12 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">Cross validation is extremely important, but how should we choose the size of our validation and test sets?
</p>
  </article>
</div>

      
        



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/lastfm.jpg" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/lastfm/" rel="permalink">How do my music preferences evolve?
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2020-08-12T00:00:00-05:00">August 12, 2020</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        5 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">I use last.fm to track my music listening. Let’s look at my data to discover how my musical preferences evolve over time.
</p>
  </article>
</div>

      
        



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/normal-data.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/normal-data/" rel="permalink">Is my data normal?
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2020-08-10T00:00:00-05:00">August 10, 2020</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        6 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">Normally distributed data is great, but how do you know whether your data is normally distributed?
</p>
  </article>
</div>

      
        



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/figure-skating.jpg" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/figure-skating/" rel="permalink">Bias in figure skating judging
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2020-06-20T00:00:00-05:00">June 20, 2020</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        3 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">Judging in figure skating is biased. Let’s use data science to figure out just how bad the issue is.
</p>
  </article>
</div>

      
        



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/first-post.jpg" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/first-post/" rel="permalink">First post
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2020-06-19T07:19:44-05:00">June 19, 2020</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        less than 1 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">My first post in this blog
</p>
  </article>
</div>

      
    </div>
    <a href="#page-title" class="back-to-top">Back to top &uarr;</a>
  </section>
 -->

  </div>
</div>
    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    <!-- 
      <li><strong>Contact info and socials</strong></li>
     -->

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2022 Rik Voorhaar. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>







  <script>
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'G-Q5W21THKW2']);
  
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>









  </body>
</html>
