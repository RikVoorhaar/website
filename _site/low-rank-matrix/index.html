<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.20.1 by Michael Rose
  Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<!-- <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> -->

<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Low-rank matrices: using structure to recover missing data - Rik Voorhaar</title>
<meta name="description" content="A lot of data is naturally of ‘low rank’. I will explain what this means, and how to exploit this fact.">


  <meta name="author" content="Dr. Rik Voorhaar">


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Rik Voorhaar">
<meta property="og:title" content="Low-rank matrices: using structure to recover missing data">
<meta property="og:url" content="https://blog.rikvoorhaar.com/low-rank-matrix/">


  <meta property="og:description" content="A lot of data is naturally of ‘low rank’. I will explain what this means, and how to exploit this fact.">



  <meta property="og:image" content="https://blog.rikvoorhaar.com/imgs/teasers/st-vitus-rank-10.png">





  <meta property="article:published_time" content="2021-09-26T00:00:00-05:00">





  

  


<link rel="canonical" href="https://blog.rikvoorhaar.com/low-rank-matrix/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Rik Voorhaar",
      "url": "https://blog.rikvoorhaar.com/"
    
  }
</script>






<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Rik Voorhaar Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<!-- <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"> -->
<script src="https://kit.fontawesome.com/ca9a31e360.js" crossorigin="anonymous"></script>


<!--[if IE]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->
<link rel="shortcut icon" type="image/x-icon" href="/favicon.ico">

<!-- end custom head snippets -->
<link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link href="https://fonts.googleapis.com/css2?family=Bitter:wght@500;600;700;800&family=Open+Sans:ital,wght@0,300;0,400;0,500;0,600;0,700;0,800;1,300;1,400;1,500;1,600;1,700;1,800&display=swap" rel="stylesheet">
  </head>

  <body class="layout--posts">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/"><img src="/assets/images/logo_name.svg" alt=""></a>
        
        <a class="site-title" href="/">
           
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/cv/">CV</a>
            </li><li class="masthead__menu-item">
              <a href="/">Blog</a>
            </li><li class="masthead__menu-item">
              <a href="/about/">Hobbies</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      <div class="page__hero-background"> </div>





<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  
    <div class="author__avatar">
      
        <img src="/imgs/avatar.jpg" alt="Dr. Rik Voorhaar" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Dr. Rik Voorhaar</h3>
    
    
      <div class="author__bio" itemprop="description">
        <p>ML Software Developer and curious mind</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Contact info and socials</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">Copenhagen, Denmark</span>
        </li>
      

      
        
          
            <li><a href="mailto:rik.voorhaar@gmail.com" rel="nofollow noopener noreferrer"><i class="far fa-fw fa-envelope" aria-hidden="true"></i><span class="label">rik.voorhaar@gmail.com</span></a></li>
          
        
          
            <li><a href="https://github.com/RikVoorhaar" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
            <li><a href="https://www.facebook.com/WH.Voorhaar" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span class="label">Facebook</span></a></li>
          
        
          
            <li><a href="https://discord.com/users/354966598643220480" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-discord" aria-hidden="true"></i><span class="label">Discord</span></a></li>
          
        
          
            <li><a href="https://www.linkedin.com/in/rik-voorhaar" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span class="label">LinkedIn</span></a></li>
          
        
          
            <li><a href="https://www.last.fm/user/Tilpo" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-lastfm-square" aria-hidden="true"></i><span class="label">Last.fm</span></a></li>
          
        
          
            <li><a href="https://www.goodreads.com/user/show/62542056-tilpo" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-goodreads" aria-hidden="true"></i><span class="label">Goodreads</span></a></li>
          
        
          
            <li><a href="https://steamcommunity.com/profiles/76561197996562422" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-steam" aria-hidden="true"></i><span class="label">Steam</span></a></li>
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <div class="archive">
    <br>
    
      <h1 id="page-title" class="page__title">Low-rank matrices: using structure to recover missing data</h1>
    
    <p>Tensor networks are probably the most important tool in my research, and I want
explain them. Before I can do this however, I should first talk about low-rank
matrix decompositions, and why they’re so incredibly useful.  At the same time I
will illustrate everything using examples in Python code, using <code class="language-plaintext highlighter-rouge">numpy</code>.</p>

<h2 id="the-singular-value-decomposition">The singular value decomposition</h2>

<p>Often if we have an \(m\times n\) matrix, we can write it as the product of two
smaller matrices. If such a matrix has <em>rank</em> \(r\), then we can write it as the
product of an \(m\times r\) and \(r\times n\) matrix. Equivalently, this is the
<em>number of linearly independent columns or rows</em> the matrix has, or if we see
the matrix as a linear map \(\mathbb R^m\to \mathbb R^n\), then it is the
<em>dimension of the image</em> of this linear map.</p>

<p>In practice we can figure out the rank of a matrix by computing its <em>singular
value decomposition</em> (SVD). If you studied data science or statistics, then you
have probably seen principal component analysis (PCA); this is very closely
related to the SVD. Using the SVD we can write a matrix \(X\) as a product</p>

\[X = U S V\]

<p>Where \(U\) and \(V\) are orthogonal matrices, and \(S\) is a diagonal matrix. The
values on the diagonals of \(S\) are known as the <em>singular values</em> of \(S\). The
matrices \(U\) and \(V\) also have nice interpretations; the rows of \(U\) form an
orthonormal basis of the <em>row space</em> of \(X\), and the columns of \(V\) are an
orthonormal basis of the <em>column space</em> of \(X\).</p>

<p>In <code class="language-plaintext highlighter-rouge">numpy</code> we can compute the SVD of a matrix using <code class="language-plaintext highlighter-rouge">np.linalg.svd</code>. Below we
compute it and verify that indeed \(X = U S V\):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># Generate a random 10x20 matrix of rank 5
</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">r</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">r</span><span class="p">))</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">A</span> <span class="o">@</span> <span class="n">B</span>

<span class="c1"># Compute the SVD
</span><span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">svd</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1"># Confirm U S V = X
</span><span class="n">np</span><span class="p">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">U</span> <span class="o">@</span> <span class="n">np</span><span class="p">.</span><span class="n">diag</span><span class="p">(</span><span class="n">S</span><span class="p">)</span> <span class="o">@</span> <span class="n">V</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
</code></pre></div></div>

<blockquote>
  <p><code class="language-plaintext highlighter-rouge">True</code></p>
</blockquote>

<p>Note that we called <code class="language-plaintext highlighter-rouge">np.linalg.svd</code> with the keyword <code class="language-plaintext highlighter-rouge">full_matrices=False</code>. If
left to the default value <code class="language-plaintext highlighter-rouge">True</code>, then in this case <code class="language-plaintext highlighter-rouge">V</code> would be a \({20\times
20}\) matrix, as opposed to the \(10\times 20\) matrix it is now. Also <code class="language-plaintext highlighter-rouge">S</code> is
returned as a 1D array, and we can convert it to a diagonal matrix using
<code class="language-plaintext highlighter-rouge">np.diag</code>. Finally the function <code class="language-plaintext highlighter-rouge">np.allclose</code> checks if all the entries of two
matrices are almost the same; they never will be exactly the same due to
numerical error.</p>

<p>As mentioned before, we can use the singular values <code class="language-plaintext highlighter-rouge">S</code> to determine what the
rank is the matrix <code class="language-plaintext highlighter-rouge">X</code>. This is obvious if we plot the singular values:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">DEFAULT_FIGSIZE</span> <span class="o">=</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">DEFAULT_FIGSIZE</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">S</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">S</span><span class="p">,</span> <span class="s">"o"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">S</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">yscale</span><span class="p">(</span><span class="s">"log"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Plot of singular values"</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/imgs/low-rank-matrix/intro-tn_3_1.png" alt="png" /></p>

<p>We see that the first 5 singular values are roughly the same size, but that the
last five singular values are much smaller; on the order of the machine epsilon.</p>

<p>Knowing the matrix is rank 5, can we write it as the product of two rank 5
matrices? Absolutely! And we do this using the SVD, or rather the <em>truncated
singular value decomposition</em>. Since the last 5 values of <code class="language-plaintext highlighter-rouge">S</code> are very close to
zero, we can simply ignore them. This then means dropping the last 5 columns of
<code class="language-plaintext highlighter-rouge">U</code> and the last 5 rows of <code class="language-plaintext highlighter-rouge">V</code>. Then finally we just need to ‘absorb’ the
singular values into one of the two matrices <code class="language-plaintext highlighter-rouge">U</code> or <code class="language-plaintext highlighter-rouge">V</code>, This way we write <code class="language-plaintext highlighter-rouge">X</code>
as the product of a \(10\times 5\) and \(5\times 20\) matrix.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">A</span> <span class="o">=</span> <span class="n">U</span><span class="p">[:,</span> <span class="p">:</span><span class="n">r</span><span class="p">]</span> <span class="o">*</span> <span class="n">S</span><span class="p">[:</span><span class="n">r</span><span class="p">]</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">V</span><span class="p">[:</span><span class="n">r</span><span class="p">,</span> <span class="p">:]</span>
<span class="k">print</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">B</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">np</span><span class="p">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">A</span> <span class="o">@</span> <span class="n">B</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
</code></pre></div></div>

<blockquote>
  <p><code class="language-plaintext highlighter-rouge">(10, 5) (5, 20)</code></p>

  <p><code class="language-plaintext highlighter-rouge">True</code></p>
</blockquote>

<h2 id="svd-as-data-compression-method">SVD as data compression method</h2>

<p>We rarely encounter real-world data that can be <em>exactly</em> represented by a low
rank matrix using the truncated SVD. But we can still use the truncated SVD to
get a good <em>approximation</em> of the data.</p>

<p>Let us look at the singular values of an image of the St. Vitus church in my 
hometown. Note that a black-and-white image is really just a matrix.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">image</span>

<span class="c1"># Load and plot the St. Vitus image
</span><span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="n">imread</span><span class="p">(</span><span class="s">"vitus512.png"</span><span class="p">)</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">img</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>  <span class="c1"># make entries lie in range [0,1]
</span><span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">"gray"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">axis</span><span class="p">(</span><span class="s">"off"</span><span class="p">)</span>

<span class="c1"># Compute and plot the singular values
</span><span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Singular values"</span><span class="p">)</span>
<span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">svd</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">yscale</span><span class="p">(</span><span class="s">"log"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">S</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/imgs/low-rank-matrix/intro-tn_7_1.png" alt="png" /></p>

<p>We see here that the first few singular values are much larger than the rest,
followed by a slow decay, and then finally a sharp drop at the very end. Note
that there are 512 singular values, because this is a 512x512 image.</p>

<p>Let’s now try to see what happens if we compress this image as a low rank matrix
using the truncated singular value decomposition. We will look what happens to
the image when seen as a rank 10,20,50 or 100 matrix.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">rank</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">]):</span>
    <span class="c1"># Compute truncated SVD
</span>    <span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">svd</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
    <span class="n">img_compressed</span> <span class="o">=</span> <span class="n">U</span><span class="p">[:,</span> <span class="p">:</span><span class="n">rank</span><span class="p">]</span> <span class="o">@</span> <span class="n">np</span><span class="p">.</span><span class="n">diag</span><span class="p">(</span><span class="n">S</span><span class="p">[:</span><span class="n">rank</span><span class="p">])</span> <span class="o">@</span> <span class="n">V</span><span class="p">[:</span><span class="n">rank</span><span class="p">,</span> <span class="p">:]</span>

    <span class="c1"># Plot the image
</span>    <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s">"Rank </span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img_compressed</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">"gray"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">axis</span><span class="p">(</span><span class="s">"off"</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/imgs/low-rank-matrix/intro-tn_9_0.png" alt="png" /></p>

<p>We see that even the rank 10 and 20 images are pretty recognizable, but with
heavy artifacts. On the other hand the rank 50 image looks pretty good, but not
as good as the original. The rank 100 image on the other hand looks really close
to the original.</p>

<p>How big is the compression if we do this? Well, if we write the image as a rank
10 matrix, we need two 512x10 matrices to store the image, which adds up to
10240 parameters, as opposed to the original 262144 parameters; a decrease in
storage of more than 25 times! On the other hand, the rank 100 image is only
about 2.6 times smaller than the original. Note that this is not a good image
compression algorithm; the SVD is relatively expensive to compute, and other
compression algorithms can achieve higher compression ratios with less image
degradation.</p>

<p>The conclusion we can draw from this is that we can use truncated SVD to
compress data. However, not all data can be compressed as efficiently by this
method. It depends on the distribution of singular values; the faster the
singular values decay, the better a low rank decomposition is going to
approximate our data. Images are not good examples of data that can be
compressed efficiently as a low rank matrix.</p>

<p>One reason why it’s difficult to compress images is because they contain many
sharp edges and transitions. Low rank matrices are especially bad at
representing diagonal lines. For example, the identity matrix is a diagonal
line seen as an image, and it is also impossible to compress using an SVD since
all singular values are equal.</p>

<p>On the other hand, images without any sharp transitions can be approximated
quite well using low rank matrices. These kind of images rarely appear as
natural images, but rather they can be discrete representations of smooth
functions \([0,1]^2 \to\mathbb R\). For example below we show a two-dimensional
discretized sum of trigonometric functions and its singular value decomposition.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Make a grid of 100 x 100 values between [0,1]
</span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># A smooth trigonometric function
</span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">200</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">75</span> <span class="o">*</span> <span class="n">y</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">50</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">cos</span><span class="p">(</span><span class="mi">100</span> <span class="o">*</span> <span class="n">y</span><span class="p">)</span>


<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">svd</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">S</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">yscale</span><span class="p">(</span><span class="s">"log"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Singular values"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"The matrix is approximately of rank: </span><span class="si">{</span><span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">S</span><span class="o">&gt;</span><span class="mf">1e-12</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>

<blockquote>
  <p><code class="language-plaintext highlighter-rouge">The matrix is approximately of rank: 4</code></p>
</blockquote>

<p><img src="/imgs/low-rank-matrix/intro-tn_11_1.png" alt="png" /></p>

<p>We see that this particular function can be represented by a rank 4 matrix! This
is not obvious if you look at the image. In these kind of situations a low-rank
matrix decomposition is much better than many image compression algorithms. In
this case we can reconstruct the image using only 8% of the parameters.
(Although more advanced image compression algorithms are based on wavelets, and
will actually compress this very well.)</p>

<h2 id="matrix-completion">Matrix completion</h2>

<p>Recall that a low rank matrix approximation can require much less parameters 
than the dense matrix it approximates. One of the powerful things about this
allows us to recover the dense matrix even in the case where we only observe
a small part of the matrix. That is, if we have many missing values.</p>

<p>In the case above we can represent the 100x100 matrix \(X\) as a product of a
100x4 and 4x100 a matrix \(A\) and \(B\), which has in total 800 parameters instead
of 10,000. We can actually recover this low-rank decomposition from a small
subset of the big dense matrix. Suppose that we observe the entries \(X_{ij}\) for
\((i,j)\in \Omega\) an index set. We can recover \(A\) and \(B\) by solving the
following least-squares problem:</p>

\[\min_{A,B}\sum_{(i,j)\in \Omega}((AB)_{ij}-X_{ij})^2\]

<p>This problem is however non-convex, and not straightforward to solve. There is
fortunately a trick: we can alternatively fix \(A\) and then optimize \(B\) and
vice-versa. This is known as Alternating Least Squares (ALS) optimization, and
in this case works well. If we fix \(A\), observe that the minimization problem
uncouples into separate linear least squares problems for each column of \(B\):</p>

\[\min_{B_{\bullet k}} \sum_{(i,j)\in \Omega,\,j=k} (\langle A_{i\bullet},B_{\bullet k}\rangle-X_{ik})^2\]

<p>Below we do use this approach to recover the same matrix as before using 2000
data points, and we can see it does so with a very low error:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">N</span> <span class="o">=</span> <span class="mi">2000</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">r</span> <span class="o">=</span> <span class="mi">4</span>

<span class="c1"># Sample N=2000 random indices
</span><span class="n">Omega</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="n">n</span> <span class="o">*</span> <span class="n">n</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">Omega</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">unravel_index</span><span class="p">(</span><span class="n">Omega</span><span class="p">,</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">Omega</span><span class="p">]</span>

<span class="c1"># Use random initialization for matrices A,B
</span><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">r</span><span class="p">))</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">linsolve_regular</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">lam</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">):</span>
    <span class="s">"""Solve linear problem A@x = b with Tikhonov regularization / ridge
    regression"""</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">solve</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">A</span> <span class="o">+</span> <span class="n">lam</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">eye</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">A</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">b</span><span class="p">)</span>


<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">40</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(((</span><span class="n">A</span> <span class="o">@</span> <span class="n">B</span><span class="p">)[</span><span class="n">Omega</span><span class="p">]</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">losses</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

    <span class="c1"># Update B
</span>    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">B</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">linsolve_regular</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">Omega</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">Omega</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">j</span><span class="p">]],</span> <span class="n">y</span><span class="p">[</span><span class="n">Omega</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">j</span><span class="p">])</span>

    <span class="c1"># Update A
</span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">linsolve_regular</span><span class="p">(</span>
            <span class="n">B</span><span class="p">[:,</span> <span class="n">Omega</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="n">Omega</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">i</span><span class="p">]].</span><span class="n">T</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="n">Omega</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">i</span><span class="p">]</span>
        <span class="p">)</span>

<span class="c1"># Plot the input image
</span><span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Input image"</span><span class="p">)</span>
<span class="n">S</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
<span class="n">S</span><span class="p">[</span><span class="n">Omega</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span>
<span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">S</span><span class="p">)</span>

<span class="c1"># Plot reconstructed image
</span><span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Reconstructed image"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">A</span> <span class="o">@</span> <span class="n">B</span><span class="p">)</span>

<span class="c1"># Plot training loss
</span><span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Mean square error loss during training"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">yscale</span><span class="p">(</span><span class="s">"log"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"steps"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"Mean squared error"</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/imgs/low-rank-matrix/intro-tn_14_1.png" alt="png" /></p>

<h2 id="netflix-prize">Netflix prize</h2>

<p>Let’s consider a particularly interesting use of matrix completion –
collaborative filtering. Think about how services like Netflix may recommend new
shows or movies to watch. They know which movies you like, and they know which
movies other people like. Netflix then recommends movies that are liked by
people with a similar taste to yours. This is called <em>collaborative filtering</em>,
because different people <em>collaborate</em> to filter out movies so that we can make
a recommendation.</p>

<p>But can we do this in practice? Well, for every user we can put their personal
ratings of every movie they watched in a big matrix. In this matrix each row
represents a movie, and each column a user. Most users have only seen a small
fraction of all the movies on the platform, so the overwhelming majority of the
entries of this matrix are unknown. Then we apply matrix completion to this
matrix. Each entry of the completed matrix then represents the rating <em>we think</em>
the user would give to a movie, even if they have never watched it.</p>

<p>In 2006 Netflix opened a competition with a grand prize of <strong>$1,000,000</strong> (!!)
to solve precisely this problem. The data consists of more than 100 million
ratings by 480,189 users for 17,769 different movies. The size of this dataset
immediately poses a practical problem; if we put this in a matrix with floating
point entries, then it would require about 68 terabytes of RAM. Fortunately we
can avoid this problem by using sparse matrices. This makes implementation a
little harder, but certainly still feasible.</p>

<p>We will also need to upgrade our matrix completion algorithm. The algorithm we
mentioned before is slow for very large matrices, and suffers from problems of
numerical stability due to the way it decouples into many smaller linear
problems. Recall that complete a matrix \(X\) by solving the following
optimization problem:</p>

\[\min_{A,B}\sum_{(i,j)\in \Omega}((AB)_{ij}-X_{ij})^2.\]

<p>We will first rewrite the problem as follows:</p>

\[\min_{A,B}\|P_\Omega(AB) -X\|.\]

<p>Here \(P_\Omega\) denotes the operation of setting all entries \(AB_{ij}\) to zero
if \((i,j)\notin \Omega\). In other words, \(P_\Omega\) turns \(AB\) into a sparse
matrix with the same sparsity pattern as \(X\). In some sense, the issue with this
optimization problem is that only a small part of the entries of \(AB\) affect the
the objective. We can solve this by adding a new matrix \(Z\) such that
\(P_\Omega(Z)=X\), and then using \(A,B\) to approximate \(Z\) instead:</p>

\[\min_{A,B,Z}\|AB-Z\|\quad \text{such that } P_\Omega Z = X\]

<p>This problem can then be solved using the same alternating least-squares
approach we have used before. For example if we fix \(A,B\) then the optimal value
of \(Z\) is given by \(Z = AB+X-P_\Omega(Z)\), and at each iteration we can update 
\(A\) and \(B\) by solving a linear least-squares problem. It is important to note
that this way \(Z\) is a sum of a low-rank and a sparse matrix at every step, and
this allows us to still efficiently manipulate it and store it in memory.</p>

<p>Although not very difficult, the implementation of this algorithm is a little
too technical for this blog post. Instead we can just look at the results. I
used this algorithm to fit matrices \(A\) and \(B\) of rank 5 and of rank 10 to the
Netflix prize dataset. I used 3000 iterations of training, taking the better
part of a day to train on my computer. I could probably do more, but I’m too
impatient. The progress of training is shown below.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">os.path</span>

<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">DEFAULT_FIGSIZE</span><span class="p">)</span>

<span class="n">DATASET_PATH</span> <span class="o">=</span> <span class="s">"/mnt/games/datasets/netflix/"</span>
<span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">]:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">DATASET_PATH</span><span class="p">,</span> <span class="sa">f</span><span class="s">"rank-</span><span class="si">{</span><span class="n">r</span><span class="si">}</span><span class="s">-model.npz"</span><span class="p">))</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">model</span><span class="p">[</span><span class="s">"X"</span><span class="p">]</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">model</span><span class="p">[</span><span class="s">"Y"</span><span class="p">]</span>
    <span class="n">train_errors</span> <span class="o">=</span> <span class="n">model</span><span class="p">[</span><span class="s">"train_errors"</span><span class="p">]</span>
    <span class="n">test_errors</span> <span class="o">=</span> <span class="n">model</span><span class="p">[</span><span class="s">"test_errors"</span><span class="p">]</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">train_errors</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s">"Train rank </span><span class="si">{</span><span class="n">r</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">test_errors</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s">"Test rank </span><span class="si">{</span><span class="n">r</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylim</span><span class="p">(</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"Training iterations"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"Root mean squared error (RMSE)"</span><span class="p">);</span>
</code></pre></div></div>

<p><img src="/imgs/low-rank-matrix/intro-tn_16_0.png" alt="png" /></p>

<p>We see that the training error for the rank 5 and rank 10 models are virtually
identical, but the test error is lower for the rank 5 model. We can interpret
this as the rank 10 model overfitting more, which is often the case for more
complex models.</p>

<p>Next, how can we use this model? Well, the rows of the matrix \(A\) correspond to
movies, and the columns of matrix \(B\) correspond to users. So if we want to know
how much user #179 likes movie #2451 (<em>Lord of the Rings: The Fellowship of the
Ring</em>), then we compute \(A[2451]\cdot B[:, 179]\):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">A</span><span class="p">[</span><span class="mi">2451</span><span class="p">]</span> <span class="o">@</span> <span class="n">B</span><span class="p">[:,</span> <span class="mi">179</span><span class="p">]</span>

</code></pre></div></div>

<blockquote>
  <p><code class="language-plaintext highlighter-rouge">4.411312294862265</code></p>
</blockquote>

<p>We see that the <em>expected rating</em> (out of 5) for this user and movie is about
4.41. So we expect that this user will like this movie, and we may choose to 
recommend it.</p>

<p>But we want to find the <em>best</em> recommendation for this user. To do this we can
simply compute the product \(A \cdot B[:,179]\), which will give a vector with
expected rating for every single movie, and then we simply sort. Below we can
see the 5 movies with the highest and lowest expected ratings for this user.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="n">movies</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span>
    <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">DATASET_PATH</span><span class="p">,</span> <span class="s">"movie_titles.csv"</span><span class="p">),</span>
    <span class="n">names</span><span class="o">=</span><span class="p">[</span><span class="s">"index"</span><span class="p">,</span> <span class="s">"year"</span><span class="p">,</span> <span class="s">"name"</span><span class="p">],</span>
    <span class="n">usecols</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
<span class="p">)</span>
<span class="n">movies</span><span class="p">[</span><span class="s">"ratings-179"</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span> <span class="o">@</span> <span class="n">B</span><span class="p">[:,</span> <span class="mi">179</span><span class="p">]</span>
<span class="n">movies</span><span class="p">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s">"ratings-179"</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>name</th>
      <th>ratings-179</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>10755</th>
      <td>Kirby: A Dark &amp; Stormy Knight</td>
      <td>9.645918</td>
    </tr>
    <tr>
      <th>15833</th>
      <td>Paternal Instinct</td>
      <td>7.712654</td>
    </tr>
    <tr>
      <th>15355</th>
      <td>Last Hero In China</td>
      <td>7.689984</td>
    </tr>
    <tr>
      <th>14902</th>
      <td>Warren Miller's: Ride</td>
      <td>7.624472</td>
    </tr>
    <tr>
      <th>2082</th>
      <td>Blood Alley</td>
      <td>7.317524</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>463</th>
      <td>The Return of Ruben Blades</td>
      <td>-6.037189</td>
    </tr>
    <tr>
      <th>12923</th>
      <td>Where the Red Fern Grows 2</td>
      <td>-6.153577</td>
    </tr>
    <tr>
      <th>7067</th>
      <td>Eric Idle's Personal Best</td>
      <td>-6.441100</td>
    </tr>
    <tr>
      <th>538</th>
      <td>Rumpole of the Bailey: Series 4</td>
      <td>-6.740144</td>
    </tr>
    <tr>
      <th>4331</th>
      <td>Sugar: Howling of Angel</td>
      <td>-7.015818</td>
    </tr>
  </tbody>
</table>
<p>17769 rows × 2 columns</p>
</div>

<p>Note that the expected ratings are not between 0 and 5, but can take on any
value (in particular non-integer ones). This is not necessarily a problem,
because we only care about the relative rating of the movies.</p>

<p>To me, all these movies all sound quite obscure. And this makes sense, the model
does not take factors such as popularity of the movie into account. It also
ignores a lot of other data that we may know about the user, such as their age,
gender and location. It ignores when the movie is released, and it doesn’t take
into account the dates of all the movie ratings of each user. These are all
important factors, that could significantly improve the quality of this the
recommendation system.</p>

<p>We could try to modify our matrix completion model to take these factors into
account, but it’s not obvious how to do this. There is no need to do this
however, we use the matrices \(A\), \(B\) to augment any data we have about the
movie and the user. And then we can train a new model on top of this data, to
create something even better.</p>

<p>We can think of the movies as lying in a really high-dimensional space, and the 
matrix \(A\) maps this space onto a much smaller space. The same is true for the 
\(B\) and the ‘space’ of users. We can then use this <em>embedding</em> into a lower
dimensional space as the input of another model.</p>

<p>Unfortunately we don’t have access to more information about the users (due
to obvious privacy concerns), so this is difficult to demonstrate. But the point
is this: the decomposition \(X\approx AB\) is both <em>interpretable</em>, and can be 
used as a building block for more advanced machine learning models.</p>

<h2 id="conclusion">Conclusion</h2>

<p>In summary we have seen that low-rank matrix decompositions have many useful
applications in machine learning. They are powerful because they can be learned
using relatively little data, and have the ability to complete missing data.
Unlike many other machine learning models, computing low-rank matrix
decompositions of data can be done quickly.</p>

<p>Even though they come with some limitations, they can always be used as a
building block for more advanced machine learning models. This is because they
can give an interpretable, low-dimensional representation of very
high-dimensional data. We also didn’t even come close to discussing all their
applications, or algorithms on how to find and optimize them.</p>

<p>In the next post I will look at a generalization of low-rank matrix
decompositions: <em>tensor decompositions</em>. While more complicated, these
decompositions are even more powerful at reducing the dimensionality of very
high-dimensional data.</p>


Recent posts




<div class="entries-grid">
  
    



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/gmres-teaser.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/gmres/" rel="permalink">GMRES: or how to do fast linear algebra
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2022-03-10T00:00:00-06:00">March 10, 2022</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        16 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">Linear least-squares system pop up everywhere, and there are many fast way to solve them. We’ll be looking at one such way: GMRES.
</p>
  </article>
</div>

  
    



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/discrete-function-tensor.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/discrete-function-tensor/" rel="permalink">Machine learning with discretized functions and tensors
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2022-03-10T00:00:00-06:00">March 10, 2022</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        18 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">We recently made a paper about supervised machine learning using tensors, here’s the gist of how this works.
</p>
  </article>
</div>

  
    



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/st-vitus-rank-10.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/low-rank-matrix/" rel="permalink">Low-rank matrices: using structure to recover missing data
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2021-09-26T00:00:00-05:00">September 26, 2021</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        10 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">A lot of data is naturally of ‘low rank’. I will explain what this means, and how to exploit this fact.
</p>
  </article>
</div>

  
    



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/python_docx/doc_comparison.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/python-docx/" rel="permalink">How to edit Microsoft Word documents in Python
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2021-08-29T00:00:00-05:00">August 29, 2021</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        6 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">Parsing and editing Word documents automatically can be extremely useful, but doing it in Python is not that straightforward.
</p>
  </article>
</div>

  
    



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/st-vitus-deblurred.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/deconvolution-part4/" rel="permalink">Blind deconvolution #4: Blind deconvolution
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2021-05-31T00:00:00-05:00">May 31, 2021</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        6 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">Finally, let’s look at how we can automatically sharpen images, without knowing how they were blurred in the first place.
</p>
  </article>
</div>

  
    



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/cow-weird-blur.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/deconvolution-part3/" rel="permalink">Blind Deconvolution #3: More about non-blind deconvolution
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2021-05-02T00:00:00-05:00">May 2, 2021</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        5 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">Deconvolving and sharpening images is actually pretty tricky. Let’s have a look at some more advanced methods for deconvolution.
</p>
  </article>
</div>

  
    



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/st-vitus-laplace.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/deconvolution-part2/" rel="permalink">Blind Deconvolution #2: Image Priors
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2021-04-09T00:00:00-05:00">April 9, 2021</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        10 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">In order to automatically sharpen images, we need to first understand how a computer can judge how ‘natural’ an image looks.
</p>
  </article>
</div>

  
    



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/st-vitus-blur.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/deconvolution-part1/" rel="permalink">Blind Deconvolution #1: Non-blind Deconvolution
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2021-03-13T00:00:00-06:00">March 13, 2021</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        6 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">Deconvolution is one of the cornerstones of image processing. Let’s take a look at how it works.
</p>
  </article>
</div>

  
</div>

<!-- <ul class="taxonomy__index">
  
  
    <li>
      <a href="#2022">
        <strong>2022</strong> <span class="taxonomy__count">2</span>
      </a>
    </li>
  
    <li>
      <a href="#2021">
        <strong>2021</strong> <span class="taxonomy__count">7</span>
      </a>
    </li>
  
    <li>
      <a href="#2020">
        <strong>2020</strong> <span class="taxonomy__count">7</span>
      </a>
    </li>
  
</ul> -->
<!-- 





  <section id="2022" class="taxonomy__section">
    <h2 class="archive__subtitle">2022</h2>
    <div class="entries-grid">
      
        



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/gmres-teaser.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/gmres/" rel="permalink">GMRES: or how to do fast linear algebra
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2022-03-10T00:00:00-06:00">March 10, 2022</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        16 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">Linear least-squares system pop up everywhere, and there are many fast way to solve them. We’ll be looking at one such way: GMRES.
</p>
  </article>
</div>

      
        



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/discrete-function-tensor.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/discrete-function-tensor/" rel="permalink">Machine learning with discretized functions and tensors
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2022-03-10T00:00:00-06:00">March 10, 2022</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        18 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">We recently made a paper about supervised machine learning using tensors, here’s the gist of how this works.
</p>
  </article>
</div>

      
    </div>
    <a href="#page-title" class="back-to-top">Back to top &uarr;</a>
  </section>

  <section id="2021" class="taxonomy__section">
    <h2 class="archive__subtitle">2021</h2>
    <div class="entries-grid">
      
        



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/st-vitus-rank-10.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/low-rank-matrix/" rel="permalink">Low-rank matrices: using structure to recover missing data
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2021-09-26T00:00:00-05:00">September 26, 2021</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        10 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">A lot of data is naturally of ‘low rank’. I will explain what this means, and how to exploit this fact.
</p>
  </article>
</div>

      
        



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/python_docx/doc_comparison.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/python-docx/" rel="permalink">How to edit Microsoft Word documents in Python
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2021-08-29T00:00:00-05:00">August 29, 2021</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        6 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">Parsing and editing Word documents automatically can be extremely useful, but doing it in Python is not that straightforward.
</p>
  </article>
</div>

      
        



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/st-vitus-deblurred.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/deconvolution-part4/" rel="permalink">Blind deconvolution #4: Blind deconvolution
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2021-05-31T00:00:00-05:00">May 31, 2021</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        6 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">Finally, let’s look at how we can automatically sharpen images, without knowing how they were blurred in the first place.
</p>
  </article>
</div>

      
        



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/cow-weird-blur.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/deconvolution-part3/" rel="permalink">Blind Deconvolution #3: More about non-blind deconvolution
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2021-05-02T00:00:00-05:00">May 2, 2021</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        5 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">Deconvolving and sharpening images is actually pretty tricky. Let’s have a look at some more advanced methods for deconvolution.
</p>
  </article>
</div>

      
        



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/st-vitus-laplace.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/deconvolution-part2/" rel="permalink">Blind Deconvolution #2: Image Priors
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2021-04-09T00:00:00-05:00">April 9, 2021</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        10 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">In order to automatically sharpen images, we need to first understand how a computer can judge how ‘natural’ an image looks.
</p>
  </article>
</div>

      
        



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/st-vitus-blur.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/deconvolution-part1/" rel="permalink">Blind Deconvolution #1: Non-blind Deconvolution
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2021-03-13T00:00:00-06:00">March 13, 2021</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        6 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">Deconvolution is one of the cornerstones of image processing. Let’s take a look at how it works.
</p>
  </article>
</div>

      
        



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/email-time-series.jpg" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/email-time-series/" rel="permalink">Time series analysis of my email traffic
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2021-02-13T00:00:00-06:00">February 13, 2021</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        6 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">I have 15 years worth of email traffic data, let’s take a closer look and discover some fascinating patterns.
</p>
  </article>
</div>

      
    </div>
    <a href="#page-title" class="back-to-top">Back to top &uarr;</a>
  </section>

  <section id="2020" class="taxonomy__section">
    <h2 class="archive__subtitle">2020</h2>
    <div class="entries-grid">
      
        



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/music-2020.jpg" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/music_2020/" rel="permalink">2020 in music
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2020-12-31T00:00:00-06:00">December 31, 2020</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        4 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">2020 was a great year for music, I will look back and give some thoughts on the best albums that came out in 20202.
</p>
  </article>
</div>

      
        



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/bayes-exam.jpg" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/bayes_exam/" rel="permalink">Modeling uncertainty in exam scores
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2020-11-09T00:00:00-06:00">November 9, 2020</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        3 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">We use exams to determine how much a student knows, but exams aren’t perfect. How can we estimate the uncertainty in students’ exams scores?
</p>
  </article>
</div>

      
        



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/validation-data.jpg" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/validation-size/" rel="permalink">How big should my validation set be?
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2020-08-26T00:00:00-05:00">August 26, 2020</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        12 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">Cross validation is extremely important, but how should we choose the size of our validation and test sets?
</p>
  </article>
</div>

      
        



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/lastfm.jpg" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/lastfm/" rel="permalink">How do my music preferences evolve?
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2020-08-12T00:00:00-05:00">August 12, 2020</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        5 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">I use last.fm to track my music listening. Let’s look at my data to discover how my musical preferences evolve over time.
</p>
  </article>
</div>

      
        



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/normal-data.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/normal-data/" rel="permalink">Is my data normal?
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2020-08-10T00:00:00-05:00">August 10, 2020</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        6 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">Normally distributed data is great, but how do you know whether your data is normally distributed?
</p>
  </article>
</div>

      
        



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/figure-skating.jpg" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/figure-skating/" rel="permalink">Bias in figure skating judging
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2020-06-20T00:00:00-05:00">June 20, 2020</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        3 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">Judging in figure skating is biased. Let’s use data science to figure out just how bad the issue is.
</p>
  </article>
</div>

      
        



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/imgs/teasers/first-post.jpg" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/first-post/" rel="permalink">First post
</a>
      
    </h2>
    


  <p class="page__meta">

    
      
      <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
      <time datetime="2020-06-19T07:19:44-05:00">June 19, 2020</time>
    

    
      
        <br \>
      
    

    
      
      

      <i class="far fa-fw fa-clock" aria-hidden="true"></i>
      
        less than 1 minute read
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">My first post in this blog
</p>
  </article>
</div>

      
    </div>
    <a href="#page-title" class="back-to-top">Back to top &uarr;</a>
  </section>
 -->

  </div>
</div>
    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    <!-- 
      <li><strong>Contact info and socials</strong></li>
     -->

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2022 Rik Voorhaar. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>







  <script>
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'G-Q5W21THKW2']);
  
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>









  </body>
</html>
