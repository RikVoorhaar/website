---
layout: posts
title: "Making my Kalman filters rusty"
date: 2023-05-01
categories: website data-science tools
excerpt: "In my first dive into rust, I made Kalman filters 100x faster."
header:
  teaser: "/imgs/teasers/dashboard.webp"
---

Python is fast enough until it isn't. There are many ways to speed up Python code, but
none are so good as to just write the thing that slows you down in another language.
Traditionally this is C/C++ for Python, but nowadays another language became a very
viable alternative:

> *Rust.*

You heard that right. We're putting on our sunglasses and getting ready to join the cool
kids: it's time to reimplement in rust. 

## The problem

At my job our main product uses multiple cameras to track people in 3d in real time.
Each camera detects keypoints (like head, feet, knees, etc.) for each person in view
many times a second. These detections are then send on to a central applicaton. Given
that we know where the cameras are in real space, we can then triangulate the position
of the keypoints by integrating the date from multiple cameras.

To understand how we can combine the information from multiple cameras, let's first
consider what we can learn from a single camera. When we detect, say, a person's nose in
a camera frame (a 2d image), then we know exactly in which direction this person's nose is
from the cameras point of view, _but we have no idea how far_.  In other words, we have 
a line of possible locations of the person's nose in the real 3d world. 

Of course since we roughly know how big a person is, we can estimate the distance of a
person just based on that. But this is difficult, error prone and imprecise. Instead we
can use multiple cameras. Since each camera gives us a line of possible locations for a
keypoint, with just two cameras we can find the intersection point of those two lines and 
find the 3d location of the keypoint!

This only works well in an ideal situation however, in reality there are different sources
of uncertainty, including:
- People move and cameras may not take a picture at the exact same time, and thus different
cameras are dealing with a different version of the truth.
- Observerving keypoints is inherently inprecise. Can you look at a picture of a person
wearing jeans and tell, with certainty, the exact pixel corresponding to their left knee?
Probably not, and neither can a machine learning algorithm.
- Calibrating the position and orientaton of cameras is never perfect; we are easily off
  by a centimeter or two. 
- Cameras have lens distortion. You can compensate for that, but in practice never 100%. 

We can improve the situation a bit by adding more cameras. This will give us more lines,
and we can find the point where the lines intersect the best (minimizig a (weighted)
least squares). But if you are like me, then you're probably just about ready to scream
the true solution:

> *Let's go Bayesian*

## Kalman filters

In our situaton a Kalman filter essentially boils down to the following:
- At any time we have an estimate of the position and velocity of a keypoint together
  with a measure of uncertainty.
- When time passes this estimate changes (because we think we know in which direction
  the keypoint is moving), and the uncertainty in position and velocity always
  increases. (If I saw a person move and now close my eyes, I can guess where he is
  100ms from now, but I have no idea where he is or fast he is moving next week Tuesday)
- Whenever we make a new observation we update our knowledge of the positon and
  velocity. However, since all observations are imprecise the new estimate of position
  and velocity will be a mixture of our previous estimate and current observaton.

If you formalize this and add some gaussians, you've got yourself a Kalman filter! We're
going to do exactly that right now, but feel free to skip ahead to the `</math>` tag if you feel so inclined.

The Kalman filter can be split into two parts: the prediction step, and the update step. 

## `<math>`
### Predict step

To warm up, let's see how to model a single static keypoint. We have a position
$x\in\mathbb R^3$ together with a $3\times 3$ covariance matrix $P$. These represent the
mean and variance of random variable $X(0)$ at time $0$.

When this system evolves over time, our estimated position $x$ shouldn't change, but $P$
should increase. We can model this using Brownian motion, and say that $X(t) = X(0) +
\eta W(t)$ where $W(t)$ is a Wiener process (Brownian motion) and $\eta>0$ is the noise
level. The expectaton of this is $\mathbb E(X(t))=X(0)=x$, but the variance evolves as 

$$\mathrm{Var}(X(t))= \mathrm{Var}(X(0)))+\mathrm{Var}(W(t))=P+\eta tI$$

If we also add a velocity parameter $v\in \mathbb R^3$, then things immediately become
more complicated. Looking at it like a _stochastic differential equation_ (SDE), the
static situation can be phrased as 

$$
    \mathrm dX(t) = \eta \mathrm dW(t)    
$$

If we add a velocity $Y(t)$ this then becomes
$$
\begin{cases}
    \mathrm dX(t) = Y(t)\mathrm dt+\eta_x \mathrm dW(t) \\ 
    \mathrm dY(t) = \eta_v \mathrm dW(t)
\end{cases}
$$

We can integrate this by first integrating $Y(t)$ to obtain
$$
Y(t) = Y(0)+\eta_v W(t)
$$

Which we then can plug in to get
$$
  \mathrm dX(t) = Y(0)+\eta_vW(t)+\eta_x\mathrm dW(t)
$$

And integration yields
$$
  X(t) = X(0)+tY(0)+\eta_xW(t)+\eta_v\int_0^t\!W(s)\,\mathrm ds
$$

The last term is no longer Gaussian, but we can compute its mean and variance. There are
some standard tricks for doing this, and you can show that
$$
  \mathbb E\left[\int_0^t\!W(s)\,\mathrm ds\right] = 0,\qquad\mathrm{Var}\left[\int_0^t\!W(s)\,\mathrm ds\right]=t^3/3
$$

In summary we get an predict step that looks like this:
$$
\begin{align*}
  x&\leftarrow x+vt \\
  v&\leftarrow v \\
  P_x&\leftarrow P_x+t^2P_y+\eta_xtI+(\eta_vt^3/3)I\\
  P_y&\leftarrow P_y+\eta_vtI
\end{align*}
$$

While this wasn't too bad, you can imagine that even slightly more complicated
stochastic differential equations become impossible to integrate analytically. You can
of course use numerical or Monte-Carlo methods to go around that issue, but this is far
too costly in practice. 

Fortunately, we can make a very useful approximation in the case that our prediction is
linear. In this case the function $(x,v)\mapsto (x+vt,v)$ certainly is linear. Let's denote
this map by $F(t)$. This is equivalently also a (block-diagonal) 6x6 matrix. For simplicity we bundle $(x,v)$ together into a single variable $x\in\mathbb R^6$, with variance $P$ a 6x6 matrix. If we ignore the $(\eta_vt^3/3)I$ term, we can then write the update step simply as 

$$ \begin{align*} x&\leftarrow F(t)x\\
  P&\leftarrow F(t)P\,F(t)^\top + Q, \end{align*}
$$

Where $Q=\mathrm{diag}(\eta_x,\eta_x,\eta_x,\eta_v,\eta_v,\eta_v)$.It turns out that
this formula will always hold so long as the predicted value of $x$ depends linearly on
the initial value of $x$. If $F(t)$ is not linear, then we're in trouble, but we'll see
more on that later.

### Update step

After the prediction step we went from a point $x_{k-1}$ to a point $\hat x_k$. This is
an estimate of the true state $x_k$ of the system at time $t_k$, based on our estimate of the
system's state at time $t_{k-1}$. Additionally we make an observation $z_k$ of the
system at this time. In the context of Kalman filters, an observation does not
necessarily 'live in the same space' as the state of the system. In the example of a
moving keypoint, we might observe the keypoint's position, but when we model the
keypoint, there is also an unobserved velocity that we do take into account in the
model.

To go from the 'model space' to the 'measurement space' we use a matrix $H$. For a
moving keypoint this matrix is a $3\times 6$ matrix given blockwise as $(I_3\,\mathbf 0)$
where $\mathbf 0$ is a $3\times 3$ matrixof zeros. This matrix is simply the act of
'forgetting' the velocity and only keeping the position.

Our goal for the update step is to derive an estimate $(\overline x_k, P_k)$ for the state $x_k$
at time $t_k$ using the prediction $\hat x_k$ and the observation $z_k$. In order to do
that, we need to make 3 assumptions:  
1. The estimate $(\hat x_k,\hat P_k)$ is _accurate_. That is, we assume $\hat x_k$ is a
   _sample_ of the distribution $N(x_k,\hat P_k)$.
2. The observaton $z_k$ is a direct measurement of $x_k$ with an error $R$, in other
   words $z_k$ is a sample of $N(Hx_k, R)$
3. There is a matrix $K$ (known as the _Kalman gain_) such that $\overline x_k = \hat
   x_k+K(z_k-H\hat x_k)$

The last assumption is not really an assumption of the model, but more a convenience to
make the math work out. With this assumption we can derive _the optimal_ estimate of the
true state $x_k$. None of the 3 assumptions are realistic. For example, in our simple
Bayesian analysis of the prediction step we have already noted that the prediction step
used by the Kalman filter is an approximation. The second point assumes that the
measurement error is exactly Guassian, which is rarely true in practice. Nevertheless,
these assumptions make for a _good_ model that can actually be used for practical
computations.

Deriving the optimal value of the Kalman gain $K$ is a little tricky, but we can
relatively easily use the assumptions above to find a formula for the new error estimate
$P_k$. We can define $P_k$ as $\mathrm{cov}(\overline x_k - x_k)$. Then we make several observations:
- _By assumption_ $\mathrm{cov}(x_k - \hat x_k) = \hat P_k$.
- We can write $z_k = Hx_k+v$ where $v\sim N(0,R)$. Moreover $v$ is
  independent from the other random variables.

The rest is a straightforward computation using properties of covariance:
$$
\begin{align*}
  P_k &= \mathrm{cov}(\overline x_k - x_k)\\
  &=\mathrm{cov}\left(\hat x_k -x_k + K(z_k-H\hat x_k) \right) \\
  &=\mathrm{cov}\left(\hat x_k -x_k + K(Hx_k-H\hat x_k+v) \right) \\
  &=\mathrm{cov}\left((I-K)(\hat x_k -x_k) + Kv \right) \\
  &=\mathrm{cov}((I-K)(\hat x_k -x_k)) + \mathrm{cov}(Kv) \\
  &=(I-K)\hat P_k(I-K)^\top + KRK^\top
\end{align*}
$$

The next question then is how to compute the optimal value of the Kalman gain. But first
we need to define what 'optimal' even means in this context. For Kalman filters this
means we want the value of $K$ that minimizes the _expected residual squared error_
$\mathbb{E}\|\overline x_k-x_k\|^2$. Using some tricks from statistics and some matrix
calculus we can then derive that optimal Kalman gain has the form:

$$ K = \hat P_kHS_k^{-1}, $$ 

where

$$ S_k = H\hat P_kH^\top + R, $$

which is just the covariance
matrix of the residual $z_k-H\hat x_k$. 

In summary, using several assumptions we can use the previously predicted estimate $\hat
x_{k}$ and an observation $z_k$ to get an improved estimate $\overline x_k$. And this is
all a Kalman filter is: we alternate predict and update steps to always have an
up-to-date estimate of the models state.

## Unscented Kalman filters

To derive the predicion and update steps of the Kalman filter we had to make one
'stinky' assumption. It has nothing to do with the approximation we made in the
prediction step, or the 3 assumptions I listed in the update step. Rather it's the
assumption that the transition map $F(t)$ and the measurement map $H$ are _linear_ maps. 

In the case of a moving keypoint both $F(t)$ and $H$ _were_ linear. But in most systems
this is not the case. Sometimes a linear approximation is fine, but other times it
really is not. For example, the case we're really interested in is when the measurement
$H$ is a camera observaton. Ignoring lens distortion, this is still not a linear map; it
is a projective transformaton. To understand why, consider the fact that if we move a
point very close to a camera 1cm to the left, it might move many pixels on the image,
but if that same point is several meters away then the same movement will result in a
chance of 1-2 pixels at best.

You may wonder why the assumption that $F(t)$ and $H$ are linear is necessary for a
Kalman filter. The simple reaon is that if $x$ is gaussian then so is $F(t)x$ or $Hx$. This is only true for linear maps. Now we can still _approximate_ $F(t)x$ and $Hx$ by a Gaussian using a linear apprximation of the functions $F(t)$ and $H$. This has two downsides however: a) the approximation may still be inaccurate if the functions are very non-linear, b) computing the linear approximaton requires computing a Jacobian, which can be very difficult or expensive for some functions.

In summary for a non-linear function $f$ and a gaussian $x\sim N(\mu,\Sigma)$ we need an
effective way to estimate the mean $\mathbb E(f(x))$ and covariance
$\mathrm{cov}(f(x))$. One method that always works is Monte-Carlo esimation: we simply
take lots random of samples $(x_1,\ldots,x_N)$ of $x$, and then compute the mean and
covariance of $(f(x_1),\ldots,f(x_N))$. The only issue with this is that we need many
samples in order to get an accurate estimate. 

### Sigma points and the unscented transform

Why choose random sampling, when we can actually do _deterministic sampling_ and
_choose_ a good set of points $(s_1,\dots,s_N)$. These points are called _sigma points_,
and using them we can estimate the mean using a weighted mean
$\mu=\sum_{i=1}^Nf(\sigma_i)W^a_i$. The covariance is then estimated using the estimated
mean and a second set of weights: $\Sigma = \sum_i W_i^c (f(s_i)-\mu)(f(s_i)-\mu)^\top$.
This is known as the _unscented transform_ (UT). It takes a mean and covariance of a
Gaussian $X$ and then ues sigma points to approximate the mean and covariance of the
transformed variable $f(X)$. 

But how do we choose these sigma points $s_i$ and the associated weights $W^a$ and
$W^c$? In theory any set of points would work, so long as, if $f$ is the identity
function, we get the original mean and covariance back. However, most people use a
particular algorithm developed by van der Merwe. It uses three parameters $\alpha,
\beta,\kappa$ . Then, given input mean and covariance $(\mu,\Sigma)$, it defines

$$
  s_i = \begin{cases}
  \mu & (i=0)
  \mu + \left[\sqrt{(n+\lambda)\Sigma\right]_i
  \mu - \left[\sqrt{(n+\lambda)\Sigma\right]_{i-n}
  \end{cases}
$$

where $n$ is the dimension of $\mu$ and $\lambda = \alpha^2(n+\kappa)-n$. Note the use
of the matrix square root which is well-defined for symmetric positive semidefinite
matrices -- i.e. covariance matrices -- and can for example be defined using our friend
the singular value decompositon. 
$$
\begin{align*}
  W_0^m &=\frac{\lambda}{n+\lambda} \\ 
  W_0^c &=\frac{\lambda}{n+\lambda} +1-\alpha^2+\beta\\ 
  W_i^m=W_i^c &=\frac{1}{2(n+\lambda)} \qquad (i=1,\ldots,2n)
\end{align*}
$$

Since these weights don't depend on $\mu,\Sigma$, we only have to compute them once. 

In summary, the unscented transform provides a good estimate of the mean and covariance
of $f(X)$ at the cost of:
- $2n+1$ evaluations of the function $f$
- Computing a matrix square root of an $n\times n$ matrix (cost: $O(n^3)$)

In return:
- We don't need to compute a Jacobian of $f$
- We get a more accurate estimate if $f$ is very non-linear

On top of that, the implementation is not difficult.

Finally you may wonder where the name 'unscented' came from. As I alluded to, this is
simply because creator, Jeffrey Uhlmann, thought the algorithm is neat and "doesn't
stink". 

### Unscented Kalman Filter



> Explain unscented transform and modifications needed to Kalman filter algorithm

## `</math>`

> Show how to use implementation from Python
> Show benchmarks comparing `filterpy`, with `ukf-pyrs` with python callbacks, and with rust functions

> Thoughts about implementation; why is rust nice, and why is it hard?