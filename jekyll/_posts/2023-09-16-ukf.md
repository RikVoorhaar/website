---
layout: posts
title: "Making my Kalman filters rusty"
date: 2023-05-01
categories: website data-science tools
excerpt: "In my first dive into rust, I made Kalman filters 100x faster."
header:
  teaser: "/imgs/teasers/dashboard.webp"
---

Python is fast enough until it isn't. There are many ways to speed up Python code, but
none are so good as to just write the thing that slows you down in another language.
Traditionally this is C/C++ for Python, but nowadays another language became a very
viable alternative:

> *Rust.*

You heard that right. We're putting on our sunglasses and getting ready to join the cool
kids: it's time to reimplement in rust. 

## The problem

At my job our main product uses multiple cameras to track people in 3d in real time.
Each camera detects keypoints (like head, feet, knees, etc.) for each person in view
many times a second. These detections are then send on to a central applicaton. Given
that we know where the cameras are in real space, we can then triangulate the position
of the keypoints by integrating the date from multiple cameras.

To understand how we can combine the information from multiple cameras, let's first
consider what we can learn from a single camera. When we detect, say, a person's nose in
a camera frame (a 2d image), then we know exactly in which direction this person's nose is
from the cameras point of view, _but we have no idea how far_.  In other words, we have 
a line of possible locations of the person's nose in the real 3d world. 

Of course since we roughly know how big a person is, we can estimate the distance of a
person just based on that. But this is difficult, error prone and imprecise. Instead we
can use multiple cameras. Since each camera gives us a line of possible locations for a
keypoint, with just two cameras we can find the intersection point of those two lines and 
find the 3d location of the keypoint!

This only works well in an ideal situation however, in reality there are different sources
of uncertainty, including:
- People move and cameras may not take a picture at the exact same time, and thus different
cameras are dealing with a different version of the truth.
- Observerving keypoints is inherently inprecise. Can you look at a picture of a person
wearing jeans and tell, with certainty, the exact pixel corresponding to their left knee?
Probably not, and neither can a machine learning algorithm.
- Calibrating the position and orientaton of cameras is never perfect; we are easily off
  by a centimeter or two. 
- Cameras have lens distortion. You can compensate for that, but in practice never 100%. 

We can improve the situation a bit by adding more cameras. This will give us more lines,
and we can find the point where the lines intersect the best (minimizig a (weighted)
least squares). But if you are like me, then you're probably just about ready to scream
the true solution:

> *Let's go Bayesian*

## Kalman filters

In our situaton a Kalman filter essentially boils down to the following:
- At any time we have an estimate of the position and velocity of a keypoint together
  with a measure of uncertainty.
- When time passes this estimate changes (because we think we know in which direction
  the keypoint is moving), and the uncertainty in position and velocity always
  increases. (If I saw a person move and now close my eyes, I can guess where he is
  100ms from now, but I have no idea where he is or fast he is moving next week Tuesday)
- Whenever we make a new observation we update our knowledge of the positon and
  velocity. However, since all observations are imprecise the new estimate of position
  and velocity will be a mixture of our previous estimate and current observaton.

If you formalize this and add some gaussians, you've got yourself a Kalman filter! We're
going to do exactly that right now, but feel free to skip ahead to the `</math>` tag if you feel so inclined.

The Kalman filter can be split into two parts: the prediction step, and the update step. 

## `<math>`
### Predict step

To warm up, let's see how to model a single static keypoint. We have a position
$x\in\mathbb R^3$ together with a $3\times 3$ covariance matrix $P$. These represent the
mean and variance of random variable $X(0)$ at time $0$.

When this system evolves over time, our estimated position $x$ shouldn't change, but $P$
should increase. We can model this using Brownian motion, and say that $X(t) = X(0) +
\eta W(t)$ where $W(t)$ is a Wiener process (Brownian motion) and $\eta>0$ is the noise
level. The expectaton of this is $\mathbb E(X(t))=X(0)=x$, but the variance evolves as 

$$\mathrm{Var}(X(t))= \mathrm{Var}(X(0)))+\mathrm{Var}(W(t))=P+\eta tI$$

If we also add a velocity parameter $v\in \mathbb R^3$, then things immediately become
more complicated. Looking at it like a _stochastic differential equation_ (SDE), the
static situation can be phrased as 

$$
    \mathrm dX(t) = \eta \mathrm dW(t)    
$$

If we add a velocity $Y(t)$ this then becomes
$$
\begin{cases}
    \mathrm dX(t) = Y(t)\mathrm dt+\eta_x \mathrm dW(t) \\ 
    \mathrm dY(t) = \eta_v \mathrm dW(t)
\end{cases}
$$

We can integrate this by first integrating $Y(t)$ to obtain
$$
Y(t) = Y(0)+\eta_v W(t)
$$

Which we then can plug in to get
$$
  \mathrm dX(t) = Y(0)+\eta_vW(t)+\eta_x\mathrm dW(t)
$$

And integration yields
$$
  X(t) = X(0)+tY(0)+\eta_xW(t)+\eta_v\int_0^t\!W(s)\,\mathrm ds
$$

The last term is no longer Gaussian, but we can compute its mean and variance. There are
some standard tricks for doing this, and you can show that
$$
  \mathbb E\left[\int_0^t\!W(s)\,\mathrm ds\right] = 0,\qquad\mathrm{Var}\left[\int_0^t\!W(s)\,\mathrm ds\right]=t^3/3
$$

In summary we get an predict step that looks like this:
$$
\begin{align*}
  x&\leftarrow x+vt \\
  v&\leftarrow v \\
  P_x&\leftarrow P_x+t^2P_y+\eta_xtI+(\eta_vt^3/3)I\\
  P_y&\leftarrow P_y+\eta_vtI
\end{align*}
$$

While this wasn't too bad, you can imagine that even slightly more complicated
stochastic differential equations become impossible to integrate analytically. You can
of course use numerical or Monte-Carlo methods to go around that issue, but this is far
too costly in practice. 

Fortunately, we can make a very useful approximation in the case that our prediction is
linear. In this case the function $(x,v)\mapsto (x+vt,v)$ certainly is linear. Let's denote
this map by $F(t)$. This is equivalently also a (block-diagonal) 6x6 matrix. For simplicity we bundle $(x,v)$ together into a single variable $x\in\mathbb R^6$, with variance $P$ a 6x6 matrix. If we ignore the $(\eta_vt^3/3)I$ term, we can then write the update step simply as 

$$ \begin{align*} x&\leftarrow F(t)x\\
  P&\leftarrow F(t)P\,F(t)^\top + Q, \end{align*}
$$

Where $Q=\mathrm{diag}(\eta_x,\eta_x,\eta_x,\eta_v,\eta_v,\eta_v)$.It turns out that
this formula will always hold so long as the predicted value of $x$ depends linearly on
the initial value of $x$. If $F(t)$ is not linear, then we're in trouble, but we'll see
more on that later.

### Update step Let's go back to the example of a static keypoint $x\in \mathbb R^3$. Suppose some time has passed and we have predicted that at the current time the keypoint is at $\hat x\in \mathbb r^3$ with covariance $\hat P$ (_the prior_). At the same time, we also observe that the keypoint is now actually at $z\in \mathbb R^3$, but our observaton comes with an uncertainty encoded by a covariance matrix $R$. How can we then combine the informaton of this observaton together with a our prior to get the most likely estimate (_the posterior_) of the true position of the keypoint?
This is a very typical problem in Bayesian statistics, and the trick is to find the mean
$x$ and variance $P$ that best fit all the infermation given to us (the prior and the
observation). We can make this precise by introducing a likelihood function $\mathcal L(x,P)$. A
normal distrubution with mean $x$ and variance $P$ has a probability density function
$f_{x,P}(y)$. The combined density of the prior $(\hat x,\hat P)$, observation
$(z,R)$ and posterior $(x,P)$ is then the product of these densities:

$$f_{\hat x,\hat P}(y)f_{z, R}(y)f_{x,P}(y)$$

The likelihood function is then simply the integral over all possible values of $y$:

$$\mathcal L(x,P)=\int_{y\in \R^3} f_{\hat x,\hat P}(y)f_{z, R}(y)f_{x,P}(y)\,\mathrm dy$$

The '_most likely_' value for the parameters $(x,P)$ the correspond to the value that 
_maximizes_ the likelihood function $\mathcal L(x,P)$. This value is an estimate for the 
'true' value of these parameters assuming that the keypoint $x$ actually follows a normal
distribution, and it is the _maximimum likelihood estimate_ (MLE). 

How would we actually compute this maximum? This is slightly out of scope for this blog
post, especially because we're interested in a more general result, but the trick is


....

What we actually do in the Kalman filter case is not MLE, but rather something else. We
assume that $z$ is actually a sample of $N(x, R)$. Then we postulate that there is
matrix $K$ (the Kalman gain) such that $x= \hat x+K(z - \hat x)$, and we want to find
the $K$ that minimizes $\mathbb E\|x-\hat x\|^2$. Normally when $H$ is not trivial this is actually $\hat x+K(z-H\hat x)$, but this case is simpler. 
But this doesn't make any sense, because we'd just get $K=0$, so we're missing something.

Yes, we need to add subscripts. We minimize $\mathbb E\|x-\hat x_k\|^2$ where $\hat x_k=\hat
x_{k-1}+K(z-\hat x_{k-1})$. To do this we look first at $\mathrm{cov}(x-\hat x_k)$:

$$
\mathrm{cov}(x-\hat x_k) = \mathrm{cov}(x-\hat x_{k-1}-K(z-\hat x_{k-1}))
$$

Using $z = x+v$ with $v\sim N(0,R)$ we get 

$$
 = \mathrm{cov}(x-\hat x_{k-1}-K(x+v-\hat x_{k-1})) = \mathrm{cov}((I-K)(x-\hat x_{k-1})-Kv)
$$

Since $v$ is uncorrelated we can take it out, and assuming that $\mathrm{cov}(x-\hat x_{k-1}) = P$, we get 
$$
 = (I-K)P(I-K)^\top + KRK^\top
$$

If we had $z=Hx_k+v$ and $KH\hat x_{k-1}$ then we would simply insert an $H$; it's not
too bad. The formula for the _optimal_ Kalman gain is then a little complicated, but at 
least we get the idea. I also then think it's not necessarily worth talking about the simplified example too much, because it kind of goes around the whole idea of the Kalman filter. 



## `</math>`