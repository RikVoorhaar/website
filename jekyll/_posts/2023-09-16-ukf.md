---
layout: posts
title: "Making my Kalman filters rusty"
date: 2023-05-01
categories: website data-science tools
excerpt: "In my first dive into rust, I made Kalman filters 100x faster."
header:
  teaser: "/imgs/teasers/setup.svg"
---

Python is fast enough until it isn't. There are many ways to speed up Python code, but
none are so good as to just write the thing that slows you down in another language.
Traditionally this is C/C++ for Python, but nowadays another language became a very
viable alternative:

> _Rust._

You heard that right. We're putting on our sunglasses and getting ready to join the cool
kids: it's time to reimplement in rust.

## The problem

At my job our main product uses multiple cameras to track people in 3d in real time.
Each camera detects keypoints (like head, feet, knees, etc.) for each person in view
many times a second. These detections are then send on to a central applicaton. Given
that we know where the cameras are in real space, we can then triangulate the position
of the keypoints by integrating the date from multiple cameras.

![A sketch of a stick figure observed by two cameras](/imgs/ukf/setup.svg)

To understand how we can combine the information from multiple cameras, let's first
consider what we can learn from a single camera. When we detect, say, a person's nose in
a camera frame (a 2d image), then we know exactly in which direction this person's nose is
from the cameras point of view, _but we have no idea how far_. In other words, we have
a line of possible locations of the person's nose in the real 3d world.

![A sketch of a stick figure scaled to different sizes](/imgs/ukf/different-scales.svg)

Of course since we roughly know how big a person is, we can estimate the distance of a
person just based on that. But this is difficult, error prone and imprecise. Instead we
can use multiple cameras. Since each camera gives us a line of possible locations for a
keypoint, with just two cameras we can find the intersection point of those two lines and
find the 3d location of the keypoint!

This only works well in an ideal situation however, in reality there are different sources
of uncertainty, including:

- People move and cameras may not take a picture at the exact same time, and thus different
  cameras are dealing with a different version of the truth.
- Observerving keypoints is inherently inprecise. Can you look at a picture of a person
  wearing jeans and tell, with certainty, the exact pixel corresponding to their left knee?
  Probably not, and neither can a machine learning algorithm.
- Calibrating the position and orientaton of cameras is never perfect; we are easily off
  by a centimeter or two.
- Cameras have lens distortion. You can compensate for that, but in practice never 100%.

We can improve the situation a bit by adding more cameras. This will give us more lines,
and we can find the point where the lines intersect the best (minimizig a (weighted)
least squares). But if you are like me, then you're probably just about ready to scream
the true solution:

> _Let's go Bayesian_

## Kalman filters

In our situaton a Kalman filter essentially boils down to the following:

- At any time we have an estimate of the position and velocity of a keypoint together
  with a measure of uncertainty.
- When time passes this estimate changes (because we think we know in which direction
  the keypoint is moving), and the uncertainty in position and velocity always
  increases. (If I saw a person move and now close my eyes, I can guess where he is
  100ms from now, but I have no idea where he is or fast he is moving next week Tuesday)
- Whenever we make a new observation we update our knowledge of the positon and
  velocity. However, since all observations are imprecise the new estimate of position
  and velocity will be a mixture of our previous estimate and current observaton.

If you formalize this and add some gaussians, you've got yourself a Kalman filter! We're
going to do exactly that right now, but feel free to skip ahead to the `</math>` tag if you feel so inclined.

The Kalman filter can be split into two parts: the prediction step, and the update step.

## `<math>`

### Predict step

To warm up, let's see how to model a single static keypoint. We have a position
$$x\in\mathbb R^3$$ together with a $$3\times 3$$ covariance matrix $$P$$. These represent the
mean and variance of random variable $$X(0)$$ at time $$0$$.

When this system evolves over time, our estimated position $$x$$ shouldn't change, but $$P$$
should increase. We can model this using Brownian motion, and say that $$X(t) = X(0) +
\eta W(t)$$ where $$W(t)$$ is a Wiener process (Brownian motion) and $$\eta>0$$ is the noise
level. The expectaton of this is $$\mathbb E(X(t))=X(0)=x$$, but the variance evolves as

$$\mathrm{Var}(X(t))= \mathrm{Var}(X(0)))+\mathrm{Var}(W(t))=P+\eta tI$$

If we also add a velocity parameter $$v\in \mathbb R^3$$, then things immediately become
more complicated. Looking at it like a _stochastic differential equation_ (SDE), the
static situation can be phrased as

$$
    \mathrm dX(t) = \eta \mathrm dW(t)
$$

If we add a velocity $$Y(t)$$ this then becomes

$$
\begin{cases}
    \mathrm dX(t) = Y(t)\mathrm dt+\eta_x \mathrm dW(t) \\
    \mathrm dY(t) = \eta_v \mathrm dW(t)
\end{cases}
$$

We can integrate this by first integrating $$Y(t)$$ to obtain

$$
Y(t) = Y(0)+\eta_v W(t)
$$

Which we then can plug in to get

$$
  \mathrm dX(t) = Y(0)+\eta_vW(t)+\eta_x\mathrm dW(t)
$$

And integration yields

$$
  X(t) = X(0)+tY(0)+\eta_xW(t)+\eta_v\int_0^t\!W(s)\,\mathrm ds
$$

The last term is no longer Gaussian, but we can compute its mean and variance. There are
some standard tricks for doing this, and you can show that

$$
  \mathbb E\left[\int_0^t\!W(s)\,\mathrm ds\right] = 0,\qquad\mathrm{Var}\left[\int_0^t\!W(s)\,\mathrm ds\right]=t^3/3
$$

In summary we get an predict step that looks like this:

$$
\begin{align*}
  \hat x_k&\leftarrow x_{k-1}+v_{k-1}t \\
  \hat v_k&\leftarrow v_{k-1} \\
  \hat P_{x,k}&\leftarrow P_{x,k-1}+t^2P_y+\eta_xtI+(\eta_vt^3/3)I\\
  P_y&\leftarrow P_y+\eta_vtI
\end{align*}
$$

While this wasn't too bad, you can imagine that even slightly more complicated
stochastic differential equations become impossible to integrate analytically. You can
of course use numerical or Monte-Carlo methods to go around that issue, but this is far
too costly in practice.

Fortunately, we can make a very useful approximation in the case that our prediction is
linear. In our case the function $$(x,v)\mapsto (x+vt,v)$$ certainly is linear. Let's
denote this map by $$F_t$$. The map $$F_t$$ is actually just a (block-diagonal) 6x6
matrix. For simplicity we bundle $$(x_k,v_k)$$ together into a single variable
$$x_k\in\mathbb R^6$$, with variance $$P$$ a $$6\times 6$$ matrix. If we ignore the
$$(\eta_vt^3/3)I$$ term, we can then write the update step simply as

$$
\begin{align*} \hat x_k&\leftarrow F_t\overline{x_{k-1}}\\
  \hat P_k&\leftarrow F_tP_{k-1}\,F_t^\top + Q, \end{align*}
$$

where $$Q=\mathrm{diag}(\eta_x,\eta_x,\eta_x,\eta_v,\eta_v,\eta_v)$$. Also note that we
wrote $$\overline{x_{k-1}}$$ for the _estimate_ of the state at step $$k-1$$; we of course
don't know the real value. It turns out that this formula will always hold so long as the
predicted value of $$x$$ depends linearly on the initial value of $$x$$. If $$F_t$$ is
not linear, then we're in trouble, but we'll see more on that later.

![A mathematical diagram explaining the predict step](/imgs/ukf/predict-step.svg)

### Update step

After the prediction step we went from a point $$x_{k-1}$$ to a point $$\hat x_k$$. This is
an estimate of the true state $$x_k$$ of the system at time $$t_k$$, based on our estimate of the
system's state at time $$t_{k-1}$$. Additionally we make an observation $$z_k$$ of the
system at this time. In the context of Kalman filters, an observation does not
necessarily 'live in the same space' as the state of the system. In the example of a
moving keypoint, we might observe the keypoint's position, but when we model the
keypoint, there is also an unobserved velocity that we do take into account in the
model.

To go from the 'model space' to the 'measurement space' we use a matrix $$H$$. For a
moving keypoint this matrix is a $$3\times 6$$ matrix given blockwise as $$(I_3\,\mathbf 0)$$
where $$\mathbf 0$$ is a $$3\times 3$$ matrixof zeros. This matrix is simply the act of
'forgetting' the velocity and only keeping the position.

Our goal for the update step is to derive an estimate $$(\overline x_k, P_k)$$ for the state $$x_k$$
at time $$t_k$$ using the prediction $$\hat x_k$$ and the observation $$z_k$$. In order to do
that, we need to make 3 assumptions:

1. The estimate $$(\hat x_k,\hat P_k)$$ is _accurate_. That is, we assume $$\hat x_k$$ is a
   _sample_ of the distribution $$N(x_k,\hat P_k)$$.
2. The observaton $$z_k$$ is a direct measurement of $$x_k$$ with an error $$R$$, in other
   words $$z_k$$ is a sample of $$N(Hx_k, R)$$
3. There is a matrix $$K$$ (known as the _Kalman gain_) such that $$\overline x_k = \hat
   x_k+K(z_k-H\hat x_k)$$

The last assumption is not really an assumption of the model, but more a convenience to
make the math work out. With this assumption we can derive _the optimal_ estimate of the
true state $$x_k$$. None of the 3 assumptions are realistic. For example, in our simple
Bayesian analysis of the prediction step we have already noted that the prediction step
used by the Kalman filter is an approximation. The second point assumes that the
measurement error is exactly Guassian, which is rarely true in practice. Nevertheless,
these assumptions make for a _good_ model that can actually be used for practical
computations.

![A mathematical diagram explaining the update step](/imgs/ukf/update-step.svg)

Deriving the optimal value of the Kalman gain $$K$$ is a little tricky, but we can
relatively easily use the assumptions above to find a formula for the new error estimate
$$P_k$$. We can define $$P_k$$ as $$\mathrm{cov}(\overline x_k - x_k)$$. Then we make several observations:

- _By assumption_ $$\mathrm{cov}(x_k - \hat x_k) = \hat P_k$$.
- We can write $$z_k = Hx_k+v$$ where $$v\sim N(0,R)$$. Moreover $$v$$ is
  independent from the other random variables.

The rest is a straightforward computation using properties of covariance:

$$
\begin{align*}
  P_k &= \mathrm{cov}(\overline x_k - x_k)\\
  &=\mathrm{cov}\left(\hat x_k -x_k + K(z_k-H\hat x_k) \right) \\
  &=\mathrm{cov}\left(\hat x_k -x_k + K(Hx_k-H\hat x_k+v) \right) \\
  &=\mathrm{cov}\left((I-K)(\hat x_k -x_k) + Kv \right) \\
  &=\mathrm{cov}((I-K)(\hat x_k -x_k)) + \mathrm{cov}(Kv) \\
  &=(I-K)\hat P_k(I-K)^\top + KRK^\top
\end{align*}
$$

The next question then is how to compute the optimal value of the Kalman gain. But first
we need to define what 'optimal' even means in this context. For Kalman filters this
means we want the value of $$K$$ that minimizes the _expected residual squared error_
$$\mathbb{E}\|\overline x_k-x_k\|^2$$. Using some tricks from statistics and some matrix
calculus we can then derive that optimal Kalman gain has the form:

$$ K = \hat P_kHS_k^{-1}, $$

where

$$ S_k = H\hat P_kH^\top + R, $$

which is just the covariance
matrix of the residual $$z_k-H\hat x_k$$.

In summary, using several assumptions we can use the previously predicted estimate $$\hat
x_{k}$$ and an observation $$z_k$$ to get an improved estimate $$\overline x_k$$. And this is
all a Kalman filter is: we alternate predict and update steps to always have an
up-to-date estimate of the models state.

## Unscented Kalman filters

To derive the predicion and update steps of the Kalman filter we had to make one
'stinky' assumption. It has nothing to do with the approximation we made in the
prediction step, or the 3 assumptions I listed in the update step. Rather it's the
assumption that the transition map $$F_t$$ and the measurement map $$H$$ are _linear_ maps.

In the case of a moving keypoint both $$F_t$$ and $$H$$ _were_ linear. But in most systems
this is not the case. Sometimes a linear approximation is fine, but other times it
really is not. For example, the case we're really interested in is when the measurement
$$H$$ is a camera observaton. Ignoring lens distortion, this is still not a linear map; it
is a projective transformaton. To understand why, consider the fact that if we move a
point very close to a camera 1cm to the left, it might move many pixels on the image,
but if that same point is several meters away then the same movement will result in a
chance of 1-2 pixels at best.

You may wonder why the assumption that $$F_t$$ and $$H$$ are linear is necessary for a
Kalman filter. The simple reaon is that if $$x$$ is gaussian then so is $$F_tx$$ or $$Hx$$. This is only true for linear maps. Now we can still _approximate_ $$F_tx$$ and $$Hx$$ by a Gaussian using a linear apprximation of the functions $$F_t$$ and $$H$$. This has two downsides however: a) the approximation may still be inaccurate if the functions are very non-linear, b) computing the linear approximaton requires computing a Jacobian, which can be very difficult or expensive for some functions.

In summary for a non-linear function $$f$$ and a gaussian $$x\sim N(\mu,\Sigma)$$ we need an
effective way to estimate the mean $$\mathbb E(f(x))$$ and covariance
$$\mathrm{cov}(f(x))$$. One method that always works is Monte-Carlo esimation: we simply
take lots random of samples $$(x_1,\ldots,x_N)$$ of $$x$$, and then compute the mean and
covariance of $$(f(x_1),\ldots,f(x_N))$$. The only issue with this is that we need many
samples in order to get an accurate estimate.

### Sigma points and the unscented transform

Why choose random sampling, when we can actually do _deterministic sampling_ and
_choose_ a good set of points $$(s_1,\dots,s_N)$$. These points are called _sigma points_,
and using them we can estimate the mean using a weighted mean
$$\mu=\sum_{i=1}^Nf(s_i)W^a_i$$. The covariance is then estimated using the estimated
mean and a second set of weights: $$\Sigma = \sum_i W_i^c (f(s_i)-\mu)(f(s_i)-\mu)^\top$$.
This is known as the _unscented transform_ (UT). It takes a mean and covariance of a
Gaussian $$X$$ and then ues sigma points to approximate the mean and covariance of the
transformed variable $$f(X)$$.

But how do we choose these sigma points $$s_i$$ and the associated weights $$W^a$$ and
$$W^c$$? In theory any set of points would work, so long as, if $$f$$ is the identity
function, we get the original mean and covariance back. However, most people use a
particular algorithm developed by van der Merwe. It uses three parameters $$\alpha,
\beta,\kappa$$ . Then, given input mean and covariance $$(\mu,\Sigma)$$, it defines

$$
  s_i = \begin{cases}
  \mu & (i=0) \\
  \mu + \left[\sqrt{(n+\lambda)\Sigma}\right]_i & (i=1,\ldots,n) \\
  \mu - \left[\sqrt{(n+\lambda)\Sigma}\right]_{i-n} & (i=n+1,\ldots,2n)

  \end{cases}
$$

where $$n$$ is the dimension of $$\mu$$ and $$\lambda = \alpha^2(n+\kappa)-n$$. Note the use
of the matrix square root which is well-defined for symmetric positive semidefinite
matrices -- i.e. covariance matrices -- and can for example be defined using our friend
the singular value decompositon.

$$
\begin{align*}
  W_0^m &=\frac{\lambda}{n+\lambda} \\
  W_0^c &=\frac{\lambda}{n+\lambda} +1-\alpha^2+\beta\\
  W_i^m=W_i^c &=\frac{1}{2(n+\lambda)} \qquad (i=1,\ldots,2n)
\end{align*}
$$

Since these weights don't depend on $$\mu,\Sigma$$, we only have to compute them once.

In summary, the unscented transform provides a good estimate of the mean and covariance
of $$f(X)$$ at the cost of:

- $$2n+1$$ evaluations of the function $$f$$
- Computing a matrix square root of an $$n\times n$$ matrix (cost: $$O(n^3)$$)

In return:

- We don't need to compute a Jacobian of $$f$$
- We get a more accurate estimate if $$f$$ is very non-linear

On top of that, the implementation is not difficult.

Finally you may wonder where the name 'unscented' came from. As I alluded to, this is
simply because creator, Jeffrey Uhlmann, thought the algorithm is neat and "doesn't
stink".

### Unscented Kalman Filter

With the unscented transform in hand, we only need minor modifications to the Kalman
filter algorithm to allow for non-linear functions. First the predict step. Instead of a
linear map $$F_t$$, we not have a non-linear 'process model' $$f_t$$. Let's consider the
predict step as it was before:

$$
\begin{align*}
  \hat x_k&\leftarrow F_tx_{k-1}\\
  \hat P_k&\leftarrow F_tP_{k-1}\,F_t^\top + Q,
\end{align*}
$$

What happens here is that we have as input a Gaussian $$(x,P)$$, we then transform it with
$$F_t$$ to get a Gaussian $$(F_tx,\, F_tPF_t^\top)$$, and then finally we add some extra
noise in the shape of $$Q$$. All we have to do is replace the new estimate of the mean and
covariance with that provided by the unscented transform, resulting in the predict step:

$$
\begin{align*}
  \hat x_k&\leftarrow\sum_{i=1}^Nf_t(s_i)W^a_i\\
  \hat P_k&\leftarrow \sum_i W_i^c (f_t(s_i)-\hat x_k)(f_t(s_i)-\hat x_k)^\top + Q,
\end{align*}
$$

where $$s_i$$ denote the sigma points computed from $$(x_{k-1},P_{k-1})$$. This may look
more complicated than what we had before, but if we denote the unscented transform by
$$\mathrm{UT}[f_t]$$ then this is just:

$$
\begin{align*}
  (\hat x_{k},\hat P_{k})\leftarrow \mathrm{UT}[f_t] (x_{k-1},P_{k-1})+(0, Q)\\
\end{align*}
$$

Which actually looks very simple! This also shows that the unscencted transform plays no
special role here; you could substitute _any_ method for estimating the mean/covariance
of the transformed variable $$f_t(X)$$. We could even drop the assumption that the state
is Gaussian, so long as we have a parametric distribution where we can estimate the
parameters afte applying a non-linear function.

Then what about the predict step? We're working with an estimate $$(\hat x_k,\hat P_k)$$
and an observation $$(z_k, R)$$. Instead of a measurment matrix $$H$$, we have a measurment
function $$h$$. The first thing we do is use the unscented transform to transform the
estimate $$(\hat x_k, \hat P_k)$$ to the measurement space:

$$
  (\mu_z,\Sigma_z)\leftarrow \mathrm{UT}[h](\hat x_k,\hat P_k)+(0,R)
$$

Next we can express the matrix $$S$$ as the cross-variance beween $$(\mu_z,\Sigma_z)$$ and $$(\hat x_k,\hat P_k)$$. If $$\{s_i\}$$ are the sigma points associated to $$\mathrm{UT}[h](\hat x_k,\hat P_k)$$, then this cross variane $$P_{xz}$$ is defined by:

$$
P_{xz} = \sum_i W_i^c(s_i-\hat x_k)(h(s_i)-\mu_z)^\top
$$

This cross-variance takes the role of the matrix $$S$$ we had before. Then the Kalman gain
can be computed as 

$$
K=P_{xz} P_{z}^{-1}
$$

And then finally we get the new estimate $$(x_k,P_k)$$ defined by:

$$
\begin{align*}
  x_k&\leftarrow \hat x_k+K(z_k-\mu_z)\\
  P_k&\leftarrow \hat P_k-KP_{z}K^\top
\end{align*}
$$

And so the unscented kalman filter is born! This concludes the mathematical part of this
blog post, and next up we're going to have a look at implementing this.


## `</math>`

## `<code>`

Let's see the unscented Kalman filter in action on a real problem. This real problem also serves as a benchmark to understand why we need to speed things up in the first place. 

### Model problem

Let's go back to the beginning of this post. The setup is that we have 2 or more cameras that observe a moving person. Some machine learning algorithm will give us the 'keypoints' on the skeleton of the person (e.g. nose, left elbow, right knee, etc.). Our job is to turn these 2D positions (pixels on a camera) to positions in the real 3D world. 

I made a little simultation of a keypoint moving around in 3D and then projected it to the point of view of two cameras. Below you can see two plots showing what each of the two cameras can see. Due to he noise I added the cameras don't see a smooth curve at all. Real curves produced by applying machine learning pose detection algorithms on footage of moving people tends to look like this. 


<details>
<summary>Click here to see the code for the sumulation</summary>

{% highlight python %}

from time import perf_counter
from typing import Callable

import matplotlib.pyplot as plt
import numpy as np
from scipy.ndimage import gaussian_filter1d

from ukf_pyrs import (
    UKF,
    FirstOrderTransitionFunction,
    SigmaPoints,
    measurement_function,
    transition_function,
    UKFParallel,
)
from ukf_pyrs.pinhole_camera import CameraProjector, PinholeCamera
import argparse


N_points = 50
point_a = np.array([-50, 0, 0])
point_b = np.array([0, 120, 130])
point_c = np.array([10, -10, 10]) * 2
t = np.linspace(0, 1, N_points)
dt = t[1] - t[0]
points = (1 - t)[:, None] * point_a[None, :] + t[:, None] * point_b[None, :]
points += (np.cos(t * np.pi) ** 2)[:, None] * point_c[None, :]
points = points.astype(np.float32)
np.random.seed(0)
rand_scale = 1
points_rand = (points + np.random.randn(*points.shape) * rand_scale).astype(np.float32)

cam1 = PinholeCamera.from_params(
    camera_position=np.array([50, 100, 0]),
    lookat_target=np.array([0, 0, 0]),
    fov_x_degrees=90,
    resolution=np.array([640, 480]),
)
cam2 = PinholeCamera.from_params(
    camera_position=np.array([-50, 80, 0]),
    lookat_target=np.array([0, 0, 0]),
    fov_x_degrees=90,
    resolution=np.array([640, 480]),
)
proj_points_obs1 = cam1.project(np.ascontiguousarray(points_rand[0::2])).reshape(-1, 2)
proj_points1 = cam1.project(points).reshape(-1, 2)
proj_points_obs2 = cam2.project(np.ascontiguousarray(points_rand[1::2])).reshape(-1, 2)
proj_points2 = cam2.project(points).reshape(-1, 2)
{% endhighlight %}


We can then plot this simulation:

{% highlight python %}

plt.figure(figsize=(8, 4))
plt.subplot(1,2,1)
plt.plot(*proj_points_obs1.T)
plt.xlabel("x")
plt.ylabel("y")
plt.title("View from camera #1")

plt.subplot(1,2,2)
plt.plot(*proj_points_obs2.T)
plt.xlabel("x")
plt.ylabel("y")
plt.title("View from camera #2")

{% endhighlight %}

</details>
    
![png](/imgs/ukf/blog_post_3_1.png)
    


Then to model this problem we will use a model dimension of $$6$$ (position+velocity) and 
a measurement dimension of 2 (pixels on a screen). 

The measurement function takes in two arguments; the position and an int denoting the camera number (this is something that we'll get back to later). To turn this function into an object that Rust can understand, we use the `@measurement_function` decorator.
The model transition function is just $$(x,\,v)\mapsto (x+v\Delta t,\,v)$$, and again we use a decorator to make our Rust implementation understand it.


```python
dim_x = 6
dim_z = 2


@measurement_function(dim_z)
def h_py(x: np.ndarray, cam_id: int) -> np.ndarray:
    pos = x[:3]
    if cam_id == 0:
        return cam1.world_to_screen_single(pos)
    elif cam_id == 1:
        return cam2.world_to_screen_single(pos)
    else:
        return np.zeros(2, dtype=np.float32)


@transition_function
def f_py(x: np.ndarray, dt: float) -> np.ndarray:
    pos = x[:3]
    vel = x[3:]
    return np.concatenate([pos + vel * dt, vel])
```

Next we need to instantiate a class that can generate sigma points, and one that actually does the work of the Kalman filter. We then also set the values of the matrix $$Q$$, $$R$$ and the initial value of the matrix $$P$$ for the filter. 

We actually only changed the value of $$Q$$ from its default, it turned out to be OK for $$P$$ and $$R$$ to be the identity matrix. If you have a really good theoretical model, it can be possible to derive good values for these matrices. In practice though, you're just going to have to fiddle around with it. 


```python
sigma_points = SigmaPoints.merwe(dim_x, 0.5, 2, -2)
kalman_filter = UKF(dim_x, dim_z, h_py, f_py, sigma_points)


kalman_filter.Q = np.diag([1e-2] * 3 + [3e1] * 3).astype(np.float32)
kalman_filter.R = np.eye(2).astype(np.float32)
kalman_filter.P = np.eye(6).astype(np.float32)
```

Now we're going to alternatively do a predict and update step from the point of view of either camera. To tell the Kalman filter which camera is making the observation we use the `update_measurement_context` method. Whatever value we pass here is what's going to get passed to the measurement function $$h$$. 




```python
predictions_list = []
for p1, p2 in zip(
    proj_points_obs1, proj_points_obs2
):
    kalman_filter.update_measurement_context(0)
    kalman_filter.predict(dt)
    kalman_filter.update(p1)
    predictions_list.append(kalman_filter.x)

    kalman_filter.update_measurement_context(1)
    kalman_filter.predict(dt)
    kalman_filter.update(p2)
    predictions_list.append(kalman_filter.x)

predictions = np.array(predictions_list)  # type: ignore
pos_predictions = predictions[:, :3]
```

Then finally we are plotting the result below. We see that the Kalman filter is able to track the keypoint quite well in this problem. After taking a bit of time to settle it even gives an accurate estimate of the velocity of the keypoint!

<details>
<summary> Click to see the code for the plots below</summary>


{% highlight python%}
plt.figure(figsize=(8, 8))
plt.subplot(2, 2, 1)
plt.title("Camera 1 reprojected tracking")
plt.plot(
    *cam1.project(np.array(pos_predictions)).reshape(-1, 2).T, ".-", label="Predicted"
)
plt.plot(*proj_points_obs1.T, ".", label="Observed")
plt.plot(*proj_points1.T, "-", label="True")
plt.legend()

plt.subplot(2, 2, 2)
plt.title("Camera 2 reprojected tracking")
plt.plot(
    *cam2.project(np.array(pos_predictions)).reshape(-1, 2).T, ".-", label="Predicted"
)
plt.plot(*proj_points_obs2.T, ".", label="Observed")
plt.plot(*proj_points2.T, "-", label="True")
plt.legend()


plt.subplot(2, 2, 3)
plt.title("Tracking error (smoothened)")
sigma = 5
tracking_errors = np.linalg.norm(points - pos_predictions, axis=1)
tracking_errors_rand = np.linalg.norm(points_rand - pos_predictions, axis=1)
plt.plot(
    gaussian_filter1d(tracking_errors, sigma),
    label="Unperturbed",
)
plt.plot(
    gaussian_filter1d(tracking_errors_rand, sigma),
    label="Perturbed",
)
plt.ylim(0, np.mean(tracking_errors) * 2)
plt.legend()

plt.subplot(2, 2, 4)
plt.title("Tracking of velocity")
colors = plt.rcParams["axes.prop_cycle"].by_key()["color"]
names = ["x", "y", "z"]
for i, v in enumerate(predictions[:, 3:].T):
    plt.plot(v, color=colors[i], label=names[i])

velocity = np.diff(points, axis=0) / dt
for i, v in enumerate(velocity.T):
    plt.plot(v, color=colors[i], linestyle="--")
plt.legend()
plt.show()
{% endhighlight %}

</details>

    
![png](/imgs/ukf/blog_post_11_0.png)
    


## Make it blazingly fast

The implementation above was actually alreay fully in rust, and is a lot faster than the Python library `filterpy` which I based my implementation on. You might have however noticed that I still use python to define the measurement and transition funcions. My rust implementation actually calls these Python functions directly. This is fantastic if you are in a design stage (i.e., you don't know _which_ function you need precisely), but calling these Python functions has a significant overhead. 

Fortunately it's not a lot of work to make rust implementations of these functions and use those instead. To make it more interesting, we will be tracking not just a single keypoint, but 30 keypoints in parallel. This is much closer to the actual use case because a skeleton consists of many keypoints. 

Rather than looking at the accuracy, we're just looking at speed. 


<details>
<summary> Click to see the setup code and code for the rust benchmark</summary>


{% highlight python %}
from time import perf_counter

import matplotlib.pyplot as plt
import numpy as np
from scipy.ndimage import gaussian_filter1d

from ukf_pyrs import (
    UKF,
    FirstOrderTransitionFunction,
    SigmaPoints,
    measurement_function,
    transition_function,
    UKFParallel,
)
from ukf_pyrs.pinhole_camera import CameraProjector, PinholeCamera
import argparse


N_points = 10000
point_a = np.array([-50, 0, 0])
point_b = np.array([0, 120, 130])
point_c = np.array([10, -10, 10]) * 2
t = np.linspace(0, 1, N_points)
dt = t[1] - t[0]
points = (1 - t)[:, None] * point_a[None, :] + t[:, None] * point_b[None, :]
points += (np.cos(t * np.pi) ** 2)[:, None] * point_c[None, :]
points = points.astype(np.float32)
np.random.seed(0)
rand_scale = 1
points_rand = (points + np.random.randn(*points.shape) * rand_scale).astype(np.float32)

cam1 = PinholeCamera.from_params(
    camera_position=np.array([50, 100, 0]),
    lookat_target=np.array([0, 0, 0]),
    fov_x_degrees=90,
    resolution=np.array([640, 480]),
)
cam2 = PinholeCamera.from_params(
    camera_position=np.array([-50, 80, 0]),
    lookat_target=np.array([0, 0, 0]),
    fov_x_degrees=90,
    resolution=np.array([640, 480]),
)
proj_points_obs1 = cam1.project(np.ascontiguousarray(points_rand[0::2])).reshape(-1, 2)
proj_points1 = cam1.project(points).reshape(-1, 2)
proj_points_obs2 = cam2.project(np.ascontiguousarray(points_rand[1::2])).reshape(-1, 2)
proj_points2 = cam2.project(points).reshape(-1, 2)


cam1 = PinholeCamera.from_params(
    camera_position=np.array([50, 100, 0]),
    lookat_target=np.array([0, 0, 0]),
    fov_x_degrees=90,
    resolution=np.array([640, 480]),
)
cam2 = PinholeCamera.from_params(
    camera_position=np.array([-50, 80, 0]),
    lookat_target=np.array([0, 0, 0]),
    fov_x_degrees=90,
    resolution=np.array([640, 480]),
)

{% endhighlight %}


{% highlight python %}

hx_rust = CameraProjector([cam1.to_rust(), cam2.to_rust()])
fx_rust = FirstOrderTransitionFunction(3)
sigma_points = SigmaPoints.merwe(6, 0.5, 2, -2)

kalman_filters = []
n_filters = 30
for _ in range(n_filters):
    kalman_filter = UKF(6, 2, hx_rust, fx_rust, sigma_points)
    kalman_filter.Q = np.diag([1] * 3 + [3e3] * 3).astype(np.float32)
    kalman_filter.Q *= 1e-2
    kalman_filter.R = np.diag([1, 1]).astype(np.float32) * 1e0
    kalman_filter.P = np.diag([1e0] * 3 + [1e0] * 3).astype(np.float32)
    kalman_filters.append(kalman_filter)


time_begin = perf_counter()
parallel_ukf = UKFParallel(kalman_filters)
for p1, p2 in zip(
    proj_points_obs1.astype(np.float32), proj_points_obs2.astype(np.float32)
):
    parallel_ukf.update_measurement_context([0] * n_filters)
    parallel_ukf.predict(dt)

    parallel_ukf.update([p1 + i for i in range(n_filters)])

    parallel_ukf.update_measurement_context([1] * n_filters)
    parallel_ukf.predict(dt)

    parallel_ukf.update([p2 + i for i in range(n_filters)])

time_end = perf_counter()

total_keypoints = n_filters * len(proj_points_obs1) *2 
time_per_keypoint_rust = (time_end - time_begin) / total_keypoints
print(f"Time per keypoint (rust): {time_per_keypoint_rust*1e6:0.1f} µs")
{% endhighlight %}

</details>  


    Time per keypoint (rust): 8.1 µs

<details>
<summary>Click hee tosee the code for the python benchmark</summary>

{% highlight python %}

hx_rust = CameraProjector([cam1.to_rust(), cam2.to_rust()])
fx_rust = FirstOrderTransitionFunction(3)
sigma_points = SigmaPoints.merwe(6, 0.5, 2, -2)

kalman_filters = []
n_filters = 30
n_points_py = 200
for _ in range(n_filters):
    kalman_filter = UKF(6, 2, h_py, f_py, sigma_points)
    kalman_filter.Q = np.diag([1] * 3 + [3e3] * 3).astype(np.float32)
    kalman_filter.Q *= 1e-2
    kalman_filter.R = np.diag([1, 1]).astype(np.float32) * 1e0
    kalman_filter.P = np.diag([1e0] * 3 + [1e0] * 3).astype(np.float32)
    kalman_filters.append(kalman_filter)


time_begin = perf_counter()
parallel_ukf = UKFParallel(kalman_filters)
for p1, p2 in zip(
    proj_points_obs1[:n_points_py].astype(np.float32), proj_points_obs2[:n_points_py].astype(np.float32)
):
    parallel_ukf.update_measurement_context([0] * n_filters)
    parallel_ukf.predict(dt)

    parallel_ukf.update([p1 + i for i in range(n_filters)])

    parallel_ukf.update_measurement_context([1] * n_filters)
    parallel_ukf.predict(dt)

    parallel_ukf.update([p2 + i for i in range(n_filters)])

total_keypoints = n_points_py*2 *n_filters
time_end = perf_counter()
time_per_keypoint_py = (time_end - time_begin) / total_keypoints
print(f"Time per keypoint (Python): {time_per_keypoint_py*1e6:0.1f} µs")

rust_speedup = time_per_keypoint_py / time_per_keypoint_rust
print(f"Rust speedup: {rust_speedup:0.1f}x")

{% endhighlight %}

</details>

    Time per keypoint (Python}: 120.4 µs
    Rust speedup: 14.9x


That's 15x speedup just for changing out a python functon for a rust one! Here I used a class `UKFParallel` that takes a list of Kalman filters as input, and allows calling the update/predict functions for each Kalman filter in a batch. This is better than than using a for loop in Python, since we can limit the time spent interacting with the GIL. 

Originally the idea of this class was to actually evaluate the predict / update
functions in parallel, but even when the the measurement and transition functions are
rust native, we still end up waiting most of the time for Python to release the GIL. The
design decision to make it very easy to use from python also means that proper
parallelization requires redesigning parts of the codebase. 

But more importantly, how does this compare to `filterpy`? 

<details>
<summary>Click here to see the Filterpy code</summary>


{% highlight python %}
from filterpy.kalman import UnscentedKalmanFilter
from filterpy.kalman.sigma_points import MerweScaledSigmaPoints

dim_x = 6
dim_z = 2
sigma_points = MerweScaledSigmaPoints(n=6, alpha=0.5, beta=2, kappa=-2)
def fx(x, dt):
    x = x.copy()
    x[:3] += x[3:6] * dt
    return x
ukf = UnscentedKalmanFilter(
    dim_x=dim_x,
    dim_z=dim_z,
    dt=dt,
    fx=fx,
    hx=None,
    points=sigma_points,
)
ukf.Q=np.diag([1] * 3 + [3e3] * 3)*1e-2
ukf.R=np.eye(dim_z)
ukf.P=np.diag([1] * 3 + [3e3] * 3)
def make_measerument_functon(cam:PinholeCamera)->Callable[[np.ndarray],np.ndarray]:
    def hx(x):
        return cam.world_to_screen_single(x[:3])
    return hx

hx1 = make_measerument_functon(cam1)
hx2 = make_measerument_functon(cam2)

time_begin = perf_counter()
for obs1,obs2 in zip(proj_points_obs1, proj_points_obs2):
    try:
        ukf.predict(dt)
    except np.linalg.LinAlgError:
        pass
    ukf.hx = hx1
    ukf.update(obs1)
    try:
        ukf.predict(dt)
    except np.linalg.LinAlgError:
        pass
    ukf.hx = hx2
time_end = perf_counter()

n_keypoints = len(proj_points_obs1) *2 
time_per_keypoint_filterpy = (time_end - time_begin) / n_keypoints
print(f"Time per keypoint (filterpy): {time_per_keypoint_filterpy*1e6:0.1f} µs")

print(f"rust speedup: {time_per_keypoint_filterpy / time_per_keypoint_rust:.1f}x")

{% endhighlight %}

</details>

    Time per keypoint (filterpy): 176.4 µs
    rust speedup: 21.8x


We see that `filterpy` is around 22x times slower than my implementation using Rust native functions. Furthermore, on this computer using the filterpy implementation we would spend around 5ms to process 30 keypoints (without multithreading). In practice this means that if we're processing data from a 30fps camera stream, we would be spending roughly 1/6th of a frame just on the Kalman filter logic. With the rust implementation this is only 1/137th of a frame -- which gives us much more time for other logic. Since we're processing data from multiple camera streams in parallel, this is a big deal! 

## Thoughts on using rust 

This was my first project using Rust, and as a result I have learned a lot. I will take this time to make some observations on my experience, and on some design decisions that I regret in the end. 

### The Measurement function trait

I think Rusts trait system is honestly amazing. All 'classes' are just structs which make it extremely clear what all the data is that a 'class' uses in its lifetime. Abstract interaction between classes is then done by implementing certain traits. 

For example in my code I defined a `MeasurementFunction` trait, and the unscented Kalman filter then gets told which measurement function to use at runtime by taking a `Box<dyn MeasurementFunction>`. This was very important because this way I was able to treat measurement functions defined in Python and those defined in Rust on equal footing. This feature was in fact the hardest thing for me to figure out in this project.

One of the reasons we needed this is because measurement functions need _context_ in my
usecase. In particular, when we run the update function we need to know for which camera
to run this function. There are multiple ways to make this possible, but since I wanted
to avoid restrictions on what this context could be, it became a bit of a headache
dealing with this.

When using a Kalman filter it is normal to have different sensors (such as different cameras), which means having a different measurement function for each sensor. If the number of sensors is finite, then rather than having an arbitrary object as context, it woul make more sense to just keep a list of measurement functions and upon updating pass an index or enum to specify which sensor (camera) produced the data. If I made this design decision, my code would likely have been much simpler.

### PyO3

Traditionally if you wanted to write fast code that can interface with Python your main option is C/C++ together with something like pybind. With the PyO3 crate, rust offers itself as a solid alternative. Defining classes and methods, calling Python code, handling errors, and dealing with the GIL is relatively easy. I think what makes this possible is Rust's extremely good macro system. Still, you do end up with a fair bit of boilerplate. For example, you need to define getters and setters for every single property that you want to expose to Python. For example I have 143 lines of code that looks like this in the unscented Kalman filter class alone:
```rust
#[getter]
#[pyo3(name = "x")]
pub fn py_get_x(&self, py: Python<'_>) -> PyResult<Py<PyArray1<Float>>> {
    let array = self.x.clone().into_pyarray(py).to_owned();
    Ok(array)
}

#[setter]
#[pyo3(name = "x")]
pub fn py_set_x(&mut self, x: PyReadonlyArray1<Float>) -> PyResult<()> {
    self.x = x.as_array().to_owned();
    Ok(())
}
```
I understand why it's necessary, but it is certainly tedious.

Furthermore, Rust has good support for generic types. There were quite a few places where defining a struct or trait with generic types would have made my life easier, but this is not supported by PyO3 (at least for now).

As the ecosystem matures a little, I have no doubt things are going to improve even further. But for now, while it isn't difficult per se, it still can be a bit tedious to write Python modules in rust. 
### ndarray vs numpy

One reason why I really enjoy writing numerical code in Python is because of numpy and its surrounding ecosystem. Rust is faster, but without good libraries for numerical array programming, it would not be so useful for me. The main library in this area is the `ndarray` crate and, as a whole, it is intended to be quite similar to `numpy`. However, with Rust's strict typing and memory management, I did find it quite tricky at times.

For example where in Python we might write `x+=K@y` with ndarray we write `x += &K.dot(&y)`, but this might change depending on whether `K` or `y` are an `Array`, `ArrayView` or `ArrayViewMut`, and sometimes we have to call `.view()`, `.to_owned()` or `.clone()` on the arrays, and sometimes we don't. Certainly at first, but still now sometimes, it just feels like trial and error is necessary to do simple things like add or multiple two arrays. However, many of these things are good; we're making it really explicit when a memory copy occurs, and we are also making sure that each piece of data has only one owner. This prevents bugs, and helps avoid uncessary memory copies. On the other hand, it can also be very frustrating at first when coming from a language like Python where you never have to worry about that (for better or for worse).


### Profiling

> pyspy and flamegraph; can even show a flamegraph I suppose


## Conclusion

> It's a pretty cool tool. This particular project didn't end up in the best state, but I will definitely reach for rust next time I need to make some part of my python code blazignly fast.