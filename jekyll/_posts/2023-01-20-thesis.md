---
layout: posts
title: "My thesis in a nutshell"
date: 2023-01-16
categories: math
excerpt: "I recently defended my PhD thesis, and I would like to share with the world what I was working on."
header:
  teaser: "/imgs/teasers/thesis.webp"
---

<style>
    img[src*="#nice"] {
        margin: 0 1em 1em 0;
        max-width: 450px;
        width: 100%;
        margin-left: auto;
        margin-right: auto;
        display: block;
    }
    img[src*="#nice-medium"] {
        margin: 0 1em 1em 0;
        max-width: 600px;
        width: 100%;
        margin-left: auto;
        margin-right: auto;
        display: block;
    }
    img[src*="#nice-small-inline"] {
        margin: 0 1em 1em 0;
        max-width: 300px;
        width: 100%;
        margin-left: auto;
        margin-right: auto;
        display: inline;
    }
    img[src*="#nice-inline"] {
        margin: 0 1em 1em 0;
        max-width: 360px;
        width: 100%;
        margin-left: auto;
        margin-right: auto;
        display: inline;
    }
    img[src*="#nice-wide"] {
        margin: 0 1em 1em 0;
        width: 100%;
        max-width: 100%;
        margin-left: auto;
        margin-right: auto;
        display: block;
    }
    /* img[src*="#nice"] {
        margin: 0 1em 1em 0;
        max-width: 450px;
        width: 100%;
        padding-left: 50px;
        margin-right: auto;
    }
    img[src*="#nice-medium"] {
        margin: 0 1em 1em 0;
        max-width: 600px;
        width: 100%;
        padding-left: 50px;
        margin-right: auto;
    } */
</style>

A couple months ago I had the pleasure of defending my thesis and finally obtaining the elusive title of 'doctor'.  This blog post is essentially the contents of my thesis defense in written form, and I hope you will enjoy reading this to get a glimpse of the interesting research I was able to do over the past few years. This is subdivided into several parts, roughly becoming more technical as we go along.

If you just want to read my thesis, you [can find it here](https://doi.org/10.13097/archive-ouverte/unige:166308). You can also download my defense slides [right here](https://github.com/RikVoorhaar/RikVoorhaar.github.io/raw/master/_data/presentation.pdf). 

## Low-rank matrices

![A train](/imgs/thesis/chapter1_compressed.webp#nice-medium)

My thesis deals specifically with low-rank tensors, but before we dive into low-rank tensors it makes sense to first talk about low-rank matrices. This is something we also discussed in [this blog post](/low-rank-matrix). A low-rank matrix is nothing more than the product of two smaller matrices. For example below we write the matrix $$A$$ as the product $$A=XY^\top$$.

![Low-rank matrix](/imgs/thesis/def-low-rank.svg#nice)

In this case the matrix $$X$$ is of size $$m\times r$$ and the matrix $$Y$$ is $$n\times r$$. This usually means that the product $$A$$ is a rank-$$r$$ matrix, you can only conclude that the rank of $$A$$ is _at most_ $$r$$. For example, one of the rows of $$X$$ or $$Y$$ may be equal to zero.

Some matrices encountered 'in the wild' are of low rank, but usually, they are not. However, many matrices can be very well approximated by low-rank matrices. A nice example of this is images, which (if we consider each color channel separately) are just matrices.

![Several low-rank approximations of an image](/imgs/thesis/low-rank-approx-img.webp#nice-wide)

Here we see the best approximation for several given ranks, and higher ranks give a higher fidelity approximation of the original image, but even the low-rank approximations are recognizable. We can determine what the 'best' rank-$$r$$ approximation of a matrix $$A$$ is by looking at the following optimization problem:

$$
    \min_{B \text{ rank } \leq r} \|A - B\|
$$

There are many ways to solve such an approximation problem, but fortunately in this case there is a simple closed-form solution to this problem: it is simply given by the _truncated SVD_. Using `numpy` we could solve it as follows:

```py
def low_rank_approx(A, r):
    U, S, Vt = np.linalg.svd(A)
    return U[:, :r] @ np.diag(S[:r]) @ Vt[:r, :]
```

One disadvantage of this particular function is that it hides the fact that the output has rank $$\leq r$$, since we're just returning an $$m\times n$$. However, we can fix this easily as follows:

```py
def low_rank_approx(A, r):
    U, S, Vt = np.linalg.svd(A)
    X = U[:, :r] @ np.diag(S[:r])
    Y = Vt[:r, :].T
    return X, Y
```

This way the low-rank approximation is given by $$XY^\top$$. From a computational point of view, low-rank matrices are great because they allow for very fast products. Suppose that $$A$$ and $$B$$ are two very large $$n\times n$$ matrices, then it costs $$O(n^3)$$ flops to compute the product $$AB$$ using conventional matrix multiplication algorithms. However, if $$A=X_1Y_1^\top$$ and $$B=X_2Y_2^\top$$ then computing $$X_1((Y_1^\top X_2)Y_2^\top)$$ requires only $$O(rn^2)$$ flops. Better yet, this product can be written as the product of two $$n\times r$$ matrices using only $$O(r^2n)$$ flops, which is potentially much less than $$O(n^3)$$.

The same is true for matrix-vector products. If $$v$$ is a size $$n$$ vector, then $$Av$$ costs $$O(n^2)$$ flops to compute, while $$X(Y^\top v)$$ requires only $$O(rn)$$ flops.

### Matrix completion

Observe that the decomposition $$A=XY^\top$$ for a size $$m\times n$$ matrix uses only $$r(m+n)$$ parameters, instead of the $$mn$$ parameters required for the full matrix. (In fact, we only need $$r(m+n) - r^2$$ parameters due to symmetry.) This suggests that given more than $$r(m+n)$$ entries of the matrix, we can infer the remaining entries of the matrix. This is also known as _matrix completion_ and it can for example be done by solving the following optimization problem:

$$
    \min_{B\text{ rank }\leq r}\|\mathcal P_\Omega{A} - \mathcal P_{\Omega}B\|,
$$

where $$\Omega$$ is the set of entries that we observe, and $$\mathcal P_\Omega$$ is the projection that sets all entries in a matrix not in $$\Omega$$ to zero (i.e. we only compare the _known_ entries of $$A$$ to those in the matrix $$B$$).

Below we see matrix completion in action. In both cases, I removed 2.7% of the pixels and tried to reconstruct the image as a rank 100 matrix. What we see is that in the first case, this works very well, but in the second case, it doesn't work well at all. This is because, for matrix completion to work, we need to assume that the unknown pixels are reasonably spread out.

![Matrix completion applied to a chocolate cauldron](/imgs/thesis/cauldron-completion.webp#nice-wide)

There are quite a lot of techniques for solving this problem. _Since this is a little technical, feel free to read on if you are unfamiliar with optimization theory._ Perhaps the simplest is the one I outlined [in my blog post](/low-rank-matrix/), which uses alternating least squares optimzation; if we write $$B=XY^\top$$, then we can alternatingly optimize $$X$$ and $$Y$$. Another really interesting technique is to solve a slighty different optimization problem which turns out to be convex. After that we can unleash all the machinery of convex optimization onto the problem. A method known as _Riemannian gradient descent_ is even better for my purposes, since it generalizes to a very good method for low-rank tensors. The idea is to see the constraint set (in this case the set of low-rank matrices) as a Riemannian manifold. When we then do gradient descent methods, we can first project the gradient onto the tangent space of the manifold. After doing a step into the projected direction, we will stay much closer to the constraint set, and we can often cheaply compute a projection back onto the manifold. In fact, taking a step in the projection gradient direction followed by a projection onto the constraint set is usually combined into one operation known as a _retraction_. Riemannian gradient descent is usually the art of finding a retraction that is both cheap to compute, and will make the optimization objective converge fast.

## Low-rank tensors

![A train](/imgs/thesis/cover_compressed.webp#nice-medium)

Let's now move on to the basics of tensors. A tensor is nothing but a multi-dimensional array. For example, a vector is an _order 1 tensor_, while a matrix is an order 2 tensor. An order 3 tensors can be thought of for example as a collection of matrices, or an array whose entries can be represented by a cube of values as depicted below. Unfortunately, this 'geometric' way of thinking about tensors breaks down a little at higher orders, but in principle still works.

![Tensors of different orders with examples](/imgs/thesis/explanation-tensor-opt.svg#nice-wide)

Some examples of tensors include:

- Order 1 (vector): _Audio signals, stock prices_
- Order 2 (matrix): _Grayscale images, excel spreadsheets_
- Order 3: _Color images, B&W videos, Minecraft maps, MRI scans_
- Order 4: _Color video, fMRI scans_

Recall that a _matrix_ is low-rank if it is the product of two smaller matrices. That is, $$A=XY^\top$$. Unfortunately, this kind of notation doesn't generalize well to tensors. Instead, we can write down each entry of $$A$$ as a sum:

$$
    A[i,j] = \sum_{\ell=1}^rX[i,\ell]Y[j,\ell]
$$

Similarly, we could write an order 3 tensor as a product of 3 matrices as follows:

$$
    A[i, j, k] = \sum_{\ell=1}^r X[i,\ell]Y[j,\ell]Z[k,\ell]
$$

However, if we're dealing with more complicated tensors of higher order, this kind of notation will very quickly become wieldy. One way to get around this is to use the Einstein summation notation, but even that is not necessarily appropriate. Instead, I prefer a diagrammatic way of depicting these kinds of sums.

In this diagrammatic notation, we represent tensors by boxes with one leg (edge) for each of the 'indices' of the tensor. For example, a matrix is a box with two legs, and an order 3 tensor is a box with 3 legs. If we connect two boxes via one of these legs, then this denotes summation over the associated index. For example, matrix multiplication is denoted as follows:

![Diagrammatic notation of matrix multiplication](/imgs/thesis/low-rank-matrix-def.svg#nice)

I also like to label all the legs indicating the _dimension_ of the associated index. This way it's also clearer which of the legs can be contracted together; it is only possible to sum over an index belonging to two different tensors if they have the same dimension.

To denote the low-rank order 3 tensor mentioned above we can use the following diagram

![Diagrammatic notation of contraction of 3 matrices to form a tensor](/imgs/thesis/low-rank-tensor-def.svg#nice)

Here we actually sum over the same index for 3 different matrices, that's why we need to connect 3 legs together in the diagram. Either way, the resulting low-rank tensor is known as a _CP tensor_, where CP is short for canonical polyadic. This tensor 'format' generalizes very easily to higher order. For order $$d$$ we could simply write:

$$
    A[i_1,i_2,\dots,i_d] = \sum_{\ell=1}^r X_1[i_1,\ell]X_2[i_2,\ell]\cdots X_d[i_d,\ell]
$$

The CP tensor format is a very natural generalization of low-rank matrices, and it is also very simple to formulate. Unfortunately, it can be difficult to work with this format. For one thing, given a tensor, it is not clear how to find a good approximation of this tensor as a CP tensor of a given rank; something that we solve for matrices using the truncated SVD. Instead we will therefore use a slightly more complicated tensor format known as the _tensor train format_ (TT).

## Tensor trains

Consider again the formula for a low-rank matrix decomposition $$A=C_1C_2$$

$$
A[i_1, i_2] = \sum_{\ell}^r C_1[i_1,\ell]C_2[\ell,i_2]
$$

Another way we can write this is that $$A[i_1,i_2] = C_1[i_1,:]C_2[:,i_2]$$, i.e. the product of row $$i_1$$ of the first matrix and column $$i_2$$ of the second matrix. We can depict this as follows:

![Diagram depicting the product of two matrices](/imgs/thesis/tt-explanation1.svg#nice)

If we want to generalize this to obtain an order-3 tensor, then we could try to write $$A[i_1,i_2,i_3]$$ as the product of 3 vectors, and this is exactly the CP tensor format. Another option is to say that $$A[i_1,i_2,i_3]$$ is a vector-matrix-vector product. That is,

$$
\begin{align}
    A[i_1,i_2,i_3] &= C_1[i_1,:]C_2[:,i_2,:]C_3[:,i_3]\\
    &= \sum_{\ell_1=1}^{r_1}\sum_{\ell_2=1}^{r_2} C_1[i_1,\ell_1]C_2[\ell_1,i_2,\ell_2]C_3[\ell_2,i_3]
\end{align}
$$

Using a similar picture to the one above, this product can be depicted like this:

![Diagram depicting the product of two matrices and order 3 tensor](/imgs/thesis/tt-explanation2.svg#nice)

From this point the generalization to arbitrary order is simple. For order 4 we write each entry of the tensor as a vector-matrix-matrix-vector product, for order 5 as a vector-matrix-matrix-matrix-vector product, and so on. For example, for an order 4 tensor, we would write

$$
\begin{align}
    A[i_1,i_2,i_3,i_4] &= C_1[i_1,:]C_2[:,i_2,:]C_3[:,i_3,:]C_4[:,i_4]\\
    &= \sum_{\ell_1=1}^{r_1}\sum_{\ell_2=1}^{r_2} \sum_{\ell_3=1}^{r_3} C_1[i_1,\ell_1]C_2[\ell_1,i_2,\ell_2]C_3[\ell_2,i_3,\ell_3] C_4[\ell_3,i_4].
\end{align}
$$

which can be depicted as a vector-matrix-matrix-vector product like this:

![Diagram depicting the product of two matrices and 2 order 3 tensors](/imgs/thesis/tt-explanation3.svg#nice)

Now let's translate this back to our diagrammatic notation. We are trying to write down an order 4 tensor, so it should be a box with 4 legs expressed as the product of a matrix, 2 order 3 tensors, and another matrix. This is what we get:

![Alternative diagram depicting the product of two matrices and 2 order 3 tensors](/imgs/thesis/tt-explanation4.svg#nice)

We can denote a tensor train of arbitrary order like this:

![Diagram depicting an arbitrary tensor train](/imgs/thesis/def-tt.svg#nice-medium)

This may also shed some light on why we call this decomposition a tensor train. Each of the little boxes of order 2/3 tensors is a 'carriage' in the train, so we can translate the diagram above to a train like this:

![Drawing of a train](/imgs/thesis/photo-of-tt-bad.png#nice)

As you can see I'm not much of an artist, but luckily we can use stable diffusion to make a more artistically pleasing depiction:

![Painting of a train](/imgs/thesis/photo-of-tt-nice.png#nice)

### Tensor trains: what are they good for?

From what we have seen so far it is not obvious what makes the tensor train decomposition such a useful tool. Although these properties are not unique to the tensor train decomposition, here are some reasons why it is a good decomposition for many applications. Some of these are a bit technical, so feel free to read on.

**Computing entries is fast:** Computing an arbitrary entry $$A[i_1,\dots,i_d]$$ is very fast, requiring just a few matrix-vector products. These operations can be efficiently done in parallel using a GPU as well.

**Easy to implement:** Most algorithms involving tensor trains are not difficult to implement, which makes them easier to adopt. A similar tensor decomposition known as the hierarchical tucker decomposition is much more tricky to use in practical code, which is likely why it is less popular than tensor trains despite theoretically being a superior format for many purposes.

**Dimensional scaling:** If we keep the ranks $$r = r_1=\dots=r_{d-1}$$ of an order-$$d$$ tensor train fixed, then the amount of data required to store and manipulate a tensor train only scales linearly with the order of the tensor. A dense tensor format would scale exponentially with the tensor order and quickly become unmanageable, so this is an important property. Another way to phrase this is that tensor trains _do not suffer from the curse of dimensionality._

**Orthogonality and rounding:** Tensor trains can be _orthogonalized_ with respect to any mode. They can also be _rounded_, i.e. we can lower all the ranks of the tensor train. These two operations are extremely useful for many algorithms and have a reasonable computational cost of $$O(r^3nd)$$ flops, and are also very simple to implement.

**Nice Riemannian structure:** The tensor trains of a fixed maximum rank form a Riemannian manifold. The tangent space, and orthogonal projections onto this tangent space, are relatively easy to work with and compute. The manifold is also topologically closed, which means that optimization problems on this manifold are well-posed. These properties allow for some very efficient Riemannian optimization algorithms.

## Using tensor trains for machine learning

![A train](/imgs/thesis/chapter2_compressed.webp#nice-medium)

Armed with the knowledge that low-rank tensors can very efficiently represent certain discretized functions, I will show how we can use tensor trains to build an entirely novel kind of machine-learning estimator. Since I made [a more detailed blog post](/discrete-function-tensor) on this work, I will be brief and invite you to read this post if you want to learn more.

### Matrices as discretized functions

Imagine we have a function $$f(x,y)\colon I^2\to \mathbb R$$, and we plot the values of this function on a square. For example, we have a plot of the following function below:

$$
    f(x,y) = 3\cos(10(x^2 + y^2/2)) -\sin(20(2x-y))/2
$$

![A plot of a 2D function](/imgs/thesis/low-rank-function1.webp#nice)

Remember that (grayscale) images are nothing but matrices, so if we use $$m\times n$$ pixels to plot this function, we have essentially created an $$m\times n$$ matrix. Remarkably, this particular matrix is always rank-4, no matter what size of the matrix we choose. Below we plot the rows of respectively the matrices $$X$$ and $$Y$$ of the low-rank decomposition $$A=XY^\top$$. It is interesting to observe that this low-rank decomposition does not visually change when we increase the size of the matrix.

![A plot of a 2D function](/imgs/thesis/low-rank-function2.webp#nice)

What we can take away from this is that low-rank matrices can potentially represent complicated 2D functions. As a natural extension, we can thus expect that low-rank tensors can represent complicated functions in higher dimensions.

We now have a way to parametrize complicated functions using relatively few parameters, which means that we can try to use these functions as a supervised learning model. Suppose that we are thus given a few samples $$y_j = \hat f(x_j)$$ with $$j=1,\dots, N$$ for $$x_j\in\mathbb R^{d}$$, and where $$\hat f$$ is an unknown flnction. Let $$f_A$$ be the discretized function obtained from a tensor or matrix $$A$$. Supervised learning then corresponds to the following least-squares problem:

$$
    \min_{A} \sum_{j=1}^N (f_A(x_j) - y_j)^2
$$

Each sample $$x_j$$ corresponds to an entry $$A[i_1(x_j),\dots,i_d(x_j)]$$, and therefore the least-squares problem can be rephrased as a matrix / tensor-completion problem.

Let's see this in action for the 2D / matrix case to gain some intuition. First, we can generate some random points in a square and sample the function $$f(x,y)$$ defined above.
Below on the left, we see the scatterplot of the random values samples, and next, we see what this looks like as a discretized function/matrix.

![Scatter plot of the function f](/imgs/thesis/low-rank-fun/discrete_func_scatterplot_1.webp#nice-small-inline)![Discretization of the previous plot](/imgs/thesis/low-rank-fun/discrete_func_sparse_1.webp#nice-small-inline)

If we now apply matrix completion to this we get the following. First, we see the completed matrix using a rank-8 matrix, and then the matrices $$X, Y$$ of the decomposition $$A=XY^\top$$.

![Matrix completed discretization](/imgs/thesis/low-rank-fun/discrete_func_contourplot_1.webp#nice-small-inline)![Low-rank decomposition](/imgs/thesis/low-rank-fun/discrete_func_matrix_completion_1.webp#nice-small-inline)

What we have so far is already a useable supervised learning model; we plug in data, and as output, it can make reasonably accurate predictions of data points it hasn't seen so far. However, the data we put into this model consists of many data points uniformly distributed over the domain. Real data is rarely like that, and if we plot the same for images for less uniformly distributed data the result is less impressive:

![Scatter plot of the function f, non-uniform data](/imgs/thesis/low-rank-fun/discrete_func_scatterplot_2.webp#nice-small-inline)![Discretization of the previous plot](/imgs/thesis/low-rank-fun/discrete_func_sparse_2.webp#nice-small-inline)

![Matrix completed discretization](/imgs/thesis/low-rank-fun/discrete_func_contourplot_2.webp#nice-small-inline)![Low-rank decomposition](/imgs/thesis/low-rank-fun/discrete_func_matrix_completion_2.webp#nice-small-inline)

How can we get around this? Well, if the data is not uniform, then why should we use a uniform discretization? For technical reasons, the discretization is still required to be a grid, but we can adjust the spacing of the grid points to better match the data. If we do this, we get something like this:

![Scatter plot of the function f, non-uniform data and grid](/imgs/thesis/low-rank-fun/discrete_func_scatterplot_3.webp#nice-small-inline)![Discretization of the previous plot](/imgs/thesis/low-rank-fun/discrete_func_sparse_3.webp#nice-small-inline)

![Matrix completed discretization](/imgs/thesis/low-rank-fun/discrete_func_contourplot_3.webp#nice-small-inline)![Low-rank decomposition](/imgs/thesis/low-rank-fun/discrete_func_matrix_completion_3.webp#nice-small-inline)

While the final function (3rd plot) may look odd, it does achieve two important things. First, it makes the matrix-completion problem easier because we start with a matrix where a larger percentage of entries are known. And secondly, the resulting function is accurate in the vicinity of the data points. So long as the distribution of the training data is reasonably similar to the test data, this means that the model is accurate on most test data points. The model is potentially not very accurate in some regions, but this may simply not matter in practice.

### TT-ML

Now that we have a good understanding of how low-rank matrices and tensors can be used for supervised learning, let's look in more detail at the high-dimensional case. As mentioned, instead of low-rank matrices, we will use tensor trains to parameterize the discretized functions. This means solving a problem of form

$$
    \min_{ A\in \mathscr M}\sum_{j=1}^N\left(A[i_1(\mathbf x_j),\dots, i_d(\mathbf x_j)]-y_j\right)^2,\tag{$\star$}
$$

where $$\mathscr M$$ denotes the manifold of all tensor trains of a given maximum rank. One of the best methods in practice to solve this optimization problem is to use the Riemannian structure of the manifold of tensor trains. What we end up with is an optimization algorithm similar to gradient descent (with line search), but using the Riemannian gradients instead.

Unfortunately, the problem $$(\star)$$ is very non-linear and the objective has many local minima. As a result, any gradient-based method will only produce good results if it has a good initialization. Ideally, we would use as initialization a tensor describing a discretized function with low training/test loss. Fortunately, there are many such tensors within easy reach: all we have to do is train some other machine learning model (e.g. a random forest or neural network) on the same data, and then discretize the resulting function. What we then get is however not a tensor train, but rather a general dense tensor, and it would be impossible in practice to store all of its entries in memory. However, we can compute any particular entry in this tensor very cheaply; this is equivalent to evaluating the model at a point. Using this, we can use a technique known as TT-cross to efficiently obtain a tensor-train approximation of the discretized machine learning model, which we can then use as initialization for the optimization routine.

In short, we first train a different supervised machine learning model on the same data and then use that for initialization. What is the point of this? Why not just use this initialization model instead of the tensor train? The answer to this question is speed and size. After going through all these steps we end up with a machine learning model that is a fraction of the size and can do much faster inference than the initial model. Remember that accessing an entry of a tensor train is blazingly fast and easy to parallelize. In addition, low-rank tensor trains can still parameterize very complicated functions.

I don't want to dive into too many benchmarks and detail in this post, but the potential advantages of this TT-based model (which I call _TTML_) can be summarized pretty well in the following three graphs (read my thesis for more detail on what exactly is going on in these experiments):

![Benchmarks of error vs number of parameters](/imgs/thesis/complexity_comparison1.svg#nice-inline)
![Benchmarks of speed vs size](/imgs/thesis/complexity_comparison2.svg#nice-inline)
![Benchmarks of error vs size](/imgs/thesis/complexity_comparison3.svg#nice-inline)

We can see that, on the right test problem, the TTML model is both much smaller and much faster than other models given the same test error. Unfortunately, this does depend strongly on the dataset used, and for most practical machine learning problems the test error of TTML would not be very impressive. Nevertheless, if we are in an application where speed takes priority over accuracy, then this can be a very competitive machine learning model.

## Randomized linear algebra

![A train](/imgs/thesis/chapter3-2_compressed.webp#nice-medium)

As we have seen above, the singular value decomposition (SVD) can be used to find the best low-rank approximation of a any matrix. Unfortunately, the SVD is rather expensive to compute, costing $$O(mn^2)$$ flops for an $$m\times n$$ matrix. The SVD can also be used to compute good low-rank TT approximations of any tensor, and it shouldn't be surprising that the cost of the SVD can quickly become prohibitive in this context. We thus need a way to compute low-rank approximations much quicker.

In [my blog post](/low-rank-matrix) I discussed some iterative methods to compute low-rank approximations using only matrix-vector products. However, there are even faster non-iterative methods that are based on multiplying the matrix of interest with a random matrix.

If $$A$$ is a rank-$$\hat r$$ matrix of size $$m\times n$$ and $$x$$ is a random matrix of size $$r>\hat r$$, then it turns out that the product $$AX$$ almost always has the same range as $$A$$. This is because multiplying by $$X$$ like this doesn't change the rank of $$A$$, unless $$X$$ is chosen adversarially. However, because we assume $$X$$ is chosen randomly this almost never happens. And here 'almost never' is meant in the mathematical sense -- i.e. with probability zero. As a result, we have the identity $$\mathcal P_{AX}A =A$$, where $$\mathcal P_{AX}$$ denotes the orthogonal projection onto $$AX$$. This projection matrix can be computed using the QR decomposition of $$AX$$, or it can be seen simply as the matrix whose columns form an orthogonal basis of the range of $$AX$$.

If $$A$$ has rank $$\hat r>r$$ however, then $$\mathcal P_{AX}A\neq A$$. Nevertheless, we might hope that these two matrices are close, i.e. we may hope that (with probability 1) we have

$$
    \|\mathcal P_{AX}A \leq C\|A_{\hat r}-A\|.
$$

For some constant $$C$$ that depends only on $$\hat r$$ and the dimensions of the problem. Recall that here $$A_{\hat r}$$ denotes the best rank-$$\hat r$$ approximation of $$A$$ (that we can compute using the SVD). It turns out that this is true, and it in fact gives a very simple algorithm for computing a low-rank approximation of a matrix using only $$O(mnr)$$ flops -- a huge gain if $$r$$ is much smaller than the size of the matrix. This is known as the Halko-Martinsson-Tropp (HMT) method, and we might implement it in Python for example like this:

```py
def hmt_approximation(A, r):
    m, n = A.shape
    X = np.random.normal(size=(n, r))
    AX = A @ X
    Q, _ = np.linalg.qr(AX)
    return Q, Q.T @ A
```

Since $$Q(Q^\top A) = \mathcal P_{AX}A$$, this gives a low-rank approximation. It can also be used to obtain an approximate truncated SVD with a minor modification: if we take the SVD $$U\Sigma V^\top = Q^\top A$$, then $$(QU)\Sigma V^\top$$ is an approximate truncated SVD of $$A$$. In Python we could implement this like this:

```py
def hmt_truncated_svd(A, r):
    Q, QtA = hmt_approximation(A, r)
    U, S, Vt = np.linalg.svd(QtA)
    return Q @ U, S, Vt
```

The HMT method does have some downsides compared to other randomized methods. For example, suppose we want to compute a low-rank decomposition of $$A+B$$. Can we do this in parallel (completely independently of each other)? The answer is _not really_. We can compute the products $$AX$$ and $$BX$$ in parallel without a problem, but it is not possible to compute the QR factorization of $$(A+B)X$$ without first computing $$(A+B)X$$. A different but related shortcoming is that if we have computed a low-rank approximation $$Q(Q^\top A)$$, and we make a small change to $$A$$ to get $$A'=A+B$$, then we cannot take a shortcut and compute an approximation of $$A'$$ of the same rank without redoing most of the work.

The fundamental issue here is that computing a QR decomposition of the product $$AX$$ is non-linear and expensive. One way around this is to add a second random matrix $$Y$$ of size $$m\times r$$ into the mix and compute a decomposition of $$Y^\top AX$$. This matrix has only size $$r\times r$$, so if $$r$$ is small then it's not a problem to redo this computation. Moreover, computing $$Y^\top(A+B)X$$ can of course be done completely in parallel. What we end up with is a low-rank decomposition of the form:

$$
    A\approx AX(Y^\top AX)^\dagger Y^\top A,
$$

where $$\dagger$$ denotes the _pseudo-inverse_. Which is a generalization of the matrix inverse to matrices that are non-invertible or even rectangular. We can compute it using the SVD of a matrix; if $$A=U\Sigma V^\top$$ then $$A^\dagger = V\Sigma^{-1} U^\top$$, where we compute the inverse of $$\Sigma$$ by simply setting $$\Sigma^{-1}[i,i] = 1/(\Sigma[i,i])$$ (unless one of the diagonal entries are zero; in which case they remain untouched).

This randomized decomposition is known by different names and has appeared in slightly different forms many times in the literature. In my thesis, I refer to it as the _generalized Nyström_ (GN) method. Like the HMT method, it is _quasi-optimal_, that is, it satisfies

$$
    \|AX(Y^\top AX)^\dagger Y^\top A - A\|\leq C\|A_{\hat r}-A\|.
$$

We will now discuss two technical caveats concerning this method. Feel free to skip ahead if this is too technical. The first is that if we need to choose $$X$$ and $$Y$$ to be of different sizes; i.e. we must have $$X$$ of size $$m\times r_R$$ and $$Y$$ of size $$n\times r_L$$ with $$r_L\neq r_R$$. This is because otherwise $$(Y^\top AX)^\dagger$$ can have weird behavior. For example, the expected spectral norm $$\mathbb{E}\|(Y^\top AX)^\dagger\|_2$$ is infinite if $$r_L=r_R$$.

The second caveat is that explicitly computing $$(Y^\top AX)^\dagger$$ and then multiplying it by $$AX$$ and $$Y^\top A$$ is a bad idea. This is because of numerical instability. However, a product of form $$A^\dagger B$$ is actually equivalent to the solution of a linear problem of form $$AX=B$$. As a result, we could implement this method as follows in Python:

```py
def generalized_nystrom(A, rank_left, rank_right):
    m, n = A.shape
    X = np.random.normal(size=(n, rank_right))
    Y = np.random.normal(size=(m, rank_left))
    AX = A @ X
    YtA = Y.T @ A
    YtAX = Y.T @ AX
    L = YtA
    R = np.linalg.solve(YtAX, AX, rcond=None)
    return L, R
```

## Randomized tensor train approximations

![A train](/imgs/thesis/chapter3_compressed.webp#nice-medium)

Next, we will see how to generalize the GN method to a method for tensor trains. Unfortunately this will get a little technical. Recall that for the GN decomposition we had a decomposition of form

$$
    AX(Y^\top AX)Y^\top A
$$

How could we generalize this to a tensor train? If $$\mathcal T$$ is a tensor, then we can always take a matricization $$\mathcal T^{\leq \mu}$$ with respect to the mode $$\mu$$. This is a matrix of size $$(n_1\cdots n_\mu)\times (n_{\mu+1}\cdots n_d)$$. We can then multiply this matrix on the left and right with random matrices $$Y_\mu$$ and $$X_\mu$$ respectively of the size $$(n_1\cdots n_\mu)\times r_L$$ and $$(n_{\mu+1}\cdots n_d)\times r_R$$ to obtain the product

$$
    \Omega_\mu := Y_\mu^\top \mathcal T^{\leq \mu} X_\mu.
$$

We call this product a _'sketch'_, and this particular sketch corresponds to the product $$Y^\top AX$$ in the matrix case. We can visualize how to compute this product using tensor diagrams below:

![Depiction of the sketch Omega_mu](/imgs/thesis/def-omega-mu.svg#nice)

If we think of a matrix as a tensor with only two modes, then the products $$AX$$ and $$Y^\top A$$ correspond to multiplying the tensor by matrices such that 'one mode is left alone'. From this perspective, we can generalize these products to the sketch

$$
    \Psi_\mu := (Y_{\mu-1}\otimes I_{n_\mu})^\top \mathcal T^{\leq \mu} X_\mu,
$$

where $$Y_{\mu-1}$$ is now a matrix of size $$(n_1\dots n_{\mu-1})\times r_L$$. This is shown visually below:

![Depiction of the sketch Psi_mu](/imgs/thesis/def-psi-mu.svg#nice)

By extension, we can define $$Y_0=X_d=1$$, and then the definition of $$\Psi_\mu$$ reduces to $$\Psi_1=AX$$ and $$\Psi_2=Y^\top A$$ in the matrix case. We can therefore rewrite the GN method as

$$
    AX(Y^\top AX)Y^\top A = \Psi_1\Omega_1^\dagger \Psi_2
$$

More generally, we can chain the sketches $$\Omega_\mu$$ and $$\Psi_\mu$$ together to form a tensor network of the following form:

![Depiction of randomized tensor train approximation](/imgs/thesis/approximation-def.svg#nice-wide)

With only minor work, we can turn this tensor network into a tensor train, and it turns out that this defines a very useful approximation method for tensor trains. From this description, it is not necessarily clear why this method gives an approximation to the original tensor, but this becomes more clear if we rewrite this decomposition into a different form. We can then see that this approximation boils down to successively applying a series of projections to the original tensor, but the proof of this fact as well as the error analysis is outside the scope of this blog post.

### Why is this any good?

We call this decomposition the _streaming tensor train approximation_, and it has several nice properties. First of all, as its name suggests, it is a _streaming method_. This means that if we have a tensor that decomposes as $$\mathcal T = \mathcal T_1+\mathcal T_2$$, then we can compute the approximation for $$\mathcal T_1$$ and $$\mathcal T_2$$ completely independently, and only spend a small amount of effort at the end of the procedure to combine the results. This is because all the sketches $$\Omega_\mu$$ and $$\Psi_\mu$$ are linear in the input tensor, and the final step of computing a tensor train from these sketches is very cheap (and in fact even optional).

The decomposition is also _quasi-optimal_. This means that the approximation of this error will (with high probability) lie within a constant factor of the error of the best possible approximation. Unlike the matrix case, however, it is not actually possible in general to compute the best possible approximation itself in a reasonable time.

How expensive is it to compute this decomposition for a given tensor? This strongly depends on the type of tensor $$\mathcal T$$ we are considering. It is easy to derive the cost for the case where $$\mathcal T$$ is a 'dense' tensor, i.e. just a multidimensional array. However, it rarely makes sense to apply a method like that to such a tensor; usually, we apply it to tensors that are far too big to even store in memory. Instead, we assume that the tensor already has some structure. For example, $$\mathcal T$$ could be a CP tensor, a sparse tensor or even a tensor train itself. For each type of tensor, we can then derive a fast way to compute this decomposition, especially if we allow the matrices $$X_\mu$$ and $$Y_\mu$$ to be structured tensors. The exact implementation of this is however a little technical to discuss here, but safe to say the method is quite fast in practice.

In addition to this decomposition, we also derived a second kind of decomposition (called _OTTS_) that is somewhat of a hybrid generalization of the GN and HMT methods. It is no longer a streaming method, but it is in certain cases significantly more accurate than the previous method, and it can be applied in almost all of the same cases. Lastly, there is also a generalization (called _TT-HMT_) of the HMT method to tensor trains that already existed in the literature for a few years that also works in most of the same situations but is also not a streaming method.

Below we compare these three methods, STTA, OTTS and TT-HMT, to a generalization of the truncated SVD (TT-SVD). This last method is in general very expensive to compute, but has a very good approximation error, and thus serves as a good benchmark. A plot of approximation error of several TT approximation methods](/imgs/thesis/plot-cp.webp#nice-wide)

In the plot above we have taken a 10x10x10x10x10 CP tensor and computed TT approximations of different ranks. What we see is that all methods have similar behavior, and are ordered from best to worst approximation as TT-SVD > OTTS > TT-HMT > STTA. This order is something we observe in general across many different experiments, and also in terms of theoretical approximation error. Furthermore, even though these last three methods are all randomized, the variation in approximation error is relatively small, especially for larger ranks. Next, we consider another experiment below:

![Another plot of approximation error of several TT approximation methods](/imgs/thesis/plot-dimension-scaling.webp#nice-wide)

Here we compare the scaling of the error of the different approximation methods as the order of the tensor increases exponentionally. The tensor that we're approximating, in this case, is always a tensor train with a fast decay in its singular values, and the approximation error is always relative to the TT-SVD method. This is because for such a tensor it is possible to compute the TT-SVD approximation in reasonable time. What we see here is that the scaling of OTTS is very close to that of TT-SVD, while the other methods have a bigger error, but are still asymptotically within a constant of the optimal error. We can thus conclude that all these methods have their merits; we can use STTA if we want streaming, OTTS if we want a good approximation (especially for very big tensors), and TT-HMT if we just want a good approximation and care more about speed than quality.

## Conclusion

![A train](/imgs/thesis/chapter-final_compressed.webp#nice-medium)

This about sums up, on a high level, what I did in my thesis. There are plenty of things I could still talk about and plenty of details left out, but this blog post is probably too long and technical already. If you're interested in any of this, you're of course very welcome to read my thesis or send me a message.

My PhD was a long and fun ride, and I'm now looking back on it with nothing but fondness. I really enjoyed my time in Geneva, and I'm really going to miss some aspects of the PhD life. I have now started a 'real' job, and the things I'm working has very little overlap with the contents of my thesis. I really hope I will be able to discuss some of the cools things I'm working on now, as well as some new pet projects.
