---
layout: posts 
title:  "My thesis in a nutshell" 
date:   2023-01-16
categories: math
excerpt: "I recently defended my PhD thesis, and I would like to share with the world what I was working on."
header: 
    teaser: "/imgs/teasers/thesis.webp"
---

<style>
    img[src*="#nice"] {
        margin: 0 1em 1em 0;
        width: 450px;
        margin-left: auto;
        margin-right: auto;
    }
    img[src*="#nice-indent"] {
        margin: 0 1em 1em 0;
        width: 450px;
        padding-left: 50px;
        margin-right: auto;
    }
    img[src*="#nice-wide"] {
        margin: 0 1em 1em 0;
        width: 100%;
        margin-left: auto;
        margin-right: auto;
    }
</style>



A few weeks ago I had the pleasure of defending my thesis and finally obtaining the elusive title of doctor.
This blog post is essentially the contents of my thesis defense in written form, and it should for the most be
very accessible. This is subdivided into several parts, roughly becoming more technical as we go along.

## Low-rank matrices
![A train](/imgs/thesis/chapter1_compressed.webp#nice)

My thesis deals specifically with low-rank tensors, but before we dive into low-rank tensors it makes sense to first talk about low-rank matrices. This is something we also discussed in [this blog post](/low-rank-matrix). A low-rank matrix is nothing more than the product of two smaller matrices. For example below we write the matrix $$A$$ as the product $$A=XY^\top$$. 

![Low-rank matrix](/imgs/thesis/def-low-rank.svg#nice-indent)

In this case the matrix $$X$$ is of size $$m\times r$$ and the matrix $$Y$$ is $$n\times r$$. This usually means that the product $$A$$ is a rank-$$r$$ matrix, you can only conclude that the rank of $$A$$ is _at most_ $$r$$. For example, one of the rows of $$X$$ or $$Y$$ may be equal to zero.

Some matrices encountered 'in the wild' are of low rank, but usually, they are not. However, many matrices can be very well approximated by low-rank matrices. A nice example of this is images, which (if we consider each color channel separately) are just matrices. 

![Several low-rank approximations of an image](/imgs/thesis/low-rank-approx-img.webp#nice-wide)

Here we see the best approximation for several given ranks, and higher ranks give a higher fidelity approximation of the original image, but even the low-rank approximations are recognizable. We can determine what the 'best' rank-$$r$$ approximation of a matrix $$A$$ is by looking at the following optimization problem:

$$
    \min_{B \text{ rank } \leq r} \|A - B\|
$$

There are many ways to solve such an approximation problem, but fortunately in this case there is a simple closed-form solution to this problem: it is simply given by the _truncated SVD_. Using `numpy` we could solve it as follows:
```py
def low_rank_approx(A, r):
    U, S, Vt = np.linalg.svd(A)
    return U[:, :r] @ np.diag(S[:r]) @ Vt[:r, :]
```

One disadvantage of this particular function is that it hides the fact that the output has rank $$\leq r$$, since we're just returning an $$m\times n$$. However, we can fix this easily as follows:

```py
def low_rank_approx(A, r):
    U, S, Vt = np.linalg.svd(A)
    X = U[:, :r] @ np.diag(S[:r])
    Y = Vt[:r, :].T
    return X, Y
```

This way the low-rank approximation is given by $$XY^\top$$. From a computational point of view, low-rank matrices are great because they allow for very fast products. Suppose that $$A$$ and $$B$$ are two very large $$n\times n$$ matrices, then it costs $$O(n^3)$$ flops to compute the product $$AB$$ using conventional matrix multiplication algorithms. However, if $$A=X_1Y_1^\top$$ and $$B=X_2Y_2^\top$$ then computing $$X_1((Y_1^\top X_2)Y_2^\top)$$ requires only $$O(rn^2)$$ flops. Better yet, this product can be written as the product of two $$n\times r$$ matrices using only $$O(r^2n)$$ flops, which is potentially much less than $$O(n^3)$$.

The same is true for matrix-vector products. If $$v$$ is a size $$n$$ vector, then $$Av$$ costs $$O(n^2)$$ flops to compute, while $$X(Y^\top v)$$ requires only $$O(rn)$$ flops. 

### Matrix completion

Observe that the decomposition $$A=XY^\top$$ for a size $$m\times n$$ matrix uses only $$r(m+n)$$ parameters, instead of the $$mn$$ parameters required for the full matrix. (In fact, we only need $$r(m+n) - r^2$$ parameters due to symmetry.)  This suggests that given more than $$r(m+n)$$ entries of the matrix, we can infer the remaining entries of the matrix. This is also known as _matrix completion_ and it can for example be done by solving the following optimization problem:

$$
    \min_{B\text{ rank }\leq r}\|\mathcal P_\Omega{A} - \mathcal P_{\Omega}B\|,
$$
 
where $$\Omega$$ is the set of entries that we observe, and $$\mathcal P_\Omega$$ is the projection that sets all entries in a matrix not in $$\Omega$$ to zero (i.e. we only compare the _known_ entries of $$A$$ to those in the matrix $$B$$).

Below we see matrix completion in action. In both cases, I removed 2.7% of the pixels and tried to reconstruct the image as a rank 100 matrix. What we see is that in the first case, this works very well, but in the second case, it doesn't work well at all. This is because, for matrix completion to work, we need to assume that the unknown pixels are reasonably spread out. 

![Matrix completion applied to a chocolate cauldron](/imgs/thesis/cauldron-completion.webp#nice-wide)

There are quite a lot of techniques for solving this problem. _Since this is a little technical, feel free to read on if you are unfamiliar with optimization theory._ Perhaps the simplest is the one I outlined [in my blog post](/low-rank-matrix/), which uses alternating least squares optimzation; if we write $$B=XY^\top$$, then we can alternatingly optimize $$X$$ and $$Y$$. Another really interesting technique is to solve a slighty different optimization problem which turns out to be convex. After that we can unleash all the machinery of convex optimization onto the problem. A method known as _Riemannian gradient descent_ is even better for my purposes, since it generalizes to a very good method for low-rank tensors. The idea is to see the constraint set (in this case the set of low-rank matrices) as a Riemannian manifold. When we then do gradient descent methods, we can first project the gradient onto the tangent space of the manifold. After doing a step into the projected direction, we will stay much closer to the constraint set, and we can often cheaply compute a projection back onto the manifold. In fact, taking a step in the projection gradient direction followed by a projection onto the constraint set is usually combined into one operation known as a _retraction_. Riemannian gradient descent is usually the art of finding a retraction that is both cheap to compute, and will make the optimization objective converge fast. 

## Low-rank tensors

![A train](/imgs/thesis/cover_compressed.webp#nice)

Let's now move on to the basics of tensors. A tensor is nothing but a multi-dimensional array. For example, a vector is an _order 1 tensor_, while a matrix is an order 2 tensor. An order 3 tensors can be thought of for example as a collection of matrices, or an array whose entries can be represented by a cube of values as depicted below. Unfortunately, this 'geometric' way of thinking about tensors breaks down a little at higher orders, but in principle still works. 

![Tensors of different orders with examples](/imgs/thesis/explanation-tensor-opt.svg#nice-wide)

Some examples of tensors include:
- Order 1 (vector): _Audio signals, stock prices_
- Order 2 (matrix): _Grayscale images, excel spreadsheets_
- Order 3: _Color images, B&W videos, Minecraft maps, MRI scans_
- Order 4: _Color video, fMRI scans_

Recall that a _matrix_ is low-rank if it is the product of two smaller matrices. That is, $$A=XY^\top$$. Unfortunately, this kind of notation doesn't generalize well to tensors. Instead, we can write down each entry of $$A$$ as a sum:

$$
    A[i,j] = \sum_{\ell=1}^rX[i,\ell]Y[j,\ell]
$$

Similarly, we could write an order 3 tensor as a product of 3 matrices as follows:

$$
    A[i, j, k] = \sum_{\ell=1}^r X[i,\ell]Y[j,\ell]Z[k,\ell]
$$
However, if we're dealing with more complicated tensors of higher order, this kind of notation will very quickly become wieldy. One way to get around this is to use the Einstein summation notation, but even that is not necessarily appropriate. Instead, I prefer a diagrammatic way of depicting these kinds of sums. 

In this diagrammatic notation, we represent tensors by boxes with one leg (edge) for each of the 'indices' of the tensor. For example, a matrix is a box with two legs, and an order 3 tensor is a box with 3 legs. If we connect two boxes via one of these legs, then this denotes summation over the associated index. For example, matrix multiplication is denoted as follows:

![Diagrammatic notation of matrix mulitplication](/imgs/thesis/low-rank-matrix-def.svg#nice-indent)

I also like to label all the legs indicating the _dimension_ of the associated index. This way it's also clearer which of the legs can be contracted together; it is only possible to sum over an index belonging to two different tensors if they have the same dimension. 

To denote the low-rank order 3 tensor mentioned above we can use the following diagram

![Diagrammatic notation of contraction of 3 matrices to form a tensor](/imgs/thesis/low-rank-tensor-def.svg#nice-indent)

Here we actually sum over the same index for 3 different matrices, that's why we need to connect 3 legs together in the diagram. Either way, the resulting low-rank tensor is known as a _CP tensor_, where CP is short for canonical polyadic. This tensor 'format' generalizes very easily to higher order. For order $$d$$ we could simply write:

$$
    A[i_1,i_2,\dots,i_d] = \sum_{\ell=1}^r X_1[i_1,\ell]X_2[i_2,\ell]\cdots X_d[i_d,\ell]
$$

The CP tensor format is a very natural generalization of low-rank matrices, and it is also very simple to formulate. Unfortunately, it can be difficult to work with this format. For one thing, given a tensor, it is not clear how to find a good approximation of this tensor as a CP tensor of a given rank; something that we solve for matrices using the truncated SVD. Instead we will therefore use a slightly more complicated tensor format known as the _tensor train format_ (TT). 

## Tensor trains