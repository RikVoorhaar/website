---
layout: posts 
title:  "My thesis in a nutshell" 
date:   2023-01-16
categories: math
excerpt: "I recently defended my PhD thesis, and I would like to share with the world what I was working on."
header: 
    teaser: "/imgs/teasers/thesis.webp"
---

<style>
    img[src*="#nice"] {
        margin: 0 1em 1em 0;
        width: 450px;
        margin-left: auto;
        margin-right: auto;
    }
    img[src*="#nice-indent"] {
        margin: 0 1em 1em 0;
        width: 450px;
        margin-left: 50px;
        margin-right: auto;
    }
    img[src*="#nice-wide"] {
        margin: 0 1em 1em 0;
        width: 100%;
        margin-left: auto;
        margin-right: auto;
    }
</style>


![A train](/imgs/thesis/cover_compressed.webp#nice)

A few weeks ago I had the pleasure of defending my thesis and finally obtaining the elusive title of doctor.
This blog post is essentially the contents of my thesis defense in written form, and it should for the most be
very accessible. This is subdivided into several parts, roughly becoming more technical as we go along.

## Low-rank matrices
![A train](/imgs/thesis/chapter1_compressed.webp#nice)

My thesis deals specifically with low-rank tensors, but before we dive into low-rank tensors it makes sense to first talk about low-rank matrices. This is something we also discussed in [this blog post](/low-rank-matrix). A low-rank matrix is nothing more than the product of two smaller matrices. For example below we write the matrix $$A$$ as the product $$A=XY^\top$$. 

![Low-rank matrix](/imgs/thesis/def-low-rank.svg#nice-indent)

In this case the matrix $$X$$ is of size $$m\times r$$ and the matrix $$Y$$ is $$n\times r$$. This usually means that the product $$A$$ is a rank-$$r$$ matrix, you can only conclude that the rank of $$A$$ is _at most_ $$r$$. For example, one of the rows of $$X$$ or $$Y$$ may be equal to zero.

Some matrices encountered 'in the wild' are of low rank, but usually, they are not. However, many matrices can be very well approximated by low-rank matrices. A nice example of this is images, which (if we consider each color channel separately) are just matrices. 

![Several low-rank approximations of an image](/imgs/thesis/low-rank-approx-img.webp#nice-wide)

Here we see the best approximation for several given ranks, and higher ranks give a higher fidelity approximation of the original image, but even the low-rank approximations are recognizable. We can determine what the 'best' rank-$$r$$ approximation of a matrix $$A$$ is by looking at the following optimization problem:

$$
    \min_{B \text{ rank } \leq r} \|A - B\|
$$

There are many ways to solve such an approximation problem, but fortunately in this case there is a simple closed-form solution to this problem: it is simply given by the _truncated SVD_. Using `numpy` we could solve it as follows:
```py
def low_rank_approx(A, r):
    U, S, Vt = np.linalg.svd(A)
    return U[:, :r] @ np.diag(S[:r]) @ Vt[:r, :]
```

One disadvantage of this particular function is that it hides the fact that the output has rank $$\leq r$$, since we're just returning an $$m\times n$$. However, we can fix this easily as follows:

```py
def low_rank_approx(A, r):
    U, S, Vt = np.linalg.svd(A)
    X = U[:, :r] @ np.diag(S[:r])
    Y = Vt[:r, :].T
    return X, Y
```

This way the low-rank approximation is given by $$XY^\top$$. From a computational point of view, low-rank matrices are great because they allow for very fast products. Suppose that $$A$$ and $$B$$ are two very large $$n\times n$$ matrices, then it costs $$O(n^3)$$ flops to compute the product $$AB$$ using conventional matrix multiplication algorithms. However, if $$A=X_1Y_1^\top$$ and $$B=X_2Y_2^\top$$ then computing $$X_1((Y_1^\top X_2)Y_2^\top)$$ requires only $$O(rn^2)$$ flops. Better yet, this product can be written as the product of two $$n\times r$$ matrices using only $$O(r^2n)$$ flops, which is potentially much less than $$O(n^3)$$.

The same is true for matrix-vector products. If $$v$$ is a size $$n$$ vector, then $$Av$$ costs $$O(n^2)$$ flops to compute, while $$X(Y^\top v)$$ requires only $$O(rn)$$ flops. 

### Matrix completion

Observe that the decomposition $$A=XY^\top$$ for a size $$m\times n$$ matrix uses only $$r(m+n)$$ parameters, instead of the $$mn$$ parameters required for the full matrix. (In fact, we only need $$r(m+n) - r^2$$ parameters due to symmetry.)  This suggests that given more than $$r(m+n)$$ entries of the matrix, we can infer the remaining entries of the matrix. This is also known as _matrix completion_ and it can for example be done by solving the following optimization problem:

$$
    \min_{B\text{ rank }\leq r}\|\mathcal P_\Omega{A} - \mathcal P_{\Omega}B\|,
$$
 
where $$\Omega$$ is the set of entries that we observe, and $$\mathcal P_\Omega$$ is the projection that sets all entries in a matrix not in $$\Omega$$ to zero (i.e. we only compare the _known_ entries of $$A$$ to those in the matrix $$B$$).

Below we see matrix completion in action. In both cases, I removed 2.7% of the pixels and tried to reconstruct the image as a rank 100 matrix. What we see is that in the first case, this works very well, but in the second case, it doesn't work well at all. This is because, for matrix completion to work, we need to assume that the unknown pixels are reasonably spread out. 

![Matrix completion applied to a chocolate cauldron](/imgs/thesis/cauldron-completion.webp#nice-wide)

There are quite a lot of techniques for solving this problem. _Since this is a little technical, feel free to read on if you are unfamiliar with optimization theory._ Perhaps the simplest is the one I outlined [in my blog post](/low-rank-matrix/), which uses alternating least squares optimzation; if we write $$B=XY^\top$$, then we can alternatingly optimize $$X$$ and $$Y$$. Another really interesting technique is to solve a slighty different optimization problem which turns out to be convex. After that we can unleash all the machinery of convex optimization onto the problem. A method known as _Riemannian gradient descent_ is even better for my purposes, since it generalizes to a very good method for low-rank tensors. The idea is to see the constraint set (in this case the set of low-rank matrices) as a Riemannian manifold. When we then do gradient descent methods, we can first project the gradient onto the tangent space of the manifold. After doing a step into the projected direction, we will stay much closer to the constraint set, and we can often cheaply compute a projection back onto the manifold. In fact, taking a step in the projection gradient direction followed by a projection onto the constraint set is usually combined into one operation known as a _retraction_. Riemannian gradient descent is usually the art of finding a retraction that is both cheap to compute, and will make the optimization objective converge fast. 