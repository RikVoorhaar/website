---
layout: posts 
title:  "My thesis in a nutshell" 
date:   2023-01-16
categories: math
excerpt: "I recently defended my PhD thesis, and I would like to share with the world what I was working on."
header: 
    teaser: "/imgs/teasers/thesis.webp"
---

<style>
    img[src*="#nice"] {
        margin: 0 1em 1em 0;
        max-width: 450px;
        width: 100%;
        margin-left: auto;
        margin-right: auto;
    }
    img[src*="#nice-medium"] {
        margin: 0 1em 1em 0;
        max-width: 600px;
        width: 100%;
        margin-left: auto;
        margin-right: auto;
    }
    img[src*="#nice-indent"] {
        margin: 0 1em 1em 0;
        max-width: 450px;
        width: 100%;
        padding-left: 50px;
        margin-right: auto;
    }
    img[src*="#nice-indent-medium"] {
        margin: 0 1em 1em 0;
        max-width: 600px;
        width: 100%;
        padding-left: 50px;
        margin-right: auto;
    }
    img[src*="#nice-wide"] {
        margin: 0 1em 1em 0;
        width: 100%;
        max-width: 100%;
        margin-left: auto;
        margin-right: auto;
    }
</style>



A few weeks ago I had the pleasure of defending my thesis and finally obtaining the elusive title of doctor.
This blog post is essentially the contents of my thesis defense in written form, and it should for the most be
very accessible. This is subdivided into several parts, roughly becoming more technical as we go along.

## Low-rank matrices
![A train](/imgs/thesis/chapter1_compressed.webp#nice-medium)

My thesis deals specifically with low-rank tensors, but before we dive into low-rank tensors it makes sense to first talk about low-rank matrices. This is something we also discussed in [this blog post](/low-rank-matrix). A low-rank matrix is nothing more than the product of two smaller matrices. For example below we write the matrix $$A$$ as the product $$A=XY^\top$$. 

![Low-rank matrix](/imgs/thesis/def-low-rank.svg#nice-indent)

In this case the matrix $$X$$ is of size $$m\times r$$ and the matrix $$Y$$ is $$n\times r$$. This usually means that the product $$A$$ is a rank-$$r$$ matrix, you can only conclude that the rank of $$A$$ is _at most_ $$r$$. For example, one of the rows of $$X$$ or $$Y$$ may be equal to zero.

Some matrices encountered 'in the wild' are of low rank, but usually, they are not. However, many matrices can be very well approximated by low-rank matrices. A nice example of this is images, which (if we consider each color channel separately) are just matrices. 

![Several low-rank approximations of an image](/imgs/thesis/low-rank-approx-img.webp#nice-wide)

Here we see the best approximation for several given ranks, and higher ranks give a higher fidelity approximation of the original image, but even the low-rank approximations are recognizable. We can determine what the 'best' rank-$$r$$ approximation of a matrix $$A$$ is by looking at the following optimization problem:

$$
    \min_{B \text{ rank } \leq r} \|A - B\|
$$

There are many ways to solve such an approximation problem, but fortunately in this case there is a simple closed-form solution to this problem: it is simply given by the _truncated SVD_. Using `numpy` we could solve it as follows:
```py
def low_rank_approx(A, r):
    U, S, Vt = np.linalg.svd(A)
    return U[:, :r] @ np.diag(S[:r]) @ Vt[:r, :]
```

One disadvantage of this particular function is that it hides the fact that the output has rank $$\leq r$$, since we're just returning an $$m\times n$$. However, we can fix this easily as follows:

```py
def low_rank_approx(A, r):
    U, S, Vt = np.linalg.svd(A)
    X = U[:, :r] @ np.diag(S[:r])
    Y = Vt[:r, :].T
    return X, Y
```

This way the low-rank approximation is given by $$XY^\top$$. From a computational point of view, low-rank matrices are great because they allow for very fast products. Suppose that $$A$$ and $$B$$ are two very large $$n\times n$$ matrices, then it costs $$O(n^3)$$ flops to compute the product $$AB$$ using conventional matrix multiplication algorithms. However, if $$A=X_1Y_1^\top$$ and $$B=X_2Y_2^\top$$ then computing $$X_1((Y_1^\top X_2)Y_2^\top)$$ requires only $$O(rn^2)$$ flops. Better yet, this product can be written as the product of two $$n\times r$$ matrices using only $$O(r^2n)$$ flops, which is potentially much less than $$O(n^3)$$.

The same is true for matrix-vector products. If $$v$$ is a size $$n$$ vector, then $$Av$$ costs $$O(n^2)$$ flops to compute, while $$X(Y^\top v)$$ requires only $$O(rn)$$ flops. 

### Matrix completion

Observe that the decomposition $$A=XY^\top$$ for a size $$m\times n$$ matrix uses only $$r(m+n)$$ parameters, instead of the $$mn$$ parameters required for the full matrix. (In fact, we only need $$r(m+n) - r^2$$ parameters due to symmetry.)  This suggests that given more than $$r(m+n)$$ entries of the matrix, we can infer the remaining entries of the matrix. This is also known as _matrix completion_ and it can for example be done by solving the following optimization problem:

$$
    \min_{B\text{ rank }\leq r}\|\mathcal P_\Omega{A} - \mathcal P_{\Omega}B\|,
$$
 
where $$\Omega$$ is the set of entries that we observe, and $$\mathcal P_\Omega$$ is the projection that sets all entries in a matrix not in $$\Omega$$ to zero (i.e. we only compare the _known_ entries of $$A$$ to those in the matrix $$B$$).

Below we see matrix completion in action. In both cases, I removed 2.7% of the pixels and tried to reconstruct the image as a rank 100 matrix. What we see is that in the first case, this works very well, but in the second case, it doesn't work well at all. This is because, for matrix completion to work, we need to assume that the unknown pixels are reasonably spread out. 

![Matrix completion applied to a chocolate cauldron](/imgs/thesis/cauldron-completion.webp#nice-wide)

There are quite a lot of techniques for solving this problem. _Since this is a little technical, feel free to read on if you are unfamiliar with optimization theory._ Perhaps the simplest is the one I outlined [in my blog post](/low-rank-matrix/), which uses alternating least squares optimzation; if we write $$B=XY^\top$$, then we can alternatingly optimize $$X$$ and $$Y$$. Another really interesting technique is to solve a slighty different optimization problem which turns out to be convex. After that we can unleash all the machinery of convex optimization onto the problem. A method known as _Riemannian gradient descent_ is even better for my purposes, since it generalizes to a very good method for low-rank tensors. The idea is to see the constraint set (in this case the set of low-rank matrices) as a Riemannian manifold. When we then do gradient descent methods, we can first project the gradient onto the tangent space of the manifold. After doing a step into the projected direction, we will stay much closer to the constraint set, and we can often cheaply compute a projection back onto the manifold. In fact, taking a step in the projection gradient direction followed by a projection onto the constraint set is usually combined into one operation known as a _retraction_. Riemannian gradient descent is usually the art of finding a retraction that is both cheap to compute, and will make the optimization objective converge fast. 

## Low-rank tensors

![A train](/imgs/thesis/cover_compressed.webp#nice-medium)

Let's now move on to the basics of tensors. A tensor is nothing but a multi-dimensional array. For example, a vector is an _order 1 tensor_, while a matrix is an order 2 tensor. An order 3 tensors can be thought of for example as a collection of matrices, or an array whose entries can be represented by a cube of values as depicted below. Unfortunately, this 'geometric' way of thinking about tensors breaks down a little at higher orders, but in principle still works. 

![Tensors of different orders with examples](/imgs/thesis/explanation-tensor-opt.svg#nice-wide)

Some examples of tensors include:
- Order 1 (vector): _Audio signals, stock prices_
- Order 2 (matrix): _Grayscale images, excel spreadsheets_
- Order 3: _Color images, B&W videos, Minecraft maps, MRI scans_
- Order 4: _Color video, fMRI scans_

Recall that a _matrix_ is low-rank if it is the product of two smaller matrices. That is, $$A=XY^\top$$. Unfortunately, this kind of notation doesn't generalize well to tensors. Instead, we can write down each entry of $$A$$ as a sum:

$$
    A[i,j] = \sum_{\ell=1}^rX[i,\ell]Y[j,\ell]
$$

Similarly, we could write an order 3 tensor as a product of 3 matrices as follows:

$$
    A[i, j, k] = \sum_{\ell=1}^r X[i,\ell]Y[j,\ell]Z[k,\ell]
$$
However, if we're dealing with more complicated tensors of higher order, this kind of notation will very quickly become wieldy. One way to get around this is to use the Einstein summation notation, but even that is not necessarily appropriate. Instead, I prefer a diagrammatic way of depicting these kinds of sums. 

In this diagrammatic notation, we represent tensors by boxes with one leg (edge) for each of the 'indices' of the tensor. For example, a matrix is a box with two legs, and an order 3 tensor is a box with 3 legs. If we connect two boxes via one of these legs, then this denotes summation over the associated index. For example, matrix multiplication is denoted as follows:

![Diagrammatic notation of matrix mulitplication](/imgs/thesis/low-rank-matrix-def.svg#nice-indent)

I also like to label all the legs indicating the _dimension_ of the associated index. This way it's also clearer which of the legs can be contracted together; it is only possible to sum over an index belonging to two different tensors if they have the same dimension. 

To denote the low-rank order 3 tensor mentioned above we can use the following diagram

![Diagrammatic notation of contraction of 3 matrices to form a tensor](/imgs/thesis/low-rank-tensor-def.svg#nice-indent)

Here we actually sum over the same index for 3 different matrices, that's why we need to connect 3 legs together in the diagram. Either way, the resulting low-rank tensor is known as a _CP tensor_, where CP is short for canonical polyadic. This tensor 'format' generalizes very easily to higher order. For order $$d$$ we could simply write:

$$
    A[i_1,i_2,\dots,i_d] = \sum_{\ell=1}^r X_1[i_1,\ell]X_2[i_2,\ell]\cdots X_d[i_d,\ell]
$$

The CP tensor format is a very natural generalization of low-rank matrices, and it is also very simple to formulate. Unfortunately, it can be difficult to work with this format. For one thing, given a tensor, it is not clear how to find a good approximation of this tensor as a CP tensor of a given rank; something that we solve for matrices using the truncated SVD. Instead we will therefore use a slightly more complicated tensor format known as the _tensor train format_ (TT). 

## Tensor trains

Consider again the formula for a low-rank matrix decomposition $$A=C_1C_2$$

$$
A[i_1, i_2] = \sum_{\ell}^r C_1[i_1,\ell]C_2[\ell,i_2]
$$

Another way we can write this is that $$A[i_1,i_2] = C_1[i_1,:]C_2[:,i_2]$$, i.e. the product of row $$i_1$$ of the first matrix and column $$i_2$$ of the second matrix. We can depict this as follows:

![Diagram depicting the product of two matrices](/imgs/thesis/tt-explanation1.svg#nice-indent)

If we want to generalize this to obtain an order-3 tensor, then we could try to write $$A[i_1,i_2,i_3]$$ as the product of 3 vectors, and this is exactly the CP tensor format. Another option is to say that $$A[i_1,i_2,i_3]$$ is a vector-matrix-vector product. That is,

$$
\begin{align}
    A[i_1,i_2,i_3] &= C_1[i_1,:]C_2[:,i_2,:]C_3[:,i_3]\\
    &= \sum_{\ell_1=1}^{r_1}\sum_{\ell_2=1}^{r_2} C_1[i_1,\ell_1]C_2[\ell_1,i_2,\ell_2]C_3[\ell_2,i_3]
\end{align}
$$

Using a similar picture to the one above, this product can be depicted like this:

![Diagram depicting the product of two matrices and order 3 tensor](/imgs/thesis/tt-explanation2.svg#nice-indent)

From this point the generalization to arbitrary order is simple. For order 4 we write each entry of the tensor as a vector-matrix-matrix-vector product, for order 5 as a vector-matrix-matrix-matrix-vector product, and so on. For example, for an order 4 tensor, we would write 

$$
\begin{align}
    A[i_1,i_2,i_3,i_4] &= C_1[i_1,:]C_2[:,i_2,:]C_3[:,i_3,:]C_4[:,i_4]\\
    &= \sum_{\ell_1=1}^{r_1}\sum_{\ell_2=1}^{r_2} \sum_{\ell_3=1}^{r_3} C_1[i_1,\ell_1]C_2[\ell_1,i_2,\ell_2]C_3[\ell_2,i_3,\ell_3] C_4[\ell_3,i_4].
\end{align}
$$

which can be depicted as a vector-matrix-matrix-vector product like this:

![Diagram depicting the product of two matrices and 2 order 3 tensors](/imgs/thesis/tt-explanation3.svg#nice-indent)

Now let's translate this back to our diagrammatic notation. We are trying to write down an order 4 tensor, so it should be a box with 4 legs expressed as the product of a matrix, 2 order 3 tensors, and another matrix. This is what we get:

![Alternative diagram depicting the product of two matrices and 2 order 3 tensors](/imgs/thesis/tt-explanation4.svg#nice-indent)

We can denote a tensor train of arbitrary order like this:

![Diagram depicting an arbitrary tensor train](/imgs/thesis/def-tt.svg#nice-indent-medium)

This may also shed some light on why we call this decomposition a tensor train. Each of the little boxes of order 2/3 tensors is a 'carriage' in the train, so we can translate the diagram above to a train like this:

![Drawing of a train](/imgs/thesis/photo-of-tt-bad.png#nice)

As you can see I'm not much of an artist, but luckily we can use stable diffusion to make a more artistically pleasing depiction:

![Painting of a train](/imgs/thesis/photo-of-tt-nice.png#nice)

### Tensor trains: what are they good for?

From what we have seen so far it is not obvious what makes the tensor train decomposition such a useful tool. Although these properties are not unique to the tensor train decomposition, here are some reasons why it is a good decomposition for many applications. Some of these are a bit technical, so feel free to read on.

__Computing entries is fast:__ Computing an arbitrary entry $$A[i_1,\dots,i_d]$$ is very fast, requiring just a few matrix-vector products. These operations can be efficiently done in parallel using a GPU as well. 

__Easy to implement:__ Most algorithms involving tensor trains are not difficult to implement, which makes them easier to adopt. A similar tensor decomposition known as the hierarchical tucker decomposition is much more tricky to use in practical code, which is likely why it is less popular than tensor trains despite theoretically being a superior format for many purposes.

__Dimensional scaling:__ If we keep the ranks $$r = r_1=\dots=r_{d-1}$$ of an order-$$d$$ tensor train fixed, then the amount of data required to store and manipulate a tensor train only scales linearly with the order of the tensor. A dense tensor format would scale exponentially with the tensor order and quickly become unmanageable, so this is an important property. Another way to phrase this is that tensor trains _do not suffer from the curse of dimensionality._

__Orthogonality and rounding:__ Tensor trains can be _orthogonalized_ with respect to any mode. They can also be _rounded_, i.e. we can lower all the ranks of the tensor train. These two operations are extremely useful for many algorithms and have a reasonable computational cost of $$O(r^3nd)$$ flops, and are also very simple to implement.

__Nice Riemannian structure:__ The tensor trains of a fixed maximum rank form a Riemannian manifold. The tangent space, and orthogonal projections onto this tangent space, are relatively easy to work with and compute. The manifold is also topologically closed, which means that optimization problems on this manifold are well-posed. These properties allow for some very efficient Riemannian optimization algorithms.


## Using tensor trains for machine learning

![A train](/imgs/thesis/chapter2_compressed.webp#nice-medium)

Armed with the knowledge that low-rank tensors can very efficiently represent certain discretized functions, I will show how we can use tensor trains to build an entirely novel kind of machine-learning estimator. Since I made [a more detailed blog post](/discrete-function-tensor) on this work, I will be brief and invite you to read this post if you want to learn more. 

### Matrices as discretized functions

Imagine we have a function $$f(x,y)\colon I^2\to \mathbb R$$, and we plot the values of this function on a square. For example, we have a plot of the following function below:

$$
    f(x,y) = 3\cos(10(x^2 + y^2/2)) -\sin(20(2x-y))/2
$$

![A plot of a 2D function](/imgs/thesis/low-rank-function1.webp#nice)

Remember that (grayscale) images are nothing but matrices, so if we use $$m\times n$$ pixels to plot this function, we have essentially created an $$m\times n$$ matrix. Remarkably, this particular matrix is always rank-4, no matter what size of the matrix we choose. Below we plot the rows of respectively the matrices $$X$$ and $$Y$$ of the low-rank decomposition $$A=XY^\top$$. It is interesting to observe that this low-rank decomposition does not visually change when we increase the size of the matrix.

![A plot of a 2D function](/imgs/thesis/low-rank-function2.webp#nice)

What we can take away from this is that low-rank matrices can potentially represent complicated 2D functions. As a natural extension, we can thus expect that low-rank tensors can represent complicated functions in higher dimensions.

We now have a way to parametrize complicated functions using relatively few parameters, which means that we can try to use these functions as a supervised learning model. Suppose that we are thus given a few samples $$y_j = \hat f(x_j)$$ with $$j=1,\dots, N$$ for $$x_j\in\mathbb R^{d}$$, and where $$\hat f$$ is an unknown flnction. Let $$f_A$$ be the discretized function obtained from a tensor or matrix $$A$$. Supervised learning then corresponds to the following least-squares problem:

$$
    \min_{A} \sum_{j=1}^N (f_A(x_j) - y_j)^2
$$

Each sample $$x_j$$ corresponds to an entry $$A[i_1(x_j),\dots,i_d(x_j)]$$, and therefore the least-squares problem can be rephrased as a matrix / tensor-completion problem. 